<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" property="og:image" content="/og-image.png"/><meta data-react-helmet="true" property="og:author" content="하성민"/><meta data-react-helmet="true" property="og:description" content="Vectorize contexts BoW DTM 코사인 유사도 TF-IDF 가중치 LSA 특잇값 분해 LDA soynlp 응집확률 브랜칭 엔트로피 자연어 처리에서 꼭 필요한 텍스트 데이터의 Veoctorization 중에서도 통계와 머신러닝 사용하는 방법의 변천사에 대한 설명이다. 토큰화하는 방식은
Word2vec 임베딩 모델이 생기기 전 정통적인 Vectorize 방식이었다. 현재는 Word2vec 기술을 사용하지만 단어를 학습시키기 위한 Vectorize 기술의 초기 개념을 이해하여 NLP 의 이해를 심화하도록 한다. 컴퓨터는 단어를 이해하지 못한다. 때문에 단어를 숫자 데이터로 표현하여야 한다. 하지만 단순히 단어를 1, 2, 3, 4.. 로 표현한다면 단어간의 차이를 정확하게 나타내지 못할 것이다 그러기 위해 vectorize 라는 기술이 생겨났다. 단어를 적절한 숫자 데이터로 변경하는 기술을 의미한다. 다음은 이 vectorize 의 문제점과 그 해결책을
발전 순서대로 나…"/><meta data-react-helmet="true" name="description" content="Vectorize contexts BoW DTM 코사인 유사도 TF-IDF 가중치 LSA 특잇값 분해 LDA soynlp 응집확률 브랜칭 엔트로피 자연어 처리에서 꼭 필요한 텍스트 데이터의 Veoctorization 중에서도 통계와 머신러닝 사용하는 방법의 변천사에 대한 설명이다. 토큰화하는 방식은
Word2vec 임베딩 모델이 생기기 전 정통적인 Vectorize 방식이었다. 현재는 Word2vec 기술을 사용하지만 단어를 학습시키기 위한 Vectorize 기술의 초기 개념을 이해하여 NLP 의 이해를 심화하도록 한다. 컴퓨터는 단어를 이해하지 못한다. 때문에 단어를 숫자 데이터로 표현하여야 한다. 하지만 단순히 단어를 1, 2, 3, 4.. 로 표현한다면 단어간의 차이를 정확하게 나타내지 못할 것이다 그러기 위해 vectorize 라는 기술이 생겨났다. 단어를 적절한 숫자 데이터로 변경하는 기술을 의미한다. 다음은 이 vectorize 의 문제점과 그 해결책을
발전 순서대로 나…"/><meta data-react-helmet="true" property="og:site_title" content="NLP 전처리 필수 Vecorize"/><meta data-react-helmet="true" property="og:title" content="NLP 전처리 필수 Vecorize"/><meta data-react-helmet="true" name="viewport" content="initial-scale=1, width=device-width"/><meta name="generator" content="Gatsby 4.9.3"/><style data-href="/styles.853e65090dc89bfbb4ac.css" data-identity="gatsby-global-css">@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:100;src:local("Montserrat Thin "),local("Montserrat-Thin"),url(/static/montserrat-latin-100-8d7d79679b70dbe27172b6460e7a7910.woff2) format("woff2"),url(/static/montserrat-latin-100-ec38980a9e0119a379e2a9b3dbb1901a.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:100;src:local("Montserrat Thin italic"),local("Montserrat-Thinitalic"),url(/static/montserrat-latin-100italic-e279051046ba1286706adc886cf1c96b.woff2) format("woff2"),url(/static/montserrat-latin-100italic-3b325a3173c8207435cd1b76e19bf501.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:200;src:local("Montserrat Extra Light "),local("Montserrat-Extra Light"),url(/static/montserrat-latin-200-9d266fbbfa6cab7009bd56003b1eeb67.woff2) format("woff2"),url(/static/montserrat-latin-200-2d8ba08717110d27122e54c34b8a5798.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:200;src:local("Montserrat Extra Light italic"),local("Montserrat-Extra Lightitalic"),url(/static/montserrat-latin-200italic-6e5b3756583bb2263eb062eae992735e.woff2) format("woff2"),url(/static/montserrat-latin-200italic-a0d6f343e4b536c582926255367a57da.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:300;src:local("Montserrat Light "),local("Montserrat-Light"),url(/static/montserrat-latin-300-00b3e893aab5a8fd632d6342eb72551a.woff2) format("woff2"),url(/static/montserrat-latin-300-ea303695ceab35f17e7d062f30e0173b.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:300;src:local("Montserrat Light italic"),local("Montserrat-Lightitalic"),url(/static/montserrat-latin-300italic-56f34ea368f6aedf89583d444bbcb227.woff2) format("woff2"),url(/static/montserrat-latin-300italic-54b0bf2c8c4c12ffafd803be2466a790.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:400;src:local("Montserrat Regular "),local("Montserrat-Regular"),url(/static/montserrat-latin-400-b71748ae4f80ec8c014def4c5fa8688b.woff2) format("woff2"),url(/static/montserrat-latin-400-0659a9f4e90db5cf51b50d005bff1e41.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:400;src:local("Montserrat Regular italic"),local("Montserrat-Regularitalic"),url(/static/montserrat-latin-400italic-6eed6b4cbb809c6efc7aa7ddad6dbe3e.woff2) format("woff2"),url(/static/montserrat-latin-400italic-7583622cfde30ae49086d18447ab28e7.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:500;src:local("Montserrat Medium "),local("Montserrat-Medium"),url(/static/montserrat-latin-500-091b209546e16313fd4f4fc36090c757.woff2) format("woff2"),url(/static/montserrat-latin-500-edd311588712a96bbf435fad264fff62.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:500;src:local("Montserrat Medium italic"),local("Montserrat-Mediumitalic"),url(/static/montserrat-latin-500italic-c90ced68b46050061d1a41842d6dfb43.woff2) format("woff2"),url(/static/montserrat-latin-500italic-5146cbfe02b1deea5dffea27a5f2f998.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:600;src:local("Montserrat SemiBold "),local("Montserrat-SemiBold"),url(/static/montserrat-latin-600-0480d2f8a71f38db8633b84d8722e0c2.woff2) format("woff2"),url(/static/montserrat-latin-600-b77863a375260a05dd13f86a1cee598f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:600;src:local("Montserrat SemiBold italic"),local("Montserrat-SemiBolditalic"),url(/static/montserrat-latin-600italic-cf46ffb11f3a60d7df0567f8851a1d00.woff2) format("woff2"),url(/static/montserrat-latin-600italic-c4fcfeeb057724724097167e57bd7801.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:700;src:local("Montserrat Bold "),local("Montserrat-Bold"),url(/static/montserrat-latin-700-7dbcc8a5ea2289d83f657c25b4be6193.woff2) format("woff2"),url(/static/montserrat-latin-700-99271a835e1cae8c76ef8bba99a8cc4e.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:700;src:local("Montserrat Bold italic"),local("Montserrat-Bolditalic"),url(/static/montserrat-latin-700italic-c41ad6bdb4bd504a843d546d0a47958d.woff2) format("woff2"),url(/static/montserrat-latin-700italic-6779372f04095051c62ed36bc1dcc142.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:800;src:local("Montserrat ExtraBold "),local("Montserrat-ExtraBold"),url(/static/montserrat-latin-800-db9a3e0ba7eaea32e5f55328ace6cf23.woff2) format("woff2"),url(/static/montserrat-latin-800-4e3c615967a2360f5db87d2f0fd2456f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:800;src:local("Montserrat ExtraBold italic"),local("Montserrat-ExtraBolditalic"),url(/static/montserrat-latin-800italic-bf45bfa14805969eda318973947bc42b.woff2) format("woff2"),url(/static/montserrat-latin-800italic-fe82abb0bcede51bf724254878e0c374.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:900;src:local("Montserrat Black "),local("Montserrat-Black"),url(/static/montserrat-latin-900-e66c7edc609e24bacbb705175669d814.woff2) format("woff2"),url(/static/montserrat-latin-900-8211f418baeb8ec880b80ba3c682f957.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:900;src:local("Montserrat Black italic"),local("Montserrat-Blackitalic"),url(/static/montserrat-latin-900italic-4454c775e48152c1a72510ceed3603e2.woff2) format("woff2"),url(/static/montserrat-latin-900italic-efcaa0f6a82ee0640b83a0916e6e8d68.woff) format("woff")}.search-input-wrapper{align-items:center;display:none;margin-top:3px;width:180px}@media(min-width:768px){.search-input-wrapper{display:flex}}.search-icon{color:var(--primary-text-color);margin-right:2px}.search-input{height:100%;width:100%}.search-input .MuiAutocomplete-inputRoot{padding-right:0!important}.search-input .MuiInputBase-input{color:var(--primary-text-color)!important;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;font-weight:500;padding-bottom:2px!important}.search-input .MuiInput-underline:before{border-bottom-width:1px}.search-input .MuiInput-underline:after,.search-input .MuiInput-underline:before{border-bottom-color:var(--primary-text-color)}.page-header-wrapper{display:flex;height:60px;justify-content:center;width:100%}.page-header-wrapper .page-header{align-items:center;display:flex;justify-content:space-between;max-width:720px;width:100%}.page-header-wrapper .page-header .link{color:var(--primary-text-color);font-size:17px;font-weight:700}@media(min-width:768px){.page-header-wrapper .page-header .link{font-size:20px;font-weight:700}}.page-header-wrapper .page-header .trailing-section{align-items:center;display:flex}.page-header-wrapper .page-header .trailing-section .link{margin-right:10px}@media(min-width:768px){.page-header-wrapper .page-header .trailing-section .link{margin-right:20px}}.page-footer-wrapper{align-items:center;display:flex;height:62px;justify-content:center;margin-top:auto;width:100%}.page-footer-wrapper .page-footer{max-width:720px;text-align:center;width:100%}.page-footer-wrapper .page-footer .link{color:var(--primary-text-color);font-size:20px;font-weight:700;margin-right:20px}.page-footer-wrapper .page-footer a{color:#3a95ff}.dark-mode-button-wrapper{align-items:center;bottom:20px;display:flex;justify-content:center;position:fixed;right:20px}.dark-mode-button{-webkit-backdrop-filter:blur(30px);backdrop-filter:blur(30px);background-color:#363f47!important;border-radius:50px;box-shadow:0 5px 25px rgba(0,0,0,.12);cursor:pointer;height:50px;width:50px;z-index:3}.dark-mode-icon{color:#fff}a,abbr,acronym,address,applet,article,aside,audio,b,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{border:0;font-size:100%;font:inherit;margin:0;padding:0;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:"";content:none}table{border-collapse:collapse;border-spacing:0}a{outline:none;text-decoration:none}html{--background-color:#fff;--primary-text-color:#000;--secondary-text-color:#9e9e9e;--content-text-color:#37352f;--button-background-color:#f3f3f4;--button-text-color:#363f47;--tab-text-color:#6e6d7a;--tab-hover-text-color:#0d0c22;--tab-selected-background-color:rgba(13,12,34,.05);--bio-link-icon-color:rgba(0,0,0,.54);--about-link-icon-color:#a8a8a8;--chip-background-color:#f3f3f4;--link-text-color:rgba(55,53,47,.7);--post-card-border-color:rgba(0,0,0,.12);--markdown-table-even-cell-background-color:#f6f8fa;--markdown-table-border-color:#dfe2e5;--markdown-blockquote-border-color:#dfe2e5;--markdown-border-color:#e1e4e8}html[data-theme=dark]{--background-color:#232326;--primary-text-color:#e6e6e6;--secondary-text-color:#768390;--content-text-color:#e6e6e6;--button-background-color:#444c56;--button-text-color:#363f47;--tab-text-color:#768390;--tab-hover-text-color:#acbac7;--tab-selected-background-color:#373e47;--chip-background-color:#323a42;--bio-link-icon-color:#e6e6e6;--about-link-icon-color:#a8a8a8;--link-text-color:#90b0ec;--post-card-border-color:#363f47;--markdown-table-even-cell-background-color:#2d333b;--markdown-table-border-color:#444c56;--markdown-blockquote-border-color:#4f5864;--markdown-border-color:#e1e4e8}*{-webkit-appearance:none;appearance:none;box-sizing:border-box}html{font-size:14px;height:100%;overflow-y:scroll;width:100%}body{background-color:var(--background-color)!important}a{color:var(--link-text-color)}.page-wrapper{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;color:var(--primary-text-color);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;justify-content:center;min-height:100vh;padding-left:15px;padding-right:15px;word-break:keep-all}.page-wrapper,.page-wrapper .page-content{align-items:center;display:flex;flex-direction:column;width:100%}.page-wrapper .page-content{max-width:720px}.icon{color:var(--about-link-icon-color);font-size:20px}.social-links .icon{color:var(--bio-link-icon-color);font-size:30px}@-webkit-keyframes blinking-cursor{0%{opacity:0}50%{opacity:1}to{opacity:0}}@keyframes blinking-cursor{0%{opacity:0}50%{opacity:1}to{opacity:0}}.bio{color:var(--primary-text-color);display:flex;flex-direction:column;justify-content:space-between;margin-bottom:120px;margin-top:120px;width:100%}@media(min-width:768px){.bio{align-items:center;flex-direction:row}}.bio .introduction{display:flex;flex-direction:column;word-break:keep-all}.bio .introduction .react-rotating-text-cursor{-webkit-animation:blinking-cursor .8s cubic-bezier(.68,.01,.01,.99) 0s infinite;animation:blinking-cursor .8s cubic-bezier(.68,.01,.01,.99) 0s infinite}.bio .introduction strong{display:inline-block;font-weight:600}.bio .introduction.korean{font-size:32px;font-weight:100;line-height:1.2}.bio .introduction.korean .title .react-rotating-text-cursor{font-size:35px;line-height:35px}@media(min-width:768px){.bio .introduction.korean{font-size:40px}.bio .introduction.korean .title .react-rotating-text-cursor{font-size:45px;line-height:45px}}.bio .introduction.english{font-family:montserrat;font-size:25px;line-height:1.2}@media(min-width:768px){.bio .introduction.english{font-size:45px}}.bio .introduction.english .name{font-size:35px;font-weight:600}.bio .introduction.english .job{font-size:35px}.bio .introduction.english .description{font-size:20px;font-weight:200;margin-top:8px}.bio .introduction.english .social-links{display:flex;margin-top:20px}.bio .thumbnail-wrapper{display:none}@media(min-width:768px){.bio .thumbnail-wrapper{display:block}}.section-header-wrapper{display:flex;justify-content:center;margin-bottom:32px;width:100%}.section-header-wrapper .section-header{border-bottom:4px solid var(--primary-text-color);color:var(--primary-text-color);font-size:30px;font-weight:700;padding-bottom:5px}.timestamp-section{align-items:center;display:flex;flex-direction:column;justify-content:center;margin-bottom:50px;width:100%;word-break:keep-all}.timestamp-section .body{padding:0 10px;width:100%}.timestamp-section .body .timestamp{border-left:2px solid #bdbdbd;display:flex;font-size:18px;font-weight:400;justify-items:center;margin-left:5px;padding:10px 0;width:100%}.timestamp-section .body .timestamp:first-child{padding-top:7px}.timestamp-section .body .timestamp:last-child{padding-bottom:7px}.timestamp-section .body .timestamp:before{align-self:center;background-color:var(--background-color);border:2px solid #828282;border-radius:10px;content:"";height:10px;left:-1px;position:relative;-webkit-transform:translatex(-50%);transform:translatex(-50%);width:10px}.timestamp-section .body .timestamp .date{align-self:center;color:#828282;margin-left:5px;margin-right:5px;min-width:115px;width:115px}@media(min-width:768px){.timestamp-section .body .timestamp .date{min-width:200px;width:200px}}.timestamp-section .body .timestamp .activity{line-height:23px;width:100%}.project-section{align-items:center;justify-content:center}.project-section,.project-section .project{display:flex;flex-direction:column;width:100%}.project-section .project{margin-bottom:30px;padding:15px}.project-section .project .head{font-size:20px;font-weight:700;line-height:30px;margin-bottom:10px}.project-section .project .body{display:flex;flex-direction:column;width:100%}.project-section .project .body .thumbnail{margin-bottom:10px;width:100%}.project-section .project .body .tech-stack{display:flex;margin-bottom:10px}.project-section .project .body .tech-stack .tech{background-color:var(--chip-background-color);border-radius:10px;font-size:15px;font-weight:500;margin-right:5px;padding:5px 7px}.project-section .project .body .description{font-size:16px;font-weight:400;line-height:1.4}@media(min-width:768px){.project-section .project .body{flex-direction:column}.project-section .project .body .content{margin-top:0}}.post-header{border-bottom:1px solid var(--post-card-border-color);display:flex;flex-direction:column;justify-content:center;margin-bottom:40px;margin-top:20px;padding-bottom:10px;width:100%;word-break:keep-all}.post-header .emoji{font-size:78px;margin-bottom:20px}.post-header .categories{margin-bottom:5px}.post-header .categories .category{color:var(--primary-text-color);font-weight:600;margin-right:4px}.post-header .categories .category:hover{text-decoration:underline}.post-header .title{color:var(--primary-text-color);font-size:32px;font-weight:600;line-height:1.3;margin-bottom:6px}.post-header .info{color:var(--secondary-text-color);display:flex;flex-wrap:wrap;font-size:16px;font-weight:500;line-height:1.5;width:100%}.post-header .info .author{margin-right:4px}.post-header .info strong{color:var(--primary-text-color);font-weight:600}.post-navigator{-webkit-column-gap:1.4%;column-gap:1.4%;display:grid;grid-template-columns:49.3% 49.3%;width:100%}.post-navigator .post-card{border:1px solid var(--post-card-border-color);border-radius:6px;color:var(--primary-text-color);cursor:pointer;display:flex;flex-direction:column;padding:15px;transition:-webkit-transform .2s;transition:transform .2s;transition:transform .2s,-webkit-transform .2s;width:100%}.post-navigator .post-card:hover .title{text-decoration:underline}.post-navigator .post-card.prev{margin-right:auto}.post-navigator .post-card.next{margin-left:auto}.post-navigator .post-card .direction{color:gray;font-size:14px;font-weight:500;margin-bottom:5px}.post-navigator .post-card .title{font-size:16px;font-weight:600;line-height:1.4;margin-bottom:7px}.markdown .octicon{fill:currentColor;display:inline-block;vertical-align:text-bottom}.markdown .anchor{float:left;line-height:1;margin-left:-20px;padding-right:4px}.markdown .anchor:focus{outline:none}.markdown h1 .octicon-link,.markdown h2 .octicon-link,.markdown h3 .octicon-link,.markdown h4 .octicon-link,.markdown h5 .octicon-link,.markdown h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown h1:hover .anchor,.markdown h2:hover .anchor,.markdown h3:hover .anchor,.markdown h4:hover .anchor,.markdown h5:hover .anchor,.markdown h6:hover .anchor{text-decoration:none}.markdown h1:hover .anchor .octicon-link,.markdown h2:hover .anchor .octicon-link,.markdown h3:hover .anchor .octicon-link,.markdown h4:hover .anchor .octicon-link,.markdown h5:hover .anchor .octicon-link,.markdown h6:hover .anchor .octicon-link{visibility:visible}.markdown h1:hover .anchor .octicon-link:before,.markdown h2:hover .anchor .octicon-link:before,.markdown h3:hover .anchor .octicon-link:before,.markdown h4:hover .anchor .octicon-link:before,.markdown h5:hover .anchor .octicon-link:before,.markdown h6:hover .anchor .octicon-link:before{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='16' height='16' aria-hidden='true' viewBox='0 0 16 16'%3E%3Cpath fill-rule='evenodd' d='M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z'/%3E%3C/svg%3E");content:" ";display:inline-block;height:16px;width:16px}.markdown{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;word-wrap:break-word;color:var(--content-text-color);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;font-weight:500;line-height:1.5}.markdown details{display:block}.markdown summary{display:list-item}.markdown a{background-color:initial}.markdown a:active,.markdown a:hover{outline-width:0}.markdown strong{font-weight:inherit;font-weight:bolder}.markdown h1{margin:.67em 0}.markdown img{border-style:none}.markdown code,.markdown kbd,.markdown pre{font-family:monospace,monospace;font-size:1em}.markdown hr{box-sizing:initial;overflow:visible}.markdown input{font:inherit;margin:0;overflow:visible}.markdown [type=checkbox]{box-sizing:border-box;padding:0}.markdown *{box-sizing:border-box}.markdown input{font-family:inherit;font-size:inherit;line-height:inherit}.markdown a{border-bottom:.05em solid;border-color:var(--link-text-color);color:var(--link-text-color);text-decoration:none}.markdown a.anchor{border-bottom:none}.markdown strong{font-weight:700}.markdown hr{background:transparent;border-bottom:1px solid var(--markdown-blockquote-border-color);height:0;margin:15px 0;overflow:hidden}.markdown hr:after,.markdown hr:before{content:"";display:table}.markdown hr:after{clear:both}.markdown table{border-collapse:collapse;border-spacing:0}.markdown td,.markdown th{padding:0}.markdown details summary{cursor:pointer}.markdown h1,.markdown h2,.markdown h3,.markdown h4,.markdown h5,.markdown h6{margin-bottom:0;margin-top:0}.markdown h1{font-size:32px}.markdown h1,.markdown h2{font-weight:600}.markdown h2{font-size:24px}.markdown h3{font-size:20px}.markdown h3,.markdown h4{font-weight:600}.markdown h4{font-size:16px}.markdown h5{font-size:14px}.markdown h5,.markdown h6{font-weight:600}.markdown h6{font-size:12px}.markdown p{margin-bottom:10px;margin-top:0}.markdown blockquote{margin:0}.markdown ol,.markdown ul{margin-bottom:0;margin-top:0;padding-left:0}.markdown ol ol,.markdown ul ol{list-style-type:lower-roman}.markdown ol ol ol,.markdown ol ul ol,.markdown ul ol ol,.markdown ul ul ol{list-style-type:lower-alpha}.markdown dd{margin-left:0}.markdown code,.markdown pre{font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;font-size:12px}.markdown code.language-text{border-radius:3px;font-size:85%;padding:.2em .4em}.markdown :not(pre)>code.language-text{background:hsla(44,6%,50%,.15);color:#eb5757;overflow-wrap:break-word}.markdown pre{margin-bottom:0;margin-top:0}.markdown input::-webkit-inner-spin-button,.markdown input::-webkit-outer-spin-button{-webkit-appearance:none;appearance:none;margin:0}.markdown :checked+.radio-label{border-color:#0366d6;position:relative;z-index:1}.markdown .border{border:1px solid var(--markdown-border-color)!important}.markdown .border-0{border:0!important}.markdown .border-bottom{border-bottom:1px solid var(--markdown-border-color)!important}.markdown .rounded-1{border-radius:3px!important}.markdown .bg-white{background-color:transparent!important}.markdown .bg-gray-light{background-color:#fafbfc!important}.markdown .text-gray-light{color:#6a737d!important}.markdown .pl-3,.markdown .px-3{padding-left:16px!important}.markdown .px-3{padding-right:16px!important}.markdown .f6{font-size:12px!important}.markdown .lh-condensed{line-height:1.25!important}.markdown .text-bold{font-weight:600!important}.markdown .pl-c{color:#6a737d}.markdown .pl-c1,.markdown .pl-s .pl-v{color:#005cc5}.markdown .pl-e,.markdown .pl-en{color:#6f42c1}.markdown .pl-s .pl-s1,.markdown .pl-smi{color:#24292e}.markdown .pl-ent{color:#22863a}.markdown .pl-k{color:#d73a49}.markdown .pl-pds,.markdown .pl-s,.markdown .pl-s .pl-pse .pl-s1,.markdown .pl-sr,.markdown .pl-sr .pl-cce,.markdown .pl-sr .pl-sra,.markdown .pl-sr .pl-sre{color:#032f62}.markdown .pl-smw,.markdown .pl-v{color:#e36209}.markdown .pl-bu{color:#b31d28}.markdown .pl-ii{background-color:#b31d28;color:#fafbfc}.markdown .pl-c2{background-color:#d73a49;color:#fafbfc}.markdown .pl-c2:before{content:"^M"}.markdown .pl-sr .pl-cce{color:#22863a;font-weight:700}.markdown .pl-ml{color:#735c0f}.markdown .pl-mh,.markdown .pl-mh .pl-en,.markdown .pl-ms{color:#005cc5;font-weight:700}.markdown .pl-mi{color:#24292e;font-style:italic}.markdown .pl-mb{color:#24292e;font-weight:700}.markdown .pl-md{background-color:#ffeef0;color:#b31d28}.markdown .pl-mi1{background-color:#f0fff4;color:#22863a}.markdown .pl-mc{background-color:#ffebda;color:#e36209}.markdown .pl-mi2{background-color:#005cc5;color:#f6f8fa}.markdown .pl-mdr{color:#6f42c1;font-weight:700}.markdown .pl-ba{color:#586069}.markdown .pl-sg{color:#959da5}.markdown .pl-corl{color:#032f62;text-decoration:underline}.markdown .mb-0{margin-bottom:0!important}.markdown .my-2{margin-bottom:8px!important;margin-top:8px!important}.markdown .pl-0{padding-left:0!important}.markdown .py-0{padding-bottom:0!important;padding-top:0!important}.markdown .pl-1{padding-left:4px!important}.markdown .pl-2{padding-left:8px!important}.markdown .py-2{padding-bottom:8px!important;padding-top:8px!important}.markdown .pl-3{padding-left:16px!important}.markdown .pl-4{padding-left:24px!important}.markdown .pl-5{padding-left:32px!important}.markdown .pl-6{padding-left:40px!important}.markdown .pl-7{padding-left:48px!important}.markdown .pl-8{padding-left:64px!important}.markdown .pl-9{padding-left:80px!important}.markdown .pl-10{padding-left:96px!important}.markdown .pl-11{padding-left:112px!important}.markdown .pl-12{padding-left:128px!important}.markdown hr{border-bottom-color:#eee}.markdown kbd{background-color:#fafbfc;border:1px solid #d1d5da;border-radius:3px;box-shadow:inset 0 -1px 0 #d1d5da;color:#444d56;display:inline-block;font:11px SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;line-height:10px;padding:3px 5px;vertical-align:middle}.markdown:after,.markdown:before{content:"";display:table}.markdown:after{clear:both}.markdown>:first-child{margin-top:0!important}.markdown>:last-child{margin-bottom:0!important}.markdown a:not([href]){color:inherit;text-decoration:none}.markdown blockquote,.markdown details,.markdown dl,.markdown ol,.markdown p,.markdown pre,.markdown table,.markdown ul{margin-bottom:16px;margin-top:0}.markdown hr{background-color:var(--markdown-border-color);border:0;height:.25em;margin:24px 0;padding:0}.markdown blockquote{border-left:.25em solid var(--markdown-blockquote-border-color);color:#6a737d;padding:0 1em}.markdown blockquote>:first-child{margin-top:0}.markdown blockquote>:last-child{margin-bottom:0}.markdown h1,.markdown h2,.markdown h3,.markdown h4,.markdown h5,.markdown h6{font-weight:600;line-height:1.25;margin-bottom:16px;margin-top:24px}.markdown h1{font-size:2em}.markdown h2{font-size:1.5em}.markdown h3{font-size:1.25em}.markdown h4{font-size:1em}.markdown h5{font-size:.875em}.markdown h6{color:#6a737d;font-size:.85em}.markdown ol,.markdown ul{padding-left:2em}.markdown ol ol,.markdown ol ul,.markdown ul ol,.markdown ul ul{margin-bottom:0;margin-top:0}.markdown ol{list-style-type:decimal}.markdown ul{list-style-type:disc}.markdown li{word-wrap:break-all;display:list-item;text-align:-webkit-match-parent}.markdown li>p{margin-top:16px}.markdown li+li{margin-top:.25em}.markdown dl{padding:0}.markdown dl dt{font-size:1em;font-style:italic;font-weight:600;margin-top:16px;padding:0}.markdown dl dd{margin-bottom:16px;padding:0 16px}.markdown table{display:block;overflow:auto;width:100%}.markdown table th{font-weight:600}.markdown table td,.markdown table th{border:1px solid var(--markdown-table-border-color);padding:6px 13px}.markdown table tr{border-top:1px solid var(--markdown-table-border-color)}.markdown table tr:nth-child(2n){background-color:var(--markdown-table-even-cell-background-color)}.markdown img{background-color:transparent;box-sizing:initial;display:block;margin:0 auto;max-width:100%}.markdown img[align=right]{padding-left:20px}.markdown img[align=left]{padding-right:20px}.markdown code{background-color:rgba(27,31,35,.05);border-radius:3px;font-size:85%;margin:0;padding:.2em .4em}.markdown pre{word-wrap:normal}.markdown pre>code{background:transparent;border:0;font-size:100%;margin:0;padding:0;white-space:pre;word-break:normal}.markdown .highlight{margin-bottom:16px}.markdown .highlight pre{margin-bottom:0;word-break:normal}.markdown .highlight pre,.markdown pre{background-color:#f6f8fa;border-radius:3px;font-size:85%;line-height:1.45;overflow:auto;padding:16px}.markdown pre code{word-wrap:normal;background-color:initial;border:0;display:inline;line-height:inherit;margin:0;max-width:auto;overflow:visible;padding:0}.markdown .commit-tease-sha{color:#444d56;display:inline-block;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;font-size:90%}.markdown .full-commit .btn-outline:not(:disabled):hover{border-color:#005cc5;color:#005cc5}.markdown .blob-wrapper{overflow-x:auto;overflow-y:hidden}.markdown .blob-wrapper-embedded{max-height:240px;overflow-y:auto}.markdown .blob-num{color:rgba(27,31,35,.3);cursor:pointer;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;font-size:12px;line-height:20px;min-width:50px;padding-left:10px;padding-right:10px;text-align:right;-webkit-user-select:none;-ms-user-select:none;user-select:none;vertical-align:top;white-space:nowrap;width:1%}.markdown .blob-num:hover{color:rgba(27,31,35,.6)}.markdown .blob-num:before{content:attr(data-line-number)}.markdown .blob-code{line-height:20px;padding-left:10px;padding-right:10px;position:relative;vertical-align:top}.markdown .blob-code-inner{word-wrap:normal;color:#24292e;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;font-size:12px;overflow:visible;white-space:pre}.markdown .pl-token.active,.markdown .pl-token:hover{background:#ffea7f;cursor:pointer}.markdown .tab-size[data-tab-size="1"]{-o-tab-size:1;tab-size:1}.markdown .tab-size[data-tab-size="2"]{-o-tab-size:2;tab-size:2}.markdown .tab-size[data-tab-size="3"]{-o-tab-size:3;tab-size:3}.markdown .tab-size[data-tab-size="4"]{-o-tab-size:4;tab-size:4}.markdown .tab-size[data-tab-size="5"]{-o-tab-size:5;tab-size:5}.markdown .tab-size[data-tab-size="6"]{-o-tab-size:6;tab-size:6}.markdown .tab-size[data-tab-size="7"]{-o-tab-size:7;tab-size:7}.markdown .tab-size[data-tab-size="8"]{-o-tab-size:8;tab-size:8}.markdown .tab-size[data-tab-size="9"]{-o-tab-size:9;tab-size:9}.markdown .tab-size[data-tab-size="10"]{-o-tab-size:10;tab-size:10}.markdown .tab-size[data-tab-size="11"]{-o-tab-size:11;tab-size:11}.markdown .tab-size[data-tab-size="12"]{-o-tab-size:12;tab-size:12}.markdown .task-list-item{list-style-type:none}.markdown .task-list-item+.task-list-item{margin-top:3px}.markdown .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown .table-of-contents{align-items:flex-start;display:flex;flex-direction:column;justify-content:center;position:fixed;right:0;top:75px;width:340px}@media(max-width:1300px){.markdown .table-of-contents{display:none}}.markdown .table-of-contents ul{cursor:pointer;list-style-type:none}.markdown .table-of-contents ul li a{border-bottom:none;color:var(--secondary-text-color);font-size:14px;height:30px;padding:6px 2px;width:100%}.markdown .table-of-contents ul p{margin:0}.markdown .gatsby-resp-image-wrapper{display:flex!important;justify-content:center!important;max-height:560px!important;width:100%!important}.markdown .gatsby-resp-image-wrapper img{height:auto!important;max-height:560px!important;max-width:100%!important;position:relative!important;width:auto!important}.markdown .gatsby-resp-image-wrapper+em{color:#6a737d;display:block;font-size:15px;font-style:italic;text-align:center}code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#ccc;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-o-tab-size:4;tab-size:4;text-align:left;white-space:pre;word-break:normal;word-spacing:normal}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#2d2d2d}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.block-comment,.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#999}.token.punctuation{color:#ccc}.token.attr-name,.token.deleted,.token.namespace,.token.tag{color:#e2777a}.token.function-name{color:#6196cc}.token.boolean,.token.function,.token.number{color:#f08d49}.token.class-name,.token.constant,.token.property,.token.symbol{color:#f8c555}.token.atrule,.token.builtin,.token.important,.token.keyword,.token.selector{color:#cc99cd}.token.attr-value,.token.char,.token.regex,.token.string,.token.variable{color:#7ec699}.token.entity,.token.operator,.token.url{color:#67cdcc}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.token.inserted{color:green}.post-content{margin-bottom:20px;width:100%}.category-page-header-wrapper,.post-content{display:flex;flex-direction:column;justify-content:center}.category-page-header-wrapper{align-items:center;margin-bottom:30px;margin-top:30px}.category-page-header-wrapper .category-page-title{border-bottom:3px solid var(--primary-text-color);font-size:40px;font-weight:700;margin-bottom:15px;padding-bottom:7px;text-align:center;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}.category-page-header-wrapper .category-page-subtitle{font-size:20px;font-weight:500;padding-bottom:10px;text-align:center}.post-card-wrapper{display:flex;justify-content:center;min-height:150px;width:100%}.post-card-wrapper .post-card{border:1px solid var(--post-card-border-color);border-radius:6px;color:var(--primary-text-color);cursor:pointer;display:flex;flex-direction:column;height:100%;margin-bottom:15px;max-width:720px;padding:15px;transition:-webkit-transform .2s;transition:transform .2s;transition:transform .2s,-webkit-transform .2s;width:100%}.post-card-wrapper .post-card:hover .title{text-decoration:underline}@media(min-width:768px){.post-card-wrapper .post-card{margin-bottom:0}}.post-card-wrapper .post-card .title{font-size:18px;font-weight:600;line-height:1.4;margin-bottom:7px}.post-card-wrapper .post-card .description{-webkit-line-clamp:3;-webkit-box-orient:vertical;color:var(--primary-text-color);display:-webkit-box;font-size:13px;line-height:20px;margin-bottom:10px;overflow:hidden;text-overflow:ellipsis}.post-card-wrapper .post-card .info{color:var(--about-link-icon-color);display:flex;font-size:14px;justify-content:space-between;margin-top:auto}.post-card-wrapper .post-card .info .categories{display:flex}.post-card-wrapper .post-card .info .categories .category{margin-left:4px}.post-card-wrapper .post-card .info .categories .category:hover{text-decoration:underline}.post-card-column-wrapper{display:flex;justify-content:center;width:100%}.post-card-column-wrapper .post-card-column{align-items:center;display:flex;flex-direction:column;width:100%}.post-card-column-wrapper .post-card-column .post-card-wrapper{margin-bottom:10px}.post-card-column-wrapper .post-card-column .more-post-card-button{background-color:var(--button-background-color);color:var(--tab-hover-text-color);font-size:15px;font-weight:500;height:40px}.post-tabs-wrapper{align-self:flex-start;display:flex;flex-direction:column;justify-content:center;top:0;width:100%}.post-tabs-wrapper .post-tabs{display:flex;height:40px;justify-content:center;margin-bottom:12px;max-width:760px;width:100%}.post-tabs-wrapper .post-tabs .mui-tabs .MuiTab-root{color:var(--tab-text-color);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:17px;font-weight:500;height:40px;min-height:auto;min-width:auto;padding:10px 12px;transition:all .2s ease}.post-tabs-wrapper .post-tabs .mui-tabs .Mui-selected,.post-tabs-wrapper .post-tabs .mui-tabs .MuiTab-root :hover{color:var(--tab-hover-text-color);transition:all .2s ease}.post-tabs-wrapper .post-tabs .mui-tabs .Mui-selected{background-color:var(--tab-selected-background-color);border-radius:8px;font-weight:600}.post-tabs-wrapper .post-tabs .mui-tabs .MuiTabScrollButton-root{height:40px;width:20px}.post-tabs-wrapper .post-tabs .mui-tabs .MuiTabs-scrollable{height:40px}.post-tabs-wrapper .post-tabs .mui-tabs .MuiTabs-indicator{display:none}</style><style data-emotion="css-global o6gwfi">html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;box-sizing:border-box;-webkit-text-size-adjust:100%;}*,*::before,*::after{box-sizing:inherit;}strong,b{font-weight:700;}body{margin:0;color:rgba(0, 0, 0, 0.87);font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:400;font-size:1rem;line-height:1.5;letter-spacing:0.00938em;background-color:#fff;}@media print{body{background-color:#fff;}}body::backdrop{background-color:#fff;}</style><style data-emotion="css-global 1prfaxn">@-webkit-keyframes mui-auto-fill{from{display:block;}}@keyframes mui-auto-fill{from{display:block;}}@-webkit-keyframes mui-auto-fill-cancel{from{display:block;}}@keyframes mui-auto-fill-cancel{from{display:block;}}</style><style data-emotion="css 1l6di18 feqhe6 11tfndm mnn31 vubbuv 1yxmbwk 6flbmm">.css-1l6di18.Mui-focused .MuiAutocomplete-clearIndicator{visibility:visible;}@media (pointer: fine){.css-1l6di18:hover .MuiAutocomplete-clearIndicator{visibility:visible;}}.css-1l6di18 .MuiAutocomplete-tag{margin:3px;max-width:calc(100% - 6px);}.css-1l6di18 .MuiAutocomplete-inputRoot{-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.MuiAutocomplete-hasPopupIcon.css-1l6di18 .MuiAutocomplete-inputRoot,.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiAutocomplete-inputRoot{padding-right:30px;}.MuiAutocomplete-hasPopupIcon.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiAutocomplete-inputRoot{padding-right:56px;}.css-1l6di18 .MuiAutocomplete-inputRoot .MuiAutocomplete-input{width:0;min-width:30px;}.css-1l6di18 .MuiInput-root{padding-bottom:1px;}.css-1l6di18 .MuiInput-root .MuiInput-input{padding:4px 4px 4px 0px;}.css-1l6di18 .MuiInput-root.MuiInputBase-sizeSmall .MuiInput-input{padding:2px 4px 3px 0;}.css-1l6di18 .MuiOutlinedInput-root{padding:9px;}.MuiAutocomplete-hasPopupIcon.css-1l6di18 .MuiOutlinedInput-root,.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiOutlinedInput-root{padding-right:39px;}.MuiAutocomplete-hasPopupIcon.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiOutlinedInput-root{padding-right:65px;}.css-1l6di18 .MuiOutlinedInput-root .MuiAutocomplete-input{padding:7.5px 4px 7.5px 6px;}.css-1l6di18 .MuiOutlinedInput-root .MuiAutocomplete-endAdornment{right:9px;}.css-1l6di18 .MuiOutlinedInput-root.MuiInputBase-sizeSmall{padding:6px;}.css-1l6di18 .MuiOutlinedInput-root.MuiInputBase-sizeSmall .MuiAutocomplete-input{padding:2.5px 4px 2.5px 6px;}.css-1l6di18 .MuiFilledInput-root{padding-top:19px;padding-left:8px;}.MuiAutocomplete-hasPopupIcon.css-1l6di18 .MuiFilledInput-root,.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiFilledInput-root{padding-right:39px;}.MuiAutocomplete-hasPopupIcon.MuiAutocomplete-hasClearIcon.css-1l6di18 .MuiFilledInput-root{padding-right:65px;}.css-1l6di18 .MuiFilledInput-root .MuiFilledInput-input{padding:7px 4px;}.css-1l6di18 .MuiFilledInput-root .MuiAutocomplete-endAdornment{right:9px;}.css-1l6di18 .MuiFilledInput-root.MuiInputBase-sizeSmall{padding-bottom:1px;}.css-1l6di18 .MuiFilledInput-root.MuiInputBase-sizeSmall .MuiFilledInput-input{padding:2.5px 4px;}.css-1l6di18 .MuiInputBase-hiddenLabel{padding-top:8px;}.css-1l6di18 .MuiAutocomplete-input{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;text-overflow:ellipsis;opacity:1;}.css-feqhe6{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;min-width:0;padding:0;margin:0;border:0;vertical-align:top;width:100%;}.css-11tfndm{font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:400;font-size:1rem;line-height:1.4375em;letter-spacing:0.00938em;color:rgba(0, 0, 0, 0.87);box-sizing:border-box;position:relative;cursor:text;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;position:relative;}.css-11tfndm.Mui-disabled{color:rgba(0, 0, 0, 0.38);cursor:default;}label+.css-11tfndm{margin-top:16px;}.css-11tfndm:after{border-bottom:2px solid #1976d2;left:0;bottom:0;content:"";position:absolute;right:0;-webkit-transform:scaleX(0);-moz-transform:scaleX(0);-ms-transform:scaleX(0);transform:scaleX(0);-webkit-transition:-webkit-transform 200ms cubic-bezier(0.0, 0, 0.2, 1) 0ms;transition:transform 200ms cubic-bezier(0.0, 0, 0.2, 1) 0ms;pointer-events:none;}.css-11tfndm.Mui-focused:after{-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}.css-11tfndm.Mui-error:after{border-bottom-color:#d32f2f;-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}.css-11tfndm:before{border-bottom:1px solid rgba(0, 0, 0, 0.42);left:0;bottom:0;content:"\00a0";position:absolute;right:0;-webkit-transition:border-bottom-color 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:border-bottom-color 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;pointer-events:none;}.css-11tfndm:hover:not(.Mui-disabled):before{border-bottom:2px solid rgba(0, 0, 0, 0.87);}@media (hover: none){.css-11tfndm:hover:not(.Mui-disabled):before{border-bottom:1px solid rgba(0, 0, 0, 0.42);}}.css-11tfndm.Mui-disabled:before{border-bottom-style:dotted;}.css-mnn31{font:inherit;letter-spacing:inherit;color:currentColor;padding:4px 0 5px;border:0;box-sizing:content-box;background:none;height:1.4375em;margin:0;-webkit-tap-highlight-color:transparent;display:block;min-width:0;width:100%;-webkit-animation-name:mui-auto-fill-cancel;animation-name:mui-auto-fill-cancel;-webkit-animation-duration:10ms;animation-duration:10ms;}.css-mnn31::-webkit-input-placeholder{color:currentColor;opacity:0.42;-webkit-transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;}.css-mnn31::-moz-placeholder{color:currentColor;opacity:0.42;-webkit-transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;}.css-mnn31:-ms-input-placeholder{color:currentColor;opacity:0.42;-webkit-transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;}.css-mnn31::-ms-input-placeholder{color:currentColor;opacity:0.42;-webkit-transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:opacity 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;}.css-mnn31:focus{outline:0;}.css-mnn31:invalid{box-shadow:none;}.css-mnn31::-webkit-search-decoration{-webkit-appearance:none;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31::-webkit-input-placeholder{opacity:0!important;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31::-moz-placeholder{opacity:0!important;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31:-ms-input-placeholder{opacity:0!important;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31::-ms-input-placeholder{opacity:0!important;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31:focus::-webkit-input-placeholder{opacity:0.42;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31:focus::-moz-placeholder{opacity:0.42;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31:focus:-ms-input-placeholder{opacity:0.42;}label[data-shrink=false]+.MuiInputBase-formControl .css-mnn31:focus::-ms-input-placeholder{opacity:0.42;}.css-mnn31.Mui-disabled{opacity:1;-webkit-text-fill-color:rgba(0, 0, 0, 0.38);}.css-mnn31:-webkit-autofill{-webkit-animation-duration:5000s;animation-duration:5000s;-webkit-animation-name:mui-auto-fill;animation-name:mui-auto-fill;}.css-vubbuv{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;}.css-1yxmbwk{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;text-align:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;font-size:1.5rem;padding:8px;border-radius:50%;overflow:visible;color:rgba(0, 0, 0, 0.54);-webkit-transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;}.css-1yxmbwk::-moz-focus-inner{border-style:none;}.css-1yxmbwk.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-1yxmbwk{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-1yxmbwk:hover{background-color:rgba(0, 0, 0, 0.04);}@media (hover: none){.css-1yxmbwk:hover{background-color:transparent;}}.css-1yxmbwk.Mui-disabled{background-color:transparent;color:rgba(0, 0, 0, 0.26);}.css-6flbmm{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:2.1875rem;}</style><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="dns-prefetch" href="https://www.google-analytics.com"/><script>
  
  function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='0',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', '0', 'auto', {});
      ga('set', 'anonymizeIp', true);
      
      
      
      
      }</script><link rel="icon" href="/favicon-32x32.png?v=ad9e124e5060ab5ddbaf24744e1cfc72" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=ad9e124e5060ab5ddbaf24744e1cfc72"/><style type="text/css">
    .anchor.before {
      position: absolute;
      top: 0;
      left: 0;
      transform: translateX(-100%);
      padding-right: 4px;
    }
    .anchor.after {
      display: inline-block;
      padding-left: 4px;
    }
    h1 .anchor svg,
    h2 .anchor svg,
    h3 .anchor svg,
    h4 .anchor svg,
    h5 .anchor svg,
    h6 .anchor svg {
      visibility: hidden;
    }
    h1:hover .anchor svg,
    h2:hover .anchor svg,
    h3:hover .anchor svg,
    h4:hover .anchor svg,
    h5:hover .anchor svg,
    h6:hover .anchor svg,
    h1 .anchor:focus svg,
    h2 .anchor:focus svg,
    h3 .anchor:focus svg,
    h4 .anchor:focus svg,
    h5 .anchor:focus svg,
    h6 .anchor:focus svg {
      visibility: visible;
    }
  </style><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><title data-react-helmet="true">NLP 전처리 필수 Vecorize</title><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fBBc4.woff2"/><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/roboto/v30/KFOmCnqEu92Fr1Mu4mxK.woff2"/><link rel="preload" as="font" type="font/woff2" crossorigin="anonymous" href="/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmEU9fBBc4.woff2"/><style>@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:300;src:url(/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format("woff2")}@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:400;src:url(/static/webfonts/s/roboto/v30/KFOmCnqEu92Fr1Mu4mxK.woff2) format("woff2")}@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:500;src:url(/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmEU9fBBc4.woff2) format("woff2")}@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:300;src:url(/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fBBc-.woff) format("woff")}@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:400;src:url(/static/webfonts/s/roboto/v30/KFOmCnqEu92Fr1Mu4mxM.woff) format("woff")}@font-face{font-display:swap;font-family:Roboto;font-style:normal;font-weight:500;src:url(/static/webfonts/s/roboto/v30/KFOlCnqEu92Fr1MmEU9fBBc-.woff) format("woff")}</style><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link as="script" rel="preload" href="/webpack-runtime-9dfb7ebabee66c506236.js"/><link as="script" rel="preload" href="/framework-71a91a8132c4a176c255.js"/><link as="script" rel="preload" href="/app-8340b64cb5b3e506fb78.js"/><link as="script" rel="preload" href="/f9d3028dbef90a6e9b8db85387d63dd9f4edf538-e4cfa69055e2f9894560.js"/><link as="script" rel="preload" href="/component---src-templates-blog-template-js-94a4cd73c7c267c46a0e.js"/><link as="fetch" rel="preload" href="/page-data/NLP_3/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/1073350324.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/1956554647.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2938748437.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="page-wrapper"><header class="page-header-wrapper"><div class="page-header"><div class="front-section"><a class="link" href="/">Oha&#x27;s</a></div><div class="trailing-section"><a class="link" href="/about">about</a><a class="link" href="/posts">posts</a><div class="MuiAutocomplete-root MuiAutocomplete-hasPopupIcon css-1l6di18" role="combobox" aria-expanded="false"><div class="search-input-wrapper"><div class="MuiFormControl-root MuiFormControl-fullWidth MuiTextField-root search-input css-feqhe6"><div class="MuiInput-root MuiInput-underline MuiInputBase-root MuiInputBase-colorPrimary MuiInputBase-fullWidth MuiInputBase-formControl MuiInputBase-adornedEnd MuiAutocomplete-inputRoot css-11tfndm"><input type="text" aria-invalid="false" autoComplete="off" value="" class="MuiInput-input MuiInputBase-input MuiInputBase-inputAdornedEnd MuiAutocomplete-input MuiAutocomplete-inputFocused css-mnn31" aria-autocomplete="list" autoCapitalize="none" spellcheck="false"/><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium search-icon css-vubbuv" focusable="false" viewBox="0 0 24 24" aria-hidden="true" data-testid="SearchOutlinedIcon"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path></svg></div></div></div></div></div></div></header><main class="page-content"><header class="post-header"><div class="emoji">💍💍</div><div class="info"><div class="categories"><a class="category" href="/posts/NLP">NLP</a></div></div><h1 class="title">NLP 전처리 필수 Vecorize</h1><div class="info"><div class="author">posted by <strong>하성민</strong>,</div> <!-- -->March 25, 2022</div></header><div class="post-content"><div class="markdown"><h1 id="vectorize" style="position:relative;"><a href="#vectorize" aria-label="vectorize permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Vectorize</h1>
<h1 id="contexts" style="position:relative;"><a href="#contexts" aria-label="contexts permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>contexts</h1>
<ol>
<li>BoW</li>
<li>DTM</li>
</ol>
<ul>
<li>코사인 유사도</li>
</ul>
<ol start="3">
<li>TF-IDF 가중치</li>
<li>LSA</li>
</ol>
<ul>
<li>특잇값 분해</li>
</ul>
<ol start="5">
<li>LDA</li>
<li>soynlp</li>
</ol>
<ul>
<li>응집확률</li>
<li>브랜칭 엔트로피</li>
</ul>
<p>자연어 처리에서 꼭 필요한<br>
텍스트 데이터의 Veoctorization 중에서도<br>
통계와 머신러닝 사용하는 방법의 변천사에 대한 설명이다.</p>
<p>토큰화하는 방식은
Word2vec 임베딩 모델이 생기기 전<br>
정통적인 Vectorize 방식이었다.</p>
<hr>
<p>현재는 Word2vec 기술을 사용하지만<br>
단어를 학습시키기 위한 Vectorize 기술의 초기 개념을 이해하여<br>
NLP 의 이해를 심화하도록 한다.</p>
<hr>
<p>컴퓨터는 단어를 이해하지 못한다.<br>
때문에 단어를 숫자 데이터로 표현하여야 한다.<br>
하지만 단순히 단어를 1, 2, 3, 4.. 로 표현한다면<br>
단어간의 차이를 정확하게 나타내지 못할 것이다</p>
<p>그러기 위해 vectorize 라는 기술이 생겨났다.<br>
단어를 적절한 숫자 데이터로 변경하는 기술을 의미한다.</p>
<p>다음은 이 vectorize 의 문제점과 그 해결책을
발전 순서대로 나열한 내용이다.</p>
<h2 id="1-bag-of-words-단어표현-방법" style="position:relative;"><a href="#1-bag-of-words-%EB%8B%A8%EC%96%B4%ED%91%9C%ED%98%84-%EB%B0%A9%EB%B2%95" aria-label="1 bag of words 단어표현 방법 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1. Bag of Words 단어표현 방법</h2>
<blockquote>
<p>단어들의 분포로 문서의 특성을 파악하는 기법 ; BoW</p>
</blockquote>
<p>한 말뭉치를 정리할때 순서는 무시하고 중복횟수를 중시한다.</p>
<p>BoW 는 <code class="language-text">Dic</code> 형태로 저장된다.
각 Key 는 단어, value 는 빈도이다.</p>
<p>BoW 는 다음의 라이브러리로 만들 수 있다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment">#1번방법 tensorflow 사용</span>
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer
<span class="token comment">#fit_on_texts()</span>


<span class="token comment">#2번방법 scikit learn 사용</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
<span class="token comment">#fit_transform() :빈도가 저장</span>
<span class="token comment">#vocabulary_ :단어들의 빈도 index 표현</span>
<span class="token comment">#해당 라이브러리에서는 빈도가 아닌 index 를 표현</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer

sentence <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games."</span><span class="token punctuation">]</span>

tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token comment"># 단어장 생성</span>
bow <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>word_counts<span class="token punctuation">)</span> <span class="token comment"># 각 단어와 각 단어의 빈도를 bow에 저장</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Bag of Words :"</span><span class="token punctuation">,</span> bow<span class="token punctuation">)</span> <span class="token comment"># bow 출력</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'단어장(Vocabulary)의 크기 :'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>word_counts<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 중복을 제거한 단어들의 개수</span></code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer

sentence <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games."</span><span class="token punctuation">]</span>

vector <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
bow <span class="token operator">=</span> vector<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Bag of Words : '</span><span class="token punctuation">,</span> bow<span class="token punctuation">)</span> <span class="token comment"># 코퍼스로부터 각 단어의 빈도수를 기록한다.</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'각 단어의 인덱스 :'</span><span class="token punctuation">,</span> vector<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span> <span class="token comment"># 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.</span></code></pre></div>
<p>BoW의 집합으로 DTM 을 만들 수 있다.</p>
<hr>
<h2 id="2-dtm--document-term-matrix" style="position:relative;"><a href="#2-dtm--document-term-matrix" aria-label="2 dtm  document term matrix permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2. DTM ; document-Term Matrix</h2>
<p>문서 단어 행렬</p>
<p>여러 문서의 BoW 를 하나의 행렬로 구현한 것을 의미한다.
DTM 은 문서를 row 로 단어를 col 로 가진다.</p>
<p>단어를 row 로 문서를 col 로 지정한다면
TDM 이라고 한다.</p>
<p>가령,  세 개의 문서가 있다면
DTM 은 다음과 같다.</p>
<p><img src="/a87fb8568689c503c086526294178178/image1.png"></img></p>
<p>문서는 row 에, 단어들은 col 에 있다.</p>
<p>이렇게 하면 각 문서의 BoW 는 없는 단어들도 0 값이 입력되어 구성된다.</p>
<p>만약 문서가 많아진다면, 0값도 점점 많아질 것이다.</p>
<p>DTM 을 이용해 kosine 유사도를 구할 수 있다.</p>
<hr>
<h3 id="kosine-유사도" style="position:relative;"><a href="#kosine-%EC%9C%A0%EC%82%AC%EB%8F%84" aria-label="kosine 유사도 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kosine 유사도</h3>
<p>코사인 유사도는 문장들간의 유사성을 측정하기 위해 만들어진 방법이다.</p>
<p><a href="https://wikidocs.net/24603">코사인 유사도</a></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> numpy <span class="token keyword">import</span> dot
<span class="token keyword">from</span> numpy<span class="token punctuation">.</span>linalg <span class="token keyword">import</span> norm

doc1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 문서1 벡터</span>
doc2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 문서2 벡터</span>
doc3 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 문서3 벡터</span>

<span class="token keyword">def</span> <span class="token function">cos_sim</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>norm<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token operator">*</span>norm<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'문서 1과 문서2의 유사도 :'</span><span class="token punctuation">,</span>cos_sim<span class="token punctuation">(</span>doc1<span class="token punctuation">,</span> doc2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'문서 1과 문서3의 유사도 :'</span><span class="token punctuation">,</span>cos_sim<span class="token punctuation">(</span>doc1<span class="token punctuation">,</span> doc3<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'문서 2와 문서3의 유사도 :'</span><span class="token punctuation">,</span>cos_sim<span class="token punctuation">(</span>doc2<span class="token punctuation">,</span> doc3<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">문서 1과 문서2의 유사도 : 0.6666666666666667
문서 1과 문서3의 유사도 : 0.6666666666666667
문서 2와 문서3의 유사도 : 1.0000000000000002</code></pre></div>
<p>만약 문장의 길이만을 비교할 수 있는 유클리드 거리로 유사도를 계산한다면<br>
이게 무슨말일까 싶을 것이다.</p>
<p>왜냐면 문장은 길이가 비슷하다고 같은 의미를 가지고 있는게 아니지 않는가?</p>
<p>하지만 사용하는 단어가 유사하다면, 유사성을 가지고 있다고 표현할 수 있다.</p>
<p>코사인 유사도란, 행렬의 수치 하나당 방향성을 가지고 있다고 가정하고,<br>
그 방향의 코사인 값을 취해 각 문서가 얼마나 동일한 방향성을 가지고 있는지를 나타내는 방법이다.</p>
<p>방향이 정반대일때는 -1
같은 방향일때는 1의 값을 가진다.</p>
<p>위의 코드를 보면 문서 2와 3의 유사도가 1이 나왔다.
왜냐하면 [1,0,1,1] 과 [2,0,2,2] 가 크기는 다르지만 방향이 같다고 인식했기<br>
때문이다.</p>
<hr>
<p>DTM 은 두가지 큰 문제점이 있다.</p>
<ol>
<li>저장공간을 0이 많이 차지한다.</li>
<li>단순 빈도 수 만을 고려한다.</li>
</ol>
<p>이를 해결하기 위해 TF-IDF 가중치가 나타났다.</p>
<hr>
<h2 id="3-tf-idf" style="position:relative;"><a href="#3-tf-idf" aria-label="3 tf idf permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3. TF-IDF</h2>
<p>; Term Frequency- Inverse Document Frequency</p>
<p>단어 빈도 - 역 문서 빈도 를 뜻한다.</p>
<p>DTM 은 TF 의 행렬이다. 그러므로</p>
<p>기존 DTM 의 각 TF 에 IDF 를 곱하는 것이 TF-IDF가중치 의 사용법이다</p>
<p>DF(문서빈도) 는 해당 단어가 나온 문서의 빈도를 의미한다.
상단 예시에서 <code class="language-text">business</code> 의 DF 는 2이다. 3문서 중 2문서에 출현했기 때문이다.</p>
<p>그래서 공식을 이용하면 다음과 같다.</p>
<p><img src="/a9773941abe596c2dba700a3ee08bd95/image2.png"></img>
문서 1과 2에 나왔으므로</p>
<p>각 문서 별로 DF-IDF 값이 나온다, 각 문서당 TF 가 다르기 때문이다.</p>
<p>허나 여기선 똑같은 1이므로</p>
<p><img src="/1b3dc723c5e2a3749ff1084a2eb6e148/image3.png"></img>
이 된다.</p>
<p>직접 TF-IDF 를 사용해보도록 하겠다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> math <span class="token keyword">import</span> log
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

docs <span class="token operator">=</span> <span class="token punctuation">[</span>
  <span class="token string">'Ib wanna get some candy from Gary'</span><span class="token punctuation">,</span>
  <span class="token string">'Gary protect Ib to save the art gallary'</span><span class="token punctuation">,</span>
  <span class="token string">'Mary also likes to see Ib eating the candy'</span><span class="token punctuation">,</span>  
<span class="token punctuation">]</span>

vocab <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>w <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs <span class="token keyword">for</span> w <span class="token keyword">in</span> doc<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
vocab<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>
N <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>docs<span class="token punctuation">)</span> <span class="token comment"># 총 문서의 수</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'단어장의 크기 :'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">단어장의 크기 : 18
['Gary', 'Ib', 'Mary', 'also', 'art', 'candy', 'eating', 'from', 'gallary', 'get', 'likes', 'protect', 'save', 'see', 'some', 'the', 'to', 'wanna']</code></pre></div>
<p>Python 에서는 모든 식에 에러가 나지 않도록 조정이 필요하다.</p>
<ol>
<li>IDF 의 분모가 0이 되는 상황 방지 : 분모 +1</li>
<li>log 의 분모분자가 같아 1이 되어 IDF 가 0이 되는 상황 방지 : IDF + 1</li>
</ol>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">

<span class="token keyword">def</span> <span class="token function">tf</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#단어빈도</span>
    <span class="token keyword">return</span> d<span class="token punctuation">.</span>count<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
 
<span class="token keyword">def</span> <span class="token function">idf</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#역문서빈도</span>
    df <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">:</span>
        df <span class="token operator">+=</span> t <span class="token keyword">in</span> doc    
    <span class="token keyword">return</span> log<span class="token punctuation">(</span>N<span class="token operator">/</span><span class="token punctuation">(</span>df <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span> <span class="token comment">#분모와 log 에 각각 1 더해줌</span>
 
<span class="token keyword">def</span> <span class="token function">tfidf</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">(</span>t<span class="token punctuation">,</span>d<span class="token punctuation">)</span><span class="token operator">*</span> idf<span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token comment">#실제 계산</span></code></pre></div>
<p>이전 문서를 가지고 DTM 을 만들면 다음과 같다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 각 문서에 대해서 아래 명령을 수행</span>
    result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    d <span class="token operator">=</span> docs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        t <span class="token operator">=</span> vocab<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
        
        result<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">(</span>t<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
tf_ <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>result<span class="token punctuation">,</span> columns <span class="token operator">=</span> vocab<span class="token punctuation">)</span>
tf_</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre></div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gary</th>
      <th>Ib</th>
      <th>Mary</th>
      <th>also</th>
      <th>art</th>
      <th>candy</th>
      <th>eating</th>
      <th>from</th>
      <th>gallary</th>
      <th>get</th>
      <th>likes</th>
      <th>protect</th>
      <th>save</th>
      <th>see</th>
      <th>some</th>
      <th>the</th>
      <th>to</th>
      <th>wanna</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<p>각 단어의 IDF 를 구할 수 있다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    t <span class="token operator">=</span> vocab<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
    result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>idf<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span>

idf_ <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>result<span class="token punctuation">,</span> index <span class="token operator">=</span> vocab<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"IDF"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
idf_</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre></div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>IDF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Gary</th>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>Ib</th>
      <td>0.712318</td>
    </tr>
    <tr>
      <th>Mary</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>also</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>art</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>candy</th>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>eating</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>from</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>gallary</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>get</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>likes</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>protect</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>save</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>see</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>some</th>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>the</th>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>to</th>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>wanna</th>
      <td>1.405465</td>
    </tr>
  </tbody>
</table>
</div>
<p>IDF 는 모든 문서에 등장한 단어가 가장 낮은 값을 가진다.<br>
그리고 독립적으로 등장한 단어가 1.4 로 가장 높은 값을 가진다.</p>
<p>위를 보면, 한 번 나오는 단어는 1.4<br>
두 번 나오는 단어는 1<br>
세 번 나오는 단어는 0.7 의 값이 나온다.</p>
<p>각 나온 IDF 값을 각 단어들의 빈도인 TF 와 곱하면 다음과 같다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>
    result<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    d <span class="token operator">=</span> docs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        t <span class="token operator">=</span> vocab<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
        
        result<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>tfidf<span class="token punctuation">(</span>t<span class="token punctuation">,</span>d<span class="token punctuation">)</span><span class="token punctuation">)</span>

tfidf_ <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>result<span class="token punctuation">,</span> columns <span class="token operator">=</span> vocab<span class="token punctuation">)</span>
tfidf_</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre></div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gary</th>
      <th>Ib</th>
      <th>Mary</th>
      <th>also</th>
      <th>art</th>
      <th>candy</th>
      <th>eating</th>
      <th>from</th>
      <th>gallary</th>
      <th>get</th>
      <th>likes</th>
      <th>protect</th>
      <th>save</th>
      <th>see</th>
      <th>some</th>
      <th>the</th>
      <th>to</th>
      <th>wanna</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.712318</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.405465</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.712318</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.712318</td>
      <td>1.405465</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.405465</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>
<p>Gary 의 IDF 를 각 문서의 TF 별로 곱했음을 알 수 있다.</p>
<hr>
<p>그러나 이것은 단어의 의미를 모르는 문제를 편법적으로 해소했을 뿐이지<br>
실제로 단어의 의미를 깨우친 것은 아니다.</p>
<p>그래서 그 다음에 나온 방법이</p>
<p>LSA 이다.</p>
<hr>
<h2 id="4-lsa" style="position:relative;"><a href="#4-lsa" aria-label="4 lsa permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4. LSA</h2>
<p>; Latent Sementic Analysis</p>
<p>숨은 의미 분석</p>
<p>이제부터는 문서 속 단어들간의 관계까지 찾아낼 수 있다.</p>
<p>LSA 는 Singular Value Decompotion 을 이용한다.</p>
<hr>
<h3 id="singular-value-decompotion-특잇값-분해" style="position:relative;"><a href="#singular-value-decompotion-%ED%8A%B9%EC%9E%87%EA%B0%92-%EB%B6%84%ED%95%B4" aria-label="singular value decompotion 특잇값 분해 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>singular Value Decompotion (특잇값 분해)</h3>
<p><img src="/8b3904a7223cbf32836556240078ae75/image4.png"></img>
SVD 특잇값 분해는 고유값 분해의 변형이다.<br>
원래 고유값 변형은 정방 행렬에서만 가능한데</p>
<p>행과 열의 개수가 다른 <code class="language-text">m</code>x<code class="language-text">n</code>의 행렬에서 적당히 해주는 분해를 의미한다.</p>
<p>그러면 이런식으로 아래나 오른쪽이 조금 비는 대각행렬 비스무리가 나온다</p>
<p><img src="/8ab462c5499ae1bb5d0b03a2a0345491/image5.png"></img></p>
<p><code class="language-text">truncated SVD </code> 는 그 분해를 가장 윗부분 <code class="language-text">k</code> 개만 하는 것을 의미한다.<br>
truncated SVD 는 원본 행렬을 복구할 수 없다.</p>
<p>LSA 는 우리가 만든 DTM 에 (혹은 TF-IDF 가중치 처리한 DTM 에 )<br>
truncated SVD 를 실시하여</p>
<p>U : 특이값
S : 특이벡터
V : 특이값</p>
<p>세가지 행렬로 분해할 수 있다.</p>
<p><img src="/995fa8ec4ad6dd2ccb22ea7ae461475d/image6.png"></img>
U :  문서와 관련된 의미를 표현한 행렬
S : 단어와 관련된 의미를 표현한 행렬
V : 각 의미의 중요도를 표현한 행렬</p>
<p>U 는 m(행) x k 의 행렬이다. 이 때 각 행은 문서 표현 벡터이다.
V 는 k x n(열) 의 행렬이다. 이 때 각 열은 단어 표현 벡터이다.</p>
<p>원래 DTM 에서는 단어 벡터의 크기가 m 이었는데 k 로 줄었음을 확인할 수 있다.</p>
<p>V 는 전체 문서로부터 얻어낸 k 개의 주요 특징 이라고 할 수도 있다.</p>
<p>우리는 이 특징(topic) 의 수 k 를 하이퍼 파라미터로 설정하고
Truncated SVD 를 수행할 수 있다.</p>
<hr>
<p>k(주제) x n(단어수) 를 가진 V 에서</p>
<p>각 행(주제) 마다 빈도수가 가장 많은 단어 5개를 꺼내면,<br>
해당 주제의 요약을 얻을 수가 있다.</p>
<p>이를 topic modelig 이라고 한다</p>
<p>topic modeling 은 이 외에도 LDA 가 존재한다.</p>
<hr>
<h2 id="5-lda--latent-dirichlet-alloaction" style="position:relative;"><a href="#5-lda--latent-dirichlet-alloaction" aria-label="5 lda  latent dirichlet alloaction permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>5. LDA ; Latent Dirichlet Alloaction</h2>
<p>잠재 디리클레 할당이라는 뜻이다.</p>
<p>LDA 는 k (topic) 을 이용해</p>
<p>각 topic 의 단어 분포와 (특정 topic 에 이 단어가 나타날 확률)</p>
<p>각 문서의 topic 분포를 추정한다.</p>
<p>주제가 몇개 있을 것임을 가정하고 단어분포를 찾는다는 의미에서 문서생성 과정을 역추정한다고도 말한다.</p>
<p>LDA 의 과정은 다음과 같다.</p>
<ol>
<li>topic 의 수 k 정하기</li>
<li>모든 단어를 k 개의 topic 중 하나에 allocation</li>
<li>다음 단어를 넣을 때 확률과 분포에 따라 올바른 곳에 allocation</li>
</ol>
<hr>
<p>LSA : DTM 의 차원을 축소해 근접 단어끼리 topic 으로 묶는다
LDA : 단어가 특정 topic 에 들어갈 확률을 구해 나눈다.</p>
<hr>
<h2 id="6-비지도-학습-토크나이저-soynlp" style="position:relative;"><a href="#6-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-soynlp" aria-label="6 비지도 학습 토크나이저 soynlp permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>6. 비지도 학습 토크나이저 soynlp</h2>
<p>지금까지의 방식은 모두 띄어쓰기만을 토대로
단어들간의 분류 및 할당을 수행했다</p>
<p>하지만 정확한 단어의 유사도를 파악하기 위해선<br>
애초에 단어를 의미있게 분리해야 하는 작업이<br>
선행되어야 한다.</p>
<p>그래서 이 이후에 나온 방식이</p>
<p>비지도 학습 토크나이저 방식이다.</p>
<p>특히 우리나라 말은 영어처럼 띄어쓰기 토큰화는 절대 안된다.</p>
<p>조사가 다 다르고 어미 어간도 다다르기 때문이다.</p>
<p>사실 영어같은 경우는 띄어쓰기 토큰화를 해도 어느정도 들어맞는다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">en_text <span class="token operator">=</span> <span class="token string">"My favorite fruit is orange because of salty taste and cheap price."</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>en_text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">['My', 'favorite', 'fruit', 'is', 'orange', 'because', 'of', 'salty', 'taste', 'and', 'cheap', 'price.']</code></pre></div>
<p>하지만 한국어는 절대 안되므로 형태소 분석기를 이용한다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> konlpy<span class="token punctuation">.</span>tag <span class="token keyword">import</span> Okt

kor_text <span class="token operator">=</span> <span class="token string">'나 이제는 게임 안하려고, 더 재밌는 걸 찾아버렸지 뭐야. 그것은 바로 인공지능 NLP'</span>


tokenizer <span class="token operator">=</span> Okt<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>morphs<span class="token punctuation">(</span>kor_text<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">['나', '이제', '는', '게임', '안', '하려고', ',', '더', '재밌는', '걸', '찾아', '버렸지', '뭐', '야', '.', '그것', '은', '바로', '인공', '지능', 'NLP']</code></pre></div>
<p>하지만 형태소분석기는 새로운 신조어나 고유명사를 알아내기가 힘들다는 단점이 있다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>morphs<span class="token punctuation">(</span><span class="token string">'어쩔티비 저쩔티비 크크루삥뽕 아무것도 못하쥬'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">['어쩔', '티비', '저', '쩔', '티비', '크', '크루', '삥뽕', '아무', '것', '도', '못하쥬']</code></pre></div>
<p>크크루삥뽕은 하나의 말이지만 분리된 것을 볼 수가 있다.</p>
<p>근데 이런것도 감안해서 크크루삥뽕이라는 말이 여러번 나왔을 때 이걸 하나의 고유명사라고 판단까지 해주는</p>
<p>발전된 형태소 분석기 <code class="language-text">soynlp</code> 가 나왔다.</p>
<hr>
<h3 id="soynlp-사용-방법" style="position:relative;"><a href="#soynlp-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95" aria-label="soynlp 사용 방법 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>soynlp 사용 방법</h3>
<p>그것이 가능한 이유는 비지도 학습을 통해 형태소 분석을 하기 때문이다.</p>
<p><code class="language-text">soynlp</code> 는 내부의 단어 점수표가 있는데, 그 단어점수표는 다음의 두 가지 방식을 활용한다</p>
<ol>
<li>응집확률 (cohesion probability)</li>
<li>브랜칭 엔트로피 (branching entropy)</li>
</ol>
<p>soynlp 는 깃허브에서 예제 말뭉치를 제공하고 있다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> os

<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request

txt_filename <span class="token operator">=</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">'HOME'</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'/aiffel/goingDeeper/data/2016-10-20.txt'</span>
urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span><span class="token string">"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt"</span><span class="token punctuation">,</span>\
                            filename<span class="token operator">=</span>txt_filename<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">('/aiffel/aiffel/goingDeeper/data/2016-10-20.txt',
 &lt;http.client.HTTPMessage at 0x7f2fce2959a0>)</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> soynlp <span class="token keyword">import</span> DoublespaceLineCorpus

<span class="token comment"># 말뭉치에 대해서 다수의 문서로 분리</span>
corpus <span class="token operator">=</span> DoublespaceLineCorpus<span class="token punctuation">(</span>txt_filename<span class="token punctuation">)</span>
<span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">30091</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">i <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> document <span class="token keyword">in</span> corpus<span class="token punctuation">:</span>
  <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span>
    i <span class="token operator">=</span> i<span class="token operator">+</span><span class="token number">1</span>
  <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>
    <span class="token keyword">break</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">19  1990  52 1 22
오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다
테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진</code></pre></div>
<p><code class="language-text">soynlp</code> 는 비지도 학습 형태소 분석기 이기 때문에 해당 corpus 에 대한 학습을<br>
선행시켜주어야 한다.</p>
<p>이 학습 과정에서 응집확률과 브랜칭 엔트로피 단어 점수표를 만든다</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> soynlp<span class="token punctuation">.</span>word <span class="token keyword">import</span> WordExtractor

word_extractor <span class="token operator">=</span> WordExtractor<span class="token punctuation">(</span><span class="token punctuation">)</span>
word_extractor<span class="token punctuation">.</span>train<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
word_score_table <span class="token operator">=</span> word_extractor<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#응집확률과 브랜칭 엔트로피 사용</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">training was done. used memory 1.141 Gb
all cohesion probabilities was computed. # words = 223348
all branching entropies was computed # words = 361598
all accessor variety was computed # words = 361598</code></pre></div>
<h3 id="응집확률--cohesion-probability" style="position:relative;"><a href="#%EC%9D%91%EC%A7%91%ED%99%95%EB%A5%A0--cohesion-probability" aria-label="응집확률  cohesion probability permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>응집확률  cohesion probability</h3>
<p>내부 문자열 substring 이 얼마나 모여 자주 등장하는지 판단하는 기준<br>
이 값이 높을수록 그 주변 글자와 한 단어일 확률이 높아진다.</p>
<p><img src="/1eeea11cf86b9dfd05c9efcfdcb825c7/image7.png"></img>
이처럼 바로 이전에 입력된 글자와의 확률을 구해 함께 오는지 안오는지를 구한다.</p>
<p>다른 곳에서는 ‘반포한강공원’ 과 ‘에’ 의 응집확률이 낮게 나오므로 한 단어 기준이 정해진다.</p>
<p><code class="language-text">word_score_table["string"].cohesion_forward</code>
계산시 ‘반포한강공원’ 의 응집확률이 가장 최대치를 이룬다.</p>
<h3 id="브랜칭-엔트로피" style="position:relative;"><a href="#%EB%B8%8C%EB%9E%9C%EC%B9%AD-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC" aria-label="브랜칭 엔트로피 permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>브랜칭 엔트로피</h3>
<p>주어진 글자에서 다음 글자를 맞추는 데 헷갈리는 정도
브랜칭 엔트로피는 주어지는 글자가 많을 수록 헷갈림이 적으므로 점점 낮아지게 된다.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">word_score_table<span class="token punctuation">[</span><span class="token string">"디스"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>right_branching_entropy</code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">1.6371694761537934</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">word_score_table<span class="token punctuation">[</span><span class="token string">"디스플"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>right_branching_entropy</code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">-0.0</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">word_score_table<span class="token punctuation">[</span><span class="token string">"디스플레"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>right_branching_entropy</code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">-0.0</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">word_score_table<span class="token punctuation">[</span><span class="token string">"디스플레이"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>right_branching_entropy</code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">3.1400392861792916</code></pre></div>
<p>이처럼 브랜칭 엔트로피는 하나의 단어가 끝나가면서 점점 낮아졌다가<br>
단어가 끝나면 조사 등 다른 것이 올 확률이 높아지므로(다시 헷갈려지므로)<br>
수치가 다시 오른다.</p>
<p>이 수치의 고저로 단어를 나눌 수 있다.</p>
<p>결과적으로</p>
<p>응집확률 과 브랜칭 엔트로피 를 사용해</p>
<p>토크나이저 한다.</p>
<p>그런데 soynlp 토크나이저도 두 가지 방식이 있다.</p>
<ol>
<li>L Tokenizer</li>
</ol>
<p>우리나라는 조사가 대부분 오른쪽에 붙는것을 감안해</p>
<p>한 단어를 L+R 로 나누어
L 의 중요도를 더 높인 tokenize</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> soynlp<span class="token punctuation">.</span>tokenizer <span class="token keyword">import</span> LTokenizer

scores <span class="token operator">=</span> <span class="token punctuation">{</span>word<span class="token punctuation">:</span>score<span class="token punctuation">.</span>cohesion_forward <span class="token keyword">for</span> word<span class="token punctuation">,</span> score <span class="token keyword">in</span> word_score_table<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
l_tokenizer <span class="token operator">=</span> LTokenizer<span class="token punctuation">(</span>scores<span class="token operator">=</span>scores<span class="token punctuation">)</span>
l_tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">"국제사회와 우리의 노력들로 범죄를 척결하자"</span><span class="token punctuation">,</span> flatten<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]</code></pre></div>
<p>하나의 tuple 안에 L 과 R 이 들어있는 것을 볼 수 있다.</p>
<ol start="2">
<li>Max score Tokenizer</li>
</ol>
<p>띄어쓰기가 되어있지 않은 문장에서<br>
가장 점수가 높은 단어의 중요도를 더 높인 tokenize</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> soynlp<span class="token punctuation">.</span>tokenizer <span class="token keyword">import</span> MaxScoreTokenizer

maxscore_tokenizer <span class="token operator">=</span> MaxScoreTokenizer<span class="token punctuation">(</span>scores<span class="token operator">=</span>scores<span class="token punctuation">)</span>
maxscore_tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">"국제사회와우리의노력들로범죄를척결하자"</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']</code></pre></div>
<p>해당 input 을 보면 띄어쓰기를 하지 않았음에도 충실히 나누게 된 것을 볼 수 있다.</p>
<div class="table-of-contents">
<ul>
<li>
<p><a href="#1-bag-of-words-%EB%8B%A8%EC%96%B4%ED%91%9C%ED%98%84-%EB%B0%A9%EB%B2%95">1. Bag of Words 단어표현 방법</a></p>
</li>
<li>
<p><a href="#2-dtm--document-term-matrix">2. DTM ; document-Term Matrix</a></p>
<ul>
<li><a href="#kosine-%EC%9C%A0%EC%82%AC%EB%8F%84">Kosine 유사도</a></li>
</ul>
</li>
<li>
<p><a href="#3-tf-idf">3. TF-IDF</a></p>
</li>
<li>
<p><a href="#4-lsa">4. LSA</a></p>
<ul>
<li><a href="#singular-value-decompotion-%ED%8A%B9%EC%9E%87%EA%B0%92-%EB%B6%84%ED%95%B4">singular Value Decompotion (특잇값 분해)</a></li>
</ul>
</li>
<li>
<p><a href="#5-lda--latent-dirichlet-alloaction">5. LDA ; Latent Dirichlet Alloaction</a></p>
</li>
<li>
<p><a href="#6-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-soynlp">6. 비지도 학습 토크나이저 soynlp</a></p>
<ul>
<li><a href="#soynlp-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95">soynlp 사용 방법</a></li>
<li><a href="#%EC%9D%91%EC%A7%91%ED%99%95%EB%A5%A0--cohesion-probability">응집확률  cohesion probability</a></li>
<li><a href="#%EB%B8%8C%EB%9E%9C%EC%B9%AD-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC">브랜칭 엔트로피</a></li>
</ul>
</li>
</ul>
</div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"></code></pre></div></div></div><div class="post-navigator"><div class="post-navigator-card-wrapper"><a class="post-card prev" href="/NLP_2/"><div class="direction">이전 글</div><div class="title">SentencePiece Tokenizer 사용 방법</div></a></div><div class="post-navigator-card-wrapper"><a class="post-card next" href="/DML_AI3/"><div class="direction">다음 글</div><div class="title">Regression 회귀</div></a></div></div><div class="utterances"></div></main><footer class="page-footer-wrapper"><p class="page-footer">© <!-- -->2023<!-- --> <a href="https://github.com/xman227">하성민</a> powered by<a href="https://github.com/zoomKoding/zoomkoding-gatsby-blog"> zoomkoding-gatsby-blog</a></p></footer><div class="dark-mode-button-wrapper"><button class="MuiButtonBase-root MuiIconButton-root MuiIconButton-sizeMedium dark-mode-button css-1yxmbwk" tabindex="0" type="button"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeLarge dark-mode-icon css-6flbmm" focusable="false" viewBox="0 0 24 24" aria-hidden="true" data-testid="DarkModeIcon"><path d="M12 3c-4.97 0-9 4.03-9 9s4.03 9 9 9 9-4.03 9-9c0-.46-.04-.92-.1-1.36-.98 1.37-2.58 2.26-4.4 2.26-2.98 0-5.4-2.42-5.4-5.4 0-1.81.89-3.42 2.26-4.4-.44-.06-.9-.1-1.36-.1z"></path></svg></button></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/NLP_3/";window.___webpackCompilationHash="d9a4279f0172f4cfd24a";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-9b356b5dc44213e24f34.js"],"app":["/app-8340b64cb5b3e506fb78.js"],"component---cache-caches-gatsby-plugin-offline-app-shell-js":["/component---cache-caches-gatsby-plugin-offline-app-shell-js-ffdb1e83dd2925d16ce3.js"],"component---src-pages-404-js":["/component---src-pages-404-js-bc51420b294b9123f97d.js"],"component---src-pages-about-js":["/component---src-pages-about-js-39e57401fb032eafb2fb.js"],"component---src-pages-index-js":["/component---src-pages-index-js-0a6bbda26eb501968935.js"],"component---src-templates-blog-template-js":["/component---src-templates-blog-template-js-94a4cd73c7c267c46a0e.js"],"component---src-templates-category-template-js":["/component---src-templates-category-template-js-a0af7e239c6cf28d9bb9.js"]};/*]]>*/</script><script src="/polyfill-9b356b5dc44213e24f34.js" nomodule=""></script><script src="/component---src-templates-blog-template-js-94a4cd73c7c267c46a0e.js" async=""></script><script src="/f9d3028dbef90a6e9b8db85387d63dd9f4edf538-e4cfa69055e2f9894560.js" async=""></script><script src="/app-8340b64cb5b3e506fb78.js" async=""></script><script src="/framework-71a91a8132c4a176c255.js" async=""></script><script src="/webpack-runtime-9dfb7ebabee66c506236.js" async=""></script></body></html>