{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/NLP_2/",
    "result": {"data":{"cur":{"id":"a5f56b6d-f700-59e0-8291-651faebf1202","html":"<h1 id=\"-네이버-영화리뷰-감성분석에-sentencepiece-적용해보기span\" style=\"position:relative;\"><a href=\"#-%EB%84%A4%EC%9D%B4%EB%B2%84-%EC%98%81%ED%99%94%EB%A6%AC%EB%B7%B0-%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D%EC%97%90-sentencepiece-%EC%A0%81%EC%9A%A9%ED%95%B4%EB%B3%B4%EA%B8%B0span\" aria-label=\" 네이버 영화리뷰 감성분석에 sentencepiece 적용해보기span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🙄 네이버 영화리뷰 감성분석에 SentencePiece 적용해보기</span></h1>\n<p>&#x3C;NLP기초></p>\n<h2 id=\"contexts\" style=\"position:relative;\"><a href=\"#contexts\" aria-label=\"contexts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Contexts</h2>\n<h3 id=\"1-ready\" style=\"position:relative;\"><a href=\"#1-ready\" aria-label=\"1 ready permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. READY</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1-1 오늘의 Exp와 Rubric  \n1-2 사용하는 라이브러리  </code></pre></div>\n<h3 id=\"2-game\" style=\"position:relative;\"><a href=\"#2-game\" aria-label=\"2 game permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. GAME</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">2-1. 데이터 읽어오기  \n2-2. 데이터 전처리  \n  -1. Tokenize (SentencePiece)\n  -2. 학습데이터 전처리\n  -3. Split Validation \n\n2-3. 모델 학습  \n2-4. 데이터 평가   </code></pre></div>\n<h3 id=\"3-potg-best-play-of-the-game\" style=\"position:relative;\"><a href=\"#3-potg-best-play-of-the-game\" aria-label=\"3 potg best play of the game permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. POTG (best Play Of The Game</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">3-1. 소감(POTG)  \n3-2. 어려웠던 점과 극복방안  \n3-3. 추후  </code></pre></div>\n<hr>\n<h1 id=\"1-ready-1\" style=\"position:relative;\"><a href=\"#1-ready-1\" aria-label=\"1 ready 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Ready</h1>\n<h2 id=\"1-1-오늘의-exp와-rubric\" style=\"position:relative;\"><a href=\"#1-1-%EC%98%A4%EB%8A%98%EC%9D%98-exp%EC%99%80-rubric\" aria-label=\"1 1 오늘의 exp와 rubric permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1-1. 오늘의 Exp와 Rubric</h2>\n<p><a href=\"https://github.com/google/sentencepiece\">SentencePiece</a> 는 Google 에서 제공하고 있는 Tokenizer / Detokenizer 이다.</p>\n<p>Tokenize 란 NLP 에서 중요한 부분인 ‘단어사전 제작’ 을 의미한다.</p>\n<p>직관적으로 생각했을 때, 단어사전은 단어별, 형태소별, 혹은 그 사이 어떤 경계를 나누어 만들 수 있다.</p>\n<p>Sentencepiece 는\nBPE 와 unigram 이라는 두 가지의 분리 방법을 통해 subword tokenizing model 을 제공하고 있다.</p>\n<p>최근 pretrained model 은 대부분 SentencePiece 를 Tokenizer 로 설정하는 추세이기에 NLP 분야 tokenizer 의 표준이라고 표현해도 과언이 아니다.</p>\n<p>오늘은 이러한 SentencePiece 를 끌어와 사용하는 것까지를 실습해보기로 한다.</p>\n<p>실습에 쓰이는 데이터는<br>\nSentencePiece 토크나이저를 학습시킬 <a href=\"https://github.com/jungyeul/korean-parallel-corpora\">한국어 corpus</a> 와<br>\n모델을 학습시킬  <a href=\"https://github.com/e9t/nsmc/blob/master/ratings_test.txt\">Naver_Moive txt data</a> 로 한다.</p>\n<p>오늘의 rubric</p>\n<table>\n<thead>\n<tr>\n<th>평가문항</th>\n<th>상세기준</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1. SentencePiece를 이용하여 모델을 만들기까지의 과정이 정상적으로 진행되었는가?</td>\n<td>코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작이 빠짐없이 진행되었는가?</td>\n</tr>\n<tr>\n<td>2. SentencePiece를 통해 만든 Tokenizer가 자연어처리 모델과 결합하여 동작하는가?</td>\n<td>SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인되었다.</td>\n</tr>\n<tr>\n<td>3. SentencePiece의 성능을 다각도로 비교분석하였는가?</td>\n<td>SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"1-2-사용하는-라이브러리\" style=\"position:relative;\"><a href=\"#1-2-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"1 2 사용하는 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1-2. 사용하는 라이브러리</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">! python3 <span class=\"token operator\">-</span><span class=\"token operator\">-</span>version</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Python 3.7.12</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#import konlpy 단순 import 는 에러 발생</span></code></pre></div>\n<p>시작 하기전 Konlpy 라이브러리에 특이사항이 있다.<br>\ncolab에서는 Konlpy 를 install 할때 별도의 과정을 거쳐야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">!apt<span class=\"token operator\">-</span>get update\n!apt<span class=\"token operator\">-</span>get install g<span class=\"token operator\">+</span><span class=\"token operator\">+</span> openjdk<span class=\"token operator\">-</span><span class=\"token number\">8</span><span class=\"token operator\">-</span>jdk \n!pip3 install konlpy JPype1<span class=\"token operator\">-</span>py3\n!bash <span class=\"token operator\">&lt;</span><span class=\"token punctuation\">(</span>curl <span class=\"token operator\">-</span>s https<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>raw<span class=\"token punctuation\">.</span>githubusercontent<span class=\"token punctuation\">.</span>com<span class=\"token operator\">/</span>konlpy<span class=\"token operator\">/</span>konlpy<span class=\"token operator\">/</span>master<span class=\"token operator\">/</span>scripts<span class=\"token operator\">/</span>mecab<span class=\"token punctuation\">.</span>sh<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">import</span> konlpy</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\nIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\nGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [80.4 kB]\nHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\nIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\nGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\nGet:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\nGet:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\nHit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\nGet:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nGet:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [934 kB]\nHit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nGet:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\nGet:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,098 kB]\nGet:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\nGet:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [860 kB]\nHit:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nGet:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.9 kB]\nGet:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,262 kB]\nGet:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [893 kB]\nGet:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,829 kB]\nGet:24 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,484 kB]\nGet:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,660 kB]\nGet:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [937 kB]\nGet:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.2 kB]\nFetched 15.4 MB in 7s (2,158 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ng++ is already the newest version (4:7.4.0-1ubuntu2.3).\ng++ set to manually installed.\nThe following additional packages will be installed:\n  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n  libgtk2.0-common libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n  openjdk-8-jre-headless x11-utils\nSuggested packages:\n  gvfs openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n  fonts-wqy-zenhei fonts-indic mesa-utils\nThe following NEW packages will be installed:\n  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n  libgtk2.0-common libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n  openjdk-8-jre openjdk-8-jre-headless x11-utils\n0 upgraded, 15 newly installed, 0 to remove and 69 not upgraded.\nNeed to get 43.5 MB of archives.\nAfter this operation, 163 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\nGet:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\nGet:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\nGet:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\nGet:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\nGet:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\nGet:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\nGet:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\nGet:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u312-b07-0ubuntu1~18.04 [28.2 MB]\nGet:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u312-b07-0ubuntu1~18.04 [69.6 kB]\nGet:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u312-b07-0ubuntu1~18.04 [8,298 kB]\nGet:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u312-b07-0ubuntu1~18.04 [1,625 kB]\nFetched 43.5 MB in 4s (10.1 MB/s)\nSelecting previously unselected package libxxf86dga1:amd64.\n(Reading database ... 155335 files and directories currently installed.)\nPreparing to unpack .../00-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\nUnpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\nSelecting previously unselected package fonts-dejavu-core.\nPreparing to unpack .../01-fonts-dejavu-core_2.37-1_all.deb ...\nUnpacking fonts-dejavu-core (2.37-1) ...\nSelecting previously unselected package fonts-dejavu-extra.\nPreparing to unpack .../02-fonts-dejavu-extra_2.37-1_all.deb ...\nUnpacking fonts-dejavu-extra (2.37-1) ...\nSelecting previously unselected package x11-utils.\nPreparing to unpack .../03-x11-utils_7.7+3build1_amd64.deb ...\nUnpacking x11-utils (7.7+3build1) ...\nSelecting previously unselected package libatk-wrapper-java.\nPreparing to unpack .../04-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\nUnpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\nSelecting previously unselected package libatk-wrapper-java-jni:amd64.\nPreparing to unpack .../05-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\nUnpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\nSelecting previously unselected package libgtk2.0-common.\nPreparing to unpack .../06-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\nUnpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\nSelecting previously unselected package libgtk2.0-0:amd64.\nPreparing to unpack .../07-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\nUnpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\nSelecting previously unselected package libgail18:amd64.\nPreparing to unpack .../08-libgail18_2.24.32-1ubuntu1_amd64.deb ...\nUnpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\nSelecting previously unselected package libgail-common:amd64.\nPreparing to unpack .../09-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\nUnpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\nSelecting previously unselected package libgtk2.0-bin.\nPreparing to unpack .../10-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\nUnpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\nSelecting previously unselected package openjdk-8-jre-headless:amd64.\nPreparing to unpack .../11-openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\nUnpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\nSelecting previously unselected package openjdk-8-jre:amd64.\nPreparing to unpack .../12-openjdk-8-jre_8u312-b07-0ubuntu1~18.04_amd64.deb ...\nUnpacking openjdk-8-jre:amd64 (8u312-b07-0ubuntu1~18.04) ...\nSelecting previously unselected package openjdk-8-jdk-headless:amd64.\nPreparing to unpack .../13-openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\nUnpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\nSelecting previously unselected package openjdk-8-jdk:amd64.\nPreparing to unpack .../14-openjdk-8-jdk_8u312-b07-0ubuntu1~18.04_amd64.deb ...\nUnpacking openjdk-8-jdk:amd64 (8u312-b07-0ubuntu1~18.04) ...\nSetting up libgtk2.0-common (2.24.32-1ubuntu1) ...\nSetting up fonts-dejavu-core (2.37-1) ...\nSetting up libxxf86dga1:amd64 (2:1.1.4-1) ...\nSetting up fonts-dejavu-extra (2.37-1) ...\nSetting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\nSetting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\nSetting up libgail18:amd64 (2.24.32-1ubuntu1) ...\nSetting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\nSetting up x11-utils (7.7+3build1) ...\nSetting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\nSetting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\nSetting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\nSetting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\nSetting up openjdk-8-jre:amd64 (8u312-b07-0ubuntu1~18.04) ...\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\nSetting up openjdk-8-jdk:amd64 (8u312-b07-0ubuntu1~18.04) ...\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for fontconfig (2.12.6-0ubuntu2) ...\nProcessing triggers for mime-support (3.60ubuntu1) ...\nProcessing triggers for libc-bin (2.27-3ubuntu1.3) ...\n/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n\nCollecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[K     |████████████████████████████████| 19.4 MB 516 kB/s \n\u001b[?25hCollecting JPype1-py3\n  Downloading JPype1-py3-0.5.5.4.tar.gz (88 kB)\n\u001b[K     |████████████████████████████████| 88 kB 7.6 MB/s \n\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\nCollecting JPype1>=0.7.0\n  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n\u001b[K     |████████████████████████████████| 448 kB 53.3 MB/s \n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\nBuilding wheels for collected packages: JPype1-py3\n  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp37-cp37m-linux_x86_64.whl size=2679864 sha256=06f1d436f3b329b086bc2b1f59e365e9853f59f6d0a60a7c75c2fd742e3ad1b8\n  Stored in directory: /root/.cache/pip/wheels/e7/d1/09/f55dca0203b0691945bdf0f63d486a0b4d4e5ec4bd78a2502e\nSuccessfully built JPype1-py3\nInstalling collected packages: JPype1, konlpy, JPype1-py3\nSuccessfully installed JPype1-1.3.0 JPype1-py3-0.5.5.4 konlpy-0.6.0\nInstalling automake (A dependency for mecab-ko)\nHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\nIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\nIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\nHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\nHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nHit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\nHit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\nHit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\nHit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\nHit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nHit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\nHit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\nHit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  autoconf autotools-dev libsigsegv2 m4\nSuggested packages:\n  autoconf-archive gnu-standards autoconf-doc libtool gettext m4-doc\nThe following NEW packages will be installed:\n  autoconf automake autotools-dev libsigsegv2 m4\n0 upgraded, 5 newly installed, 0 to remove and 69 not upgraded.\nNeed to get 1,082 kB of archives.\nAfter this operation, 3,994 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\nGet:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\nFetched 1,082 kB in 2s (500 kB/s)\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;> line 5.)\ndebconf: falling back to frontend: Readline\ndebconf: unable to initialize frontend: Readline\ndebconf: (This frontend requires a controlling tty.)\ndebconf: falling back to frontend: Teletype\ndpkg-preconfigure: unable to re-open stdin: \nSelecting previously unselected package libsigsegv2:amd64.\n(Reading database ... 155911 files and directories currently installed.)\nPreparing to unpack .../libsigsegv2_2.12-1_amd64.deb ...\nUnpacking libsigsegv2:amd64 (2.12-1) ...\nSelecting previously unselected package m4.\nPreparing to unpack .../archives/m4_1.4.18-1_amd64.deb ...\nUnpacking m4 (1.4.18-1) ...\nSelecting previously unselected package autoconf.\nPreparing to unpack .../autoconf_2.69-11_all.deb ...\nUnpacking autoconf (2.69-11) ...\nSelecting previously unselected package autotools-dev.\nPreparing to unpack .../autotools-dev_20180224.1_all.deb ...\nUnpacking autotools-dev (20180224.1) ...\nSelecting previously unselected package automake.\nPreparing to unpack .../automake_1%3a1.15.1-3ubuntu2_all.deb ...\nUnpacking automake (1:1.15.1-3ubuntu2) ...\nSetting up libsigsegv2:amd64 (2.12-1) ...\nSetting up m4 (1.4.18-1) ...\nSetting up autotools-dev (20180224.1) ...\nSetting up autoconf (2.69-11) ...\nSetting up automake (1:1.15.1-3ubuntu2) ...\nupdate-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1.3) ...\n/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\nInstall mecab-ko\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 1381k  100 1381k    0     0   496k      0  0:00:02  0:00:02 --:--:-- 1243k\nmecab-0.996-ko-0.9.2/\nmecab-0.996-ko-0.9.2/example/\nmecab-0.996-ko-0.9.2/example/example.cpp\nmecab-0.996-ko-0.9.2/example/example_lattice.cpp\nmecab-0.996-ko-0.9.2/example/example_lattice.c\nmecab-0.996-ko-0.9.2/example/example.c\nmecab-0.996-ko-0.9.2/example/thread_test.cpp\nmecab-0.996-ko-0.9.2/mecab-config.in\nmecab-0.996-ko-0.9.2/man/\nmecab-0.996-ko-0.9.2/man/Makefile.am\nmecab-0.996-ko-0.9.2/man/mecab.1\nmecab-0.996-ko-0.9.2/man/Makefile.in\nmecab-0.996-ko-0.9.2/mecab.iss.in\nmecab-0.996-ko-0.9.2/config.guess\nmecab-0.996-ko-0.9.2/README\nmecab-0.996-ko-0.9.2/COPYING\nmecab-0.996-ko-0.9.2/CHANGES.md\nmecab-0.996-ko-0.9.2/README.md\nmecab-0.996-ko-0.9.2/INSTALL\nmecab-0.996-ko-0.9.2/config.sub\nmecab-0.996-ko-0.9.2/configure.in\nmecab-0.996-ko-0.9.2/swig/\nmecab-0.996-ko-0.9.2/swig/Makefile\nmecab-0.996-ko-0.9.2/swig/version.h.in\nmecab-0.996-ko-0.9.2/swig/version.h\nmecab-0.996-ko-0.9.2/swig/MeCab.i\nmecab-0.996-ko-0.9.2/aclocal.m4\nmecab-0.996-ko-0.9.2/LGPL\nmecab-0.996-ko-0.9.2/Makefile.am\nmecab-0.996-ko-0.9.2/configure\nmecab-0.996-ko-0.9.2/tests/\nmecab-0.996-ko-0.9.2/tests/autolink/\nmecab-0.996-ko-0.9.2/tests/autolink/unk.def\nmecab-0.996-ko-0.9.2/tests/autolink/dicrc\nmecab-0.996-ko-0.9.2/tests/autolink/dic.csv\nmecab-0.996-ko-0.9.2/tests/autolink/test\nmecab-0.996-ko-0.9.2/tests/autolink/char.def\nmecab-0.996-ko-0.9.2/tests/autolink/matrix.def\nmecab-0.996-ko-0.9.2/tests/autolink/test.gld\nmecab-0.996-ko-0.9.2/tests/t9/\nmecab-0.996-ko-0.9.2/tests/t9/unk.def\nmecab-0.996-ko-0.9.2/tests/t9/ipadic.pl\nmecab-0.996-ko-0.9.2/tests/t9/dicrc\nmecab-0.996-ko-0.9.2/tests/t9/dic.csv\nmecab-0.996-ko-0.9.2/tests/t9/test\nmecab-0.996-ko-0.9.2/tests/t9/char.def\nmecab-0.996-ko-0.9.2/tests/t9/matrix.def\nmecab-0.996-ko-0.9.2/tests/t9/mkdic.pl\nmecab-0.996-ko-0.9.2/tests/t9/test.gld\nmecab-0.996-ko-0.9.2/tests/cost-train/\nmecab-0.996-ko-0.9.2/tests/cost-train/ipa.train\nmecab-0.996-ko-0.9.2/tests/cost-train/ipa.test\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/rewrite.def\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/feature.def\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/unk.def\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/dicrc\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/dic.csv\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/char.def\nmecab-0.996-ko-0.9.2/tests/cost-train/seed/matrix.def\nmecab-0.996-ko-0.9.2/tests/run-eval.sh\nmecab-0.996-ko-0.9.2/tests/run-cost-train.sh\nmecab-0.996-ko-0.9.2/tests/Makefile.am\nmecab-0.996-ko-0.9.2/tests/katakana/\nmecab-0.996-ko-0.9.2/tests/katakana/unk.def\nmecab-0.996-ko-0.9.2/tests/katakana/dicrc\nmecab-0.996-ko-0.9.2/tests/katakana/dic.csv\nmecab-0.996-ko-0.9.2/tests/katakana/test\nmecab-0.996-ko-0.9.2/tests/katakana/char.def\nmecab-0.996-ko-0.9.2/tests/katakana/matrix.def\nmecab-0.996-ko-0.9.2/tests/katakana/test.gld\nmecab-0.996-ko-0.9.2/tests/eval/\nmecab-0.996-ko-0.9.2/tests/eval/answer\nmecab-0.996-ko-0.9.2/tests/eval/system\nmecab-0.996-ko-0.9.2/tests/eval/test.gld\nmecab-0.996-ko-0.9.2/tests/shiin/\nmecab-0.996-ko-0.9.2/tests/shiin/unk.def\nmecab-0.996-ko-0.9.2/tests/shiin/dicrc\nmecab-0.996-ko-0.9.2/tests/shiin/dic.csv\nmecab-0.996-ko-0.9.2/tests/shiin/test\nmecab-0.996-ko-0.9.2/tests/shiin/char.def\nmecab-0.996-ko-0.9.2/tests/shiin/matrix.def\nmecab-0.996-ko-0.9.2/tests/shiin/mkdic.pl\nmecab-0.996-ko-0.9.2/tests/shiin/test.gld\nmecab-0.996-ko-0.9.2/tests/latin/\nmecab-0.996-ko-0.9.2/tests/latin/unk.def\nmecab-0.996-ko-0.9.2/tests/latin/dicrc\nmecab-0.996-ko-0.9.2/tests/latin/dic.csv\nmecab-0.996-ko-0.9.2/tests/latin/test\nmecab-0.996-ko-0.9.2/tests/latin/char.def\nmecab-0.996-ko-0.9.2/tests/latin/matrix.def\nmecab-0.996-ko-0.9.2/tests/latin/test.gld\nmecab-0.996-ko-0.9.2/tests/chartype/\nmecab-0.996-ko-0.9.2/tests/chartype/unk.def\nmecab-0.996-ko-0.9.2/tests/chartype/dicrc\nmecab-0.996-ko-0.9.2/tests/chartype/dic.csv\nmecab-0.996-ko-0.9.2/tests/chartype/test\nmecab-0.996-ko-0.9.2/tests/chartype/char.def\nmecab-0.996-ko-0.9.2/tests/chartype/matrix.def\nmecab-0.996-ko-0.9.2/tests/chartype/test.gld\nmecab-0.996-ko-0.9.2/tests/run-dics.sh\nmecab-0.996-ko-0.9.2/tests/ngram/\nmecab-0.996-ko-0.9.2/tests/ngram/unk.def\nmecab-0.996-ko-0.9.2/tests/ngram/dicrc\nmecab-0.996-ko-0.9.2/tests/ngram/dic.csv\nmecab-0.996-ko-0.9.2/tests/ngram/test\nmecab-0.996-ko-0.9.2/tests/ngram/char.def\nmecab-0.996-ko-0.9.2/tests/ngram/matrix.def\nmecab-0.996-ko-0.9.2/tests/ngram/test.gld\nmecab-0.996-ko-0.9.2/tests/Makefile.in\nmecab-0.996-ko-0.9.2/ltmain.sh\nmecab-0.996-ko-0.9.2/config.rpath\nmecab-0.996-ko-0.9.2/config.h.in\nmecab-0.996-ko-0.9.2/mecabrc.in\nmecab-0.996-ko-0.9.2/GPL\nmecab-0.996-ko-0.9.2/Makefile.train\nmecab-0.996-ko-0.9.2/ChangeLog\nmecab-0.996-ko-0.9.2/install-sh\nmecab-0.996-ko-0.9.2/AUTHORS\nmecab-0.996-ko-0.9.2/doc/\nmecab-0.996-ko-0.9.2/doc/bindings.html\nmecab-0.996-ko-0.9.2/doc/posid.html\nmecab-0.996-ko-0.9.2/doc/unk.html\nmecab-0.996-ko-0.9.2/doc/learn.html\nmecab-0.996-ko-0.9.2/doc/format.html\nmecab-0.996-ko-0.9.2/doc/libmecab.html\nmecab-0.996-ko-0.9.2/doc/mecab.css\nmecab-0.996-ko-0.9.2/doc/feature.html\nmecab-0.996-ko-0.9.2/doc/Makefile.am\nmecab-0.996-ko-0.9.2/doc/soft.html\nmecab-0.996-ko-0.9.2/doc/en/\nmecab-0.996-ko-0.9.2/doc/en/bindings.html\nmecab-0.996-ko-0.9.2/doc/dic-detail.html\nmecab-0.996-ko-0.9.2/doc/flow.png\nmecab-0.996-ko-0.9.2/doc/mecab.html\nmecab-0.996-ko-0.9.2/doc/index.html\nmecab-0.996-ko-0.9.2/doc/result.png\nmecab-0.996-ko-0.9.2/doc/doxygen/\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_a.png\nmecab-0.996-ko-0.9.2/doc/doxygen/globals_eval.html\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/functions_vars.html\nmecab-0.996-ko-0.9.2/doc/doxygen/doxygen.css\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_r.gif\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice.html\nmecab-0.996-ko-0.9.2/doc/doxygen/functions.html\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger.html\nmecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h_source.html\nmecab-0.996-ko-0.9.2/doc/doxygen/tabs.css\nmecab-0.996-ko-0.9.2/doc/doxygen/nav_f.png\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_b.png\nmecab-0.996-ko-0.9.2/doc/doxygen/globals.html\nmecab-0.996-ko-0.9.2/doc/doxygen/nav_h.png\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_h.png\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model.html\nmecab-0.996-ko-0.9.2/doc/doxygen/globals_func.html\nmecab-0.996-ko-0.9.2/doc/doxygen/closed.png\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_l.gif\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/functions_func.html\nmecab-0.996-ko-0.9.2/doc/doxygen/globals_type.html\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t.html\nmecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_func.html\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_s.png\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_type.html\nmecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t.html\nmecab-0.996-ko-0.9.2/doc/doxygen/namespaces.html\nmecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers.html\nmecab-0.996-ko-0.9.2/doc/doxygen/namespaceMeCab.html\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t.html\nmecab-0.996-ko-0.9.2/doc/doxygen/files.html\nmecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t-members.html\nmecab-0.996-ko-0.9.2/doc/doxygen/index.html\nmecab-0.996-ko-0.9.2/doc/doxygen/annotated.html\nmecab-0.996-ko-0.9.2/doc/doxygen/globals_defs.html\nmecab-0.996-ko-0.9.2/doc/doxygen/classes.html\nmecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h-source.html\nmecab-0.996-ko-0.9.2/doc/doxygen/doxygen.png\nmecab-0.996-ko-0.9.2/doc/doxygen/tab_b.gif\nmecab-0.996-ko-0.9.2/doc/doxygen/bc_s.png\nmecab-0.996-ko-0.9.2/doc/doxygen/open.png\nmecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h.html\nmecab-0.996-ko-0.9.2/doc/dic.html\nmecab-0.996-ko-0.9.2/doc/partial.html\nmecab-0.996-ko-0.9.2/doc/feature.png\nmecab-0.996-ko-0.9.2/doc/Makefile.in\nmecab-0.996-ko-0.9.2/missing\nmecab-0.996-ko-0.9.2/BSD\nmecab-0.996-ko-0.9.2/NEWS\nmecab-0.996-ko-0.9.2/mkinstalldirs\nmecab-0.996-ko-0.9.2/src/\nmecab-0.996-ko-0.9.2/src/dictionary.h\nmecab-0.996-ko-0.9.2/src/writer.h\nmecab-0.996-ko-0.9.2/src/utils.h\nmecab-0.996-ko-0.9.2/src/string_buffer.cpp\nmecab-0.996-ko-0.9.2/src/tokenizer.cpp\nmecab-0.996-ko-0.9.2/src/make.bat\nmecab-0.996-ko-0.9.2/src/mecab.h\nmecab-0.996-ko-0.9.2/src/freelist.h\nmecab-0.996-ko-0.9.2/src/string_buffer.h\nmecab-0.996-ko-0.9.2/src/learner_tagger.h\nmecab-0.996-ko-0.9.2/src/dictionary_compiler.cpp\nmecab-0.996-ko-0.9.2/src/eval.cpp\nmecab-0.996-ko-0.9.2/src/mecab-system-eval.cpp\nmecab-0.996-ko-0.9.2/src/darts.h\nmecab-0.996-ko-0.9.2/src/param.h\nmecab-0.996-ko-0.9.2/src/char_property.h\nmecab-0.996-ko-0.9.2/src/learner_node.h\nmecab-0.996-ko-0.9.2/src/mecab-dict-gen.cpp\nmecab-0.996-ko-0.9.2/src/mecab-dict-index.cpp\nmecab-0.996-ko-0.9.2/src/winmain.h\nmecab-0.996-ko-0.9.2/src/thread.h\nmecab-0.996-ko-0.9.2/src/context_id.cpp\nmecab-0.996-ko-0.9.2/src/Makefile.am\nmecab-0.996-ko-0.9.2/src/connector.h\nmecab-0.996-ko-0.9.2/src/common.h\nmecab-0.996-ko-0.9.2/src/dictionary_rewriter.cpp\nmecab-0.996-ko-0.9.2/src/Makefile.msvc.in\nmecab-0.996-ko-0.9.2/src/dictionary_rewriter.h\nmecab-0.996-ko-0.9.2/src/feature_index.h\nmecab-0.996-ko-0.9.2/src/iconv_utils.cpp\nmecab-0.996-ko-0.9.2/src/char_property.cpp\nmecab-0.996-ko-0.9.2/src/mecab-test-gen.cpp\nmecab-0.996-ko-0.9.2/src/tagger.cpp\nmecab-0.996-ko-0.9.2/src/mecab-cost-train.cpp\nmecab-0.996-ko-0.9.2/src/learner.cpp\nmecab-0.996-ko-0.9.2/src/dictionary.cpp\nmecab-0.996-ko-0.9.2/src/lbfgs.cpp\nmecab-0.996-ko-0.9.2/src/ucs.h\nmecab-0.996-ko-0.9.2/src/writer.cpp\nmecab-0.996-ko-0.9.2/src/learner_tagger.cpp\nmecab-0.996-ko-0.9.2/src/lbfgs.h\nmecab-0.996-ko-0.9.2/src/libmecab.cpp\nmecab-0.996-ko-0.9.2/src/tokenizer.h\nmecab-0.996-ko-0.9.2/src/mecab.cpp\nmecab-0.996-ko-0.9.2/src/utils.cpp\nmecab-0.996-ko-0.9.2/src/dictionary_generator.cpp\nmecab-0.996-ko-0.9.2/src/param.cpp\nmecab-0.996-ko-0.9.2/src/context_id.h\nmecab-0.996-ko-0.9.2/src/mmap.h\nmecab-0.996-ko-0.9.2/src/viterbi.h\nmecab-0.996-ko-0.9.2/src/viterbi.cpp\nmecab-0.996-ko-0.9.2/src/stream_wrapper.h\nmecab-0.996-ko-0.9.2/src/feature_index.cpp\nmecab-0.996-ko-0.9.2/src/nbest_generator.h\nmecab-0.996-ko-0.9.2/src/ucstable.h\nmecab-0.996-ko-0.9.2/src/nbest_generator.cpp\nmecab-0.996-ko-0.9.2/src/iconv_utils.h\nmecab-0.996-ko-0.9.2/src/connector.cpp\nmecab-0.996-ko-0.9.2/src/Makefile.in\nmecab-0.996-ko-0.9.2/src/scoped_ptr.h\nmecab-0.996-ko-0.9.2/Makefile.in\nchecking for a BSD-compatible install... /usr/bin/install -c\nchecking whether build environment is sane... yes\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\nchecking for gawk... no\nchecking for mawk... mawk\nchecking whether make sets $(MAKE)... yes\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables... \nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking for style of include used by make... GNU\nchecking dependency style of gcc... none\nchecking for g++... g++\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\nchecking dependency style of g++... none\nchecking how to run the C preprocessor... gcc -E\nchecking for grep that handles long lines and -e... /bin/grep\nchecking for egrep... /bin/grep -E\nchecking whether gcc needs -traditional... no\nchecking whether make sets $(MAKE)... (cached) yes\nchecking build system type... x86_64-unknown-linux-gnu\nchecking host system type... x86_64-unknown-linux-gnu\nchecking how to print strings... printf\nchecking for a sed that does not truncate output... /bin/sed\nchecking for fgrep... /bin/grep -F\nchecking for ld used by gcc... /usr/bin/ld\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\nchecking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\nchecking the name lister (/usr/bin/nm -B) interface... BSD nm\nchecking whether ln -s works... yes\nchecking the maximum length of command line arguments... 1572864\nchecking whether the shell understands some XSI constructs... yes\nchecking whether the shell understands \"+=\"... yes\nchecking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\nchecking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\nchecking for /usr/bin/ld option to reload object files... -r\nchecking for objdump... objdump\nchecking how to recognize dependent libraries... pass_all\nchecking for dlltool... dlltool\nchecking how to associate runtime and link libraries... printf %s\\n\nchecking for ar... ar\nchecking for archiver @FILE support... @\nchecking for strip... strip\nchecking for ranlib... ranlib\nchecking command to parse /usr/bin/nm -B output from gcc object... ok\nchecking for sysroot... no\n./configure: line 7378: /usr/bin/file: No such file or directory\nchecking for mt... no\nchecking if : is a manifest tool... no\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for dlfcn.h... yes\nchecking for objdir... .libs\nchecking if gcc supports -fno-rtti -fno-exceptions... no\nchecking for gcc option to produce PIC... -fPIC -DPIC\nchecking if gcc PIC flag -fPIC -DPIC works... yes\nchecking if gcc static flag -static works... yes\nchecking if gcc supports -c -o file.o... yes\nchecking if gcc supports -c -o file.o... (cached) yes\nchecking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\nchecking whether -lc should be explicitly linked in... no\nchecking dynamic linker characteristics... GNU/Linux ld.so\nchecking how to hardcode library paths into programs... immediate\nchecking whether stripping libraries is possible... yes\nchecking if libtool supports shared libraries... yes\nchecking whether to build shared libraries... yes\nchecking whether to build static libraries... yes\nchecking how to run the C++ preprocessor... g++ -E\nchecking for ld used by g++... /usr/bin/ld\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\nchecking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\nchecking for g++ option to produce PIC... -fPIC -DPIC\nchecking if g++ PIC flag -fPIC -DPIC works... yes\nchecking if g++ static flag -static works... yes\nchecking if g++ supports -c -o file.o... yes\nchecking if g++ supports -c -o file.o... (cached) yes\nchecking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\nchecking dynamic linker characteristics... (cached) GNU/Linux ld.so\nchecking how to hardcode library paths into programs... immediate\nchecking for library containing strerror... none required\nchecking whether byte ordering is bigendian... no\nchecking for ld used by GCC... /usr/bin/ld\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\nchecking for shared library run path origin... done\nchecking for iconv... yes\nchecking for working iconv... yes\nchecking for iconv declaration... \n         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\nchecking for ANSI C header files... (cached) yes\nchecking for an ANSI C-conforming const... yes\nchecking whether byte ordering is bigendian... (cached) no\nchecking for string.h... (cached) yes\nchecking for stdlib.h... (cached) yes\nchecking for unistd.h... (cached) yes\nchecking fcntl.h usability... yes\nchecking fcntl.h presence... yes\nchecking for fcntl.h... yes\nchecking for stdint.h... (cached) yes\nchecking for sys/stat.h... (cached) yes\nchecking sys/mman.h usability... yes\nchecking sys/mman.h presence... yes\nchecking for sys/mman.h... yes\nchecking sys/times.h usability... yes\nchecking sys/times.h presence... yes\nchecking for sys/times.h... yes\nchecking for sys/types.h... (cached) yes\nchecking dirent.h usability... yes\nchecking dirent.h presence... yes\nchecking for dirent.h... yes\nchecking ctype.h usability... yes\nchecking ctype.h presence... yes\nchecking for ctype.h... yes\nchecking for sys/types.h... (cached) yes\nchecking io.h usability... no\nchecking io.h presence... no\nchecking for io.h... no\nchecking windows.h usability... no\nchecking windows.h presence... no\nchecking for windows.h... no\nchecking pthread.h usability... yes\nchecking pthread.h presence... yes\nchecking for pthread.h... yes\nchecking for off_t... yes\nchecking for size_t... yes\nchecking size of char... 1\nchecking size of short... 2\nchecking size of int... 4\nchecking size of long... 8\nchecking size of long long... 8\nchecking size of size_t... 8\nchecking for size_t... (cached) yes\nchecking for unsigned long long int... yes\nchecking for stdlib.h... (cached) yes\nchecking for unistd.h... (cached) yes\nchecking for sys/param.h... yes\nchecking for getpagesize... yes\nchecking for working mmap... yes\nchecking for main in -lstdc++... yes\nchecking for pthread_create in -lpthread... yes\nchecking for pthread_join in -lpthread... yes\nchecking for getenv... yes\nchecking for opendir... yes\nchecking whether make is GNU Make... yes\nchecking if g++ supports stl &lt;vector> (required)... yes\nchecking if g++ supports stl &lt;list> (required)... yes\nchecking if g++ supports stl &lt;map> (required)... yes\nchecking if g++ supports stl &lt;set> (required)... yes\nchecking if g++ supports stl &lt;queue> (required)... yes\nchecking if g++ supports stl &lt;functional> (required)... yes\nchecking if g++ supports stl &lt;algorithm> (required)... yes\nchecking if g++ supports stl &lt;string> (required)... yes\nchecking if g++ supports stl &lt;iostream> (required)... yes\nchecking if g++ supports stl &lt;sstream> (required)... yes\nchecking if g++ supports stl &lt;fstream> (required)... yes\nchecking if g++ supports template &lt;class T> (required)... yes\nchecking if g++ supports const_cast&lt;> (required)... yes\nchecking if g++ supports static_cast&lt;> (required)... yes\nchecking if g++ supports reinterpret_cast&lt;> (required)... yes\nchecking if g++ supports namespaces (required) ... yes\nchecking if g++ supports __thread (optional)... yes\nchecking if g++ supports template &lt;class T> (required)... yes\nchecking if g++ supports GCC native atomic operations (optional)... yes\nchecking if g++ supports OSX native atomic operations (optional)... no\nchecking if g++ environment provides all required features... yes\nconfigure: creating ./config.status\nconfig.status: creating Makefile\nconfig.status: creating src/Makefile\nconfig.status: creating src/Makefile.msvc\nconfig.status: creating man/Makefile\nconfig.status: creating doc/Makefile\nconfig.status: creating tests/Makefile\nconfig.status: creating swig/version.h\nconfig.status: creating mecab.iss\nconfig.status: creating mecab-config\nconfig.status: creating mecabrc\nconfig.status: creating config.h\nconfig.status: executing depfiles commands\nconfig.status: executing libtool commands\nconfig.status: executing default commands\nmake  all-recursive\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\nMaking all in src\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o viterbi.lo viterbi.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp  -fPIC -DPIC -o .libs/viterbi.o\nIn file included from \u001b[01m\u001b[Kviterbi.cpp:14:0\u001b[m\u001b[K:\n\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string&lt;char>; Source = std::__cxx11::basic_string&lt;char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n std::string \u001b[01;35m\u001b[Klexical_cast&lt;std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp -o viterbi.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tagger.lo tagger.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o utils.lo utils.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp -o utils.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o eval.lo eval.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp -o eval.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o iconv_utils.lo iconv_utils.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp  -fPIC -DPIC -o .libs/iconv_utils.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp -o iconv_utils.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_rewriter.lo dictionary_rewriter.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp  -fPIC -DPIC -o .libs/dictionary_rewriter.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp -o dictionary_rewriter.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_generator.lo dictionary_generator.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp  -fPIC -DPIC -o .libs/dictionary_generator.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp -o dictionary_generator.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_compiler.lo dictionary_compiler.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp  -fPIC -DPIC -o .libs/dictionary_compiler.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp -o dictionary_compiler.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o context_id.lo context_id.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp  -fPIC -DPIC -o .libs/context_id.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp -o context_id.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o connector.lo connector.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp  -fPIC -DPIC -o .libs/connector.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp -o connector.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o nbest_generator.lo nbest_generator.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp  -fPIC -DPIC -o .libs/nbest_generator.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp -o nbest_generator.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o writer.lo writer.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp  -fPIC -DPIC -o .libs/writer.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp -o writer.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o string_buffer.lo string_buffer.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o param.lo param.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tokenizer.lo tokenizer.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp  -fPIC -DPIC -o .libs/tokenizer.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp -o tokenizer.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o char_property.lo char_property.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp  -fPIC -DPIC -o .libs/char_property.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp -o char_property.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary.lo dictionary.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp  -fPIC -DPIC -o .libs/dictionary.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp -o dictionary.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o feature_index.lo feature_index.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o lbfgs.lo lbfgs.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner_tagger.lo learner_tagger.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp  -fPIC -DPIC -o .libs/learner_tagger.o\n\u001b[01m\u001b[Klearner_tagger.cpp:25:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[Kchar* MeCab::{anonymous}::mystrdup(const string&amp;)\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n char *\u001b[01;35m\u001b[Kmystrdup\u001b[m\u001b[K(const std::string &amp;str) {\n       \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp -o learner_tagger.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner.lo learner.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp -o learner.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o libmecab.lo libmecab.cpp\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp  -fPIC -DPIC -o .libs/libmecab.o\nlibtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp -o libmecab.o >/dev/null 2>&amp;1\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall  -no-undefined -version-info 2:0:0  -o libmecab.la -rpath /usr/local/lib viterbi.lo tagger.lo utils.lo eval.lo iconv_utils.lo dictionary_rewriter.lo dictionary_generator.lo dictionary_compiler.lo context_id.lo connector.lo nbest_generator.lo writer.lo string_buffer.lo param.lo tokenizer.lo char_property.lo dictionary.lo feature_index.lo lbfgs.lo learner_tagger.lo learner.lo libmecab.lo  -lpthread -lpthread  -lstdc++ \nlibtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/viterbi.o .libs/tagger.o .libs/utils.o .libs/eval.o .libs/iconv_utils.o .libs/dictionary_rewriter.o .libs/dictionary_generator.o .libs/dictionary_compiler.o .libs/context_id.o .libs/connector.o .libs/nbest_generator.o .libs/writer.o .libs/string_buffer.o .libs/param.o .libs/tokenizer.o .libs/char_property.o .libs/dictionary.o .libs/feature_index.o .libs/lbfgs.o .libs/learner_tagger.o .libs/learner.o .libs/libmecab.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libmecab.so.2 -o .libs/libmecab.so.2.0.0\nlibtool: link: (cd \".libs\" &amp;&amp; rm -f \"libmecab.so.2\" &amp;&amp; ln -s \"libmecab.so.2.0.0\" \"libmecab.so.2\")\nlibtool: link: (cd \".libs\" &amp;&amp; rm -f \"libmecab.so\" &amp;&amp; ln -s \"libmecab.so.2.0.0\" \"libmecab.so\")\nlibtool: link: ar cru .libs/libmecab.a  viterbi.o tagger.o utils.o eval.o iconv_utils.o dictionary_rewriter.o dictionary_generator.o dictionary_compiler.o context_id.o connector.o nbest_generator.o writer.o string_buffer.o param.o tokenizer.o char_property.o dictionary.o feature_index.o lbfgs.o learner_tagger.o learner.o libmecab.o\nar: `u' modifier ignored since `D' is the default (see `U')\nlibtool: link: ranlib .libs/libmecab.a\nlibtool: link: ( cd \".libs\" &amp;&amp; rm -f \"libmecab.la\" &amp;&amp; ln -s \"../libmecab.la\" \"libmecab.la\" )\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab.o mecab.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab mecab.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab mecab.o  ./.libs/libmecab.so -lpthread -lstdc++\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-index.o mecab-dict-index.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-index mecab-dict-index.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab-dict-index mecab-dict-index.o  ./.libs/libmecab.so -lpthread -lstdc++\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-gen.o mecab-dict-gen.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-gen mecab-dict-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab-dict-gen mecab-dict-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-cost-train.o mecab-cost-train.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-cost-train mecab-cost-train.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab-cost-train mecab-cost-train.o  ./.libs/libmecab.so -lpthread -lstdc++\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-system-eval.o mecab-system-eval.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-system-eval mecab-system-eval.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab-system-eval mecab-system-eval.o  ./.libs/libmecab.so -lpthread -lstdc++\ng++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-test-gen.o mecab-test-gen.cpp\n/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-test-gen mecab-test-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \nlibtool: link: g++ -O3 -Wall -o .libs/mecab-test-gen mecab-test-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\nMaking all in man\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\nmake[2]: Nothing to be done for 'all'.\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\nMaking all in doc\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nmake[2]: Nothing to be done for 'all'.\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nMaking all in tests\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[2]: Nothing to be done for 'all'.\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\nMaking check in src\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\nmake[1]: Nothing to be done for 'check'.\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\nMaking check in man\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\nmake[1]: Nothing to be done for 'check'.\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\nMaking check in doc\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nmake[1]: Nothing to be done for 'check'.\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nMaking check in tests\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake  check-TESTS\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 177\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 178x178\nemitting matrix      : 100% |###########################################| \n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 83\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 84x84\nemitting matrix      : 100% |###########################################| \n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 450\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 1x1\n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 162\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 3x3\nemitting matrix      : 100% |###########################################| \n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 4\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 1x1\n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 11\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 1\nreading ./matrix.def ... 1x1\n\ndone!\n./pos-id.def is not found. minimum setting is used\nreading ./unk.def ... 2\nemitting double-array: 100% |###########################################| \n./model.def is not found. skipped.\n./pos-id.def is not found. minimum setting is used\nreading ./dic.csv ... 1\nreading ./matrix.def ... 1x1\n\ndone!\nPASS: run-dics.sh\nPASS: run-eval.sh\nseed/pos-id.def is not found. minimum setting is used\nreading seed/unk.def ... 40\nemitting double-array: 100% |###########################################| \nseed/model.def is not found. skipped.\nseed/pos-id.def is not found. minimum setting is used\nreading seed/dic.csv ... 4335\nemitting double-array: 100% |###########################################| \nreading seed/matrix.def ... 1x1\n\ndone!\nreading corpus ...\nNumber of sentences: 34\nNumber of features:  64108\neta:                 0.00005\nfreq:                1\neval-size:           6\nunk-eval-size:       4\nthreads:             1\ncharset:             EUC-JP\nC(sigma^2):          1.00000\n\niter=0 err=1.00000 F=0.35771 target=2406.28355 diff=1.00000\niter=1 err=0.97059 F=0.65652 target=1484.25231 diff=0.38318\niter=2 err=0.91176 F=0.79331 target=863.32765 diff=0.41834\niter=3 err=0.85294 F=0.89213 target=596.72480 diff=0.30881\niter=4 err=0.61765 F=0.95467 target=336.30744 diff=0.43641\niter=5 err=0.50000 F=0.96702 target=246.53039 diff=0.26695\niter=6 err=0.35294 F=0.95472 target=188.93963 diff=0.23361\niter=7 err=0.20588 F=0.99106 target=168.62665 diff=0.10751\niter=8 err=0.05882 F=0.99777 target=158.64865 diff=0.05917\niter=9 err=0.08824 F=0.99665 target=154.14530 diff=0.02839\niter=10 err=0.08824 F=0.99665 target=151.94257 diff=0.01429\niter=11 err=0.02941 F=0.99888 target=147.20825 diff=0.03116\niter=12 err=0.00000 F=1.00000 target=147.34956 diff=0.00096\niter=13 err=0.02941 F=0.99888 target=146.32592 diff=0.00695\niter=14 err=0.00000 F=1.00000 target=145.77299 diff=0.00378\niter=15 err=0.02941 F=0.99888 target=145.24641 diff=0.00361\niter=16 err=0.00000 F=1.00000 target=144.96490 diff=0.00194\niter=17 err=0.02941 F=0.99888 target=144.90246 diff=0.00043\niter=18 err=0.00000 F=1.00000 target=144.75959 diff=0.00099\niter=19 err=0.00000 F=1.00000 target=144.71727 diff=0.00029\niter=20 err=0.00000 F=1.00000 target=144.66337 diff=0.00037\niter=21 err=0.00000 F=1.00000 target=144.61349 diff=0.00034\niter=22 err=0.00000 F=1.00000 target=144.62987 diff=0.00011\niter=23 err=0.00000 F=1.00000 target=144.60060 diff=0.00020\niter=24 err=0.00000 F=1.00000 target=144.59125 diff=0.00006\niter=25 err=0.00000 F=1.00000 target=144.58619 diff=0.00004\niter=26 err=0.00000 F=1.00000 target=144.58219 diff=0.00003\niter=27 err=0.00000 F=1.00000 target=144.58059 diff=0.00001\n\nDone! writing model file ... \nmodel-ipadic.c1.0.f1.model is not a binary model. reopen it as text mode...\nreading seed/unk.def ... 40\nreading seed/dic.csv ... 4335\nemitting model-ipadic.c1.0.f1.dic/left-id.def/ model-ipadic.c1.0.f1.dic/right-id.def\nemitting model-ipadic.c1.0.f1.dic/unk.def ... 40\nemitting model-ipadic.c1.0.f1.dic/dic.csv ... 4335\nemitting matrix      : 100% |###########################################| \ncopying seed/char.def to model-ipadic.c1.0.f1.dic/char.def\ncopying seed/rewrite.def to model-ipadic.c1.0.f1.dic/rewrite.def\ncopying seed/dicrc to model-ipadic.c1.0.f1.dic/dicrc\ncopying seed/feature.def to model-ipadic.c1.0.f1.dic/feature.def\ncopying model-ipadic.c1.0.f1.model to model-ipadic.c1.0.f1.dic/model.def\n\ndone!\nmodel-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\nreading model-ipadic.c1.0.f1.dic/unk.def ... 40\nemitting double-array: 100% |###########################################| \nmodel-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\nreading model-ipadic.c1.0.f1.dic/dic.csv ... 4335\nemitting double-array: 100% |###########################################| \nreading model-ipadic.c1.0.f1.dic/matrix.def ... 346x346\nemitting matrix      : 100% |###########################################| \n\ndone!\n              precision          recall         F\nLEVEL 0:    12.8959(57/442) 11.8998(57/479) 12.3779\nLEVEL 1:    12.2172(54/442) 11.2735(54/479) 11.7264\nLEVEL 2:    11.7647(52/442) 10.8559(52/479) 11.2921\nLEVEL 4:    11.7647(52/442) 10.8559(52/479) 11.2921\nPASS: run-cost-train.sh\n==================\nAll 3 tests passed\n==================\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\nMaking install in src\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\ntest -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n /bin/bash ../libtool   --mode=install /usr/bin/install -c   libmecab.la '/usr/local/lib'\nlibtool: install: /usr/bin/install -c .libs/libmecab.so.2.0.0 /usr/local/lib/libmecab.so.2.0.0\nlibtool: install: (cd /usr/local/lib &amp;&amp; { ln -s -f libmecab.so.2.0.0 libmecab.so.2 || { rm -f libmecab.so.2 &amp;&amp; ln -s libmecab.so.2.0.0 libmecab.so.2; }; })\nlibtool: install: (cd /usr/local/lib &amp;&amp; { ln -s -f libmecab.so.2.0.0 libmecab.so || { rm -f libmecab.so &amp;&amp; ln -s libmecab.so.2.0.0 libmecab.so; }; })\nlibtool: install: /usr/bin/install -c .libs/libmecab.lai /usr/local/lib/libmecab.la\nlibtool: install: /usr/bin/install -c .libs/libmecab.a /usr/local/lib/libmecab.a\nlibtool: install: chmod 644 /usr/local/lib/libmecab.a\nlibtool: install: ranlib /usr/local/lib/libmecab.a\nlibtool: finish: PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/sbin\" ldconfig -n /usr/local/lib\n----------------------------------------------------------------------\nLibraries have been installed in:\n   /usr/local/lib\n\nIf you ever happen to want to link against installed libraries\nin a given directory, LIBDIR, you must either use libtool, and\nspecify the full pathname of the library, or use the `-LLIBDIR'\nflag during linking and do at least one of the following:\n   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n     during execution\n   - add LIBDIR to the `LD_RUN_PATH' environment variable\n     during linking\n   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n\nSee any operating system documentation about shared libraries for\nmore information, such as the ld(1) and ld.so(8) manual pages.\n----------------------------------------------------------------------\ntest -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab '/usr/local/bin'\nlibtool: install: /usr/bin/install -c .libs/mecab /usr/local/bin/mecab\ntest -z \"/usr/local/libexec/mecab\" || /bin/mkdir -p \"/usr/local/libexec/mecab\"\n  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab-dict-index mecab-dict-gen mecab-cost-train mecab-system-eval mecab-test-gen '/usr/local/libexec/mecab'\nlibtool: install: /usr/bin/install -c .libs/mecab-dict-index /usr/local/libexec/mecab/mecab-dict-index\nlibtool: install: /usr/bin/install -c .libs/mecab-dict-gen /usr/local/libexec/mecab/mecab-dict-gen\nlibtool: install: /usr/bin/install -c .libs/mecab-cost-train /usr/local/libexec/mecab/mecab-cost-train\nlibtool: install: /usr/bin/install -c .libs/mecab-system-eval /usr/local/libexec/mecab/mecab-system-eval\nlibtool: install: /usr/bin/install -c .libs/mecab-test-gen /usr/local/libexec/mecab/mecab-test-gen\ntest -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n /usr/bin/install -c -m 644 mecab.h '/usr/local/include'\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\nMaking install in man\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\nmake[2]: Nothing to be done for 'install-exec-am'.\ntest -z \"/usr/local/share/man/man1\" || /bin/mkdir -p \"/usr/local/share/man/man1\"\n /usr/bin/install -c -m 644 mecab.1 '/usr/local/share/man/man1'\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\nMaking install in doc\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nmake[2]: Nothing to be done for 'install-exec-am'.\nmake[2]: Nothing to be done for 'install-data-am'.\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\nMaking install in tests\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[2]: Nothing to be done for 'install-exec-am'.\nmake[2]: Nothing to be done for 'install-data-am'.\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\nmake[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\nmake[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\ntest -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n /usr/bin/install -c mecab-config '/usr/local/bin'\ntest -z \"/usr/local/etc\" || /bin/mkdir -p \"/usr/local/etc\"\n /usr/bin/install -c -m 644 mecabrc '/usr/local/etc'\nmake[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\nmake[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\nInstall mecab-ko-dic\nInstall mecab-ko-dic\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 47.4M  100 47.4M    0     0  7757k      0  0:00:06  0:00:06 --:--:-- 10.1M\nmecab-ko-dic-2.1.1-20180720/\nmecab-ko-dic-2.1.1-20180720/configure\nmecab-ko-dic-2.1.1-20180720/COPYING\nmecab-ko-dic-2.1.1-20180720/autogen.sh\nmecab-ko-dic-2.1.1-20180720/Place-station.csv\nmecab-ko-dic-2.1.1-20180720/NNG.csv\nmecab-ko-dic-2.1.1-20180720/README\nmecab-ko-dic-2.1.1-20180720/EF.csv\nmecab-ko-dic-2.1.1-20180720/MAG.csv\nmecab-ko-dic-2.1.1-20180720/Preanalysis.csv\nmecab-ko-dic-2.1.1-20180720/NNB.csv\nmecab-ko-dic-2.1.1-20180720/Person-actor.csv\nmecab-ko-dic-2.1.1-20180720/VV.csv\nmecab-ko-dic-2.1.1-20180720/Makefile.in\nmecab-ko-dic-2.1.1-20180720/matrix.def\nmecab-ko-dic-2.1.1-20180720/EC.csv\nmecab-ko-dic-2.1.1-20180720/NNBC.csv\nmecab-ko-dic-2.1.1-20180720/clean\nmecab-ko-dic-2.1.1-20180720/ChangeLog\nmecab-ko-dic-2.1.1-20180720/J.csv\nmecab-ko-dic-2.1.1-20180720/.keep\nmecab-ko-dic-2.1.1-20180720/feature.def\nmecab-ko-dic-2.1.1-20180720/Foreign.csv\nmecab-ko-dic-2.1.1-20180720/XPN.csv\nmecab-ko-dic-2.1.1-20180720/EP.csv\nmecab-ko-dic-2.1.1-20180720/NR.csv\nmecab-ko-dic-2.1.1-20180720/left-id.def\nmecab-ko-dic-2.1.1-20180720/Place.csv\nmecab-ko-dic-2.1.1-20180720/Symbol.csv\nmecab-ko-dic-2.1.1-20180720/dicrc\nmecab-ko-dic-2.1.1-20180720/NP.csv\nmecab-ko-dic-2.1.1-20180720/ETM.csv\nmecab-ko-dic-2.1.1-20180720/IC.csv\nmecab-ko-dic-2.1.1-20180720/Place-address.csv\nmecab-ko-dic-2.1.1-20180720/Group.csv\nmecab-ko-dic-2.1.1-20180720/model.def\nmecab-ko-dic-2.1.1-20180720/XSN.csv\nmecab-ko-dic-2.1.1-20180720/INSTALL\nmecab-ko-dic-2.1.1-20180720/rewrite.def\nmecab-ko-dic-2.1.1-20180720/Inflect.csv\nmecab-ko-dic-2.1.1-20180720/configure.ac\nmecab-ko-dic-2.1.1-20180720/NNP.csv\nmecab-ko-dic-2.1.1-20180720/CoinedWord.csv\nmecab-ko-dic-2.1.1-20180720/XSV.csv\nmecab-ko-dic-2.1.1-20180720/pos-id.def\nmecab-ko-dic-2.1.1-20180720/Makefile.am\nmecab-ko-dic-2.1.1-20180720/unk.def\nmecab-ko-dic-2.1.1-20180720/missing\nmecab-ko-dic-2.1.1-20180720/VCP.csv\nmecab-ko-dic-2.1.1-20180720/install-sh\nmecab-ko-dic-2.1.1-20180720/Hanja.csv\nmecab-ko-dic-2.1.1-20180720/MAJ.csv\nmecab-ko-dic-2.1.1-20180720/XSA.csv\nmecab-ko-dic-2.1.1-20180720/Wikipedia.csv\nmecab-ko-dic-2.1.1-20180720/tools/\nmecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh\nmecab-ko-dic-2.1.1-20180720/tools/mecab-bestn.sh\nmecab-ko-dic-2.1.1-20180720/tools/convert_for_using_store.sh\nmecab-ko-dic-2.1.1-20180720/user-dic/\nmecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\nmecab-ko-dic-2.1.1-20180720/user-dic/place.csv\nmecab-ko-dic-2.1.1-20180720/user-dic/person.csv\nmecab-ko-dic-2.1.1-20180720/user-dic/README.md\nmecab-ko-dic-2.1.1-20180720/NorthKorea.csv\nmecab-ko-dic-2.1.1-20180720/VX.csv\nmecab-ko-dic-2.1.1-20180720/right-id.def\nmecab-ko-dic-2.1.1-20180720/VA.csv\nmecab-ko-dic-2.1.1-20180720/char.def\nmecab-ko-dic-2.1.1-20180720/NEWS\nmecab-ko-dic-2.1.1-20180720/MM.csv\nmecab-ko-dic-2.1.1-20180720/ETN.csv\nmecab-ko-dic-2.1.1-20180720/AUTHORS\nmecab-ko-dic-2.1.1-20180720/Person.csv\nmecab-ko-dic-2.1.1-20180720/XR.csv\nmecab-ko-dic-2.1.1-20180720/VCN.csv\nLooking in current directory for macros.\nconfigure.ac:2: warning: AM_INIT_AUTOMAKE: two- and three-arguments forms are deprecated.  For more info, see:\nconfigure.ac:2: http://www.gnu.org/software/automake/manual/automake.html#Modernize-AM_005fINIT_005fAUTOMAKE-invocation\nchecking for a BSD-compatible install... /usr/bin/install -c\nchecking whether build environment is sane... yes\n/tmp/mecab-ko-dic-2.1.1-20180720/missing: Unknown `--is-lightweight' option\nTry `/tmp/mecab-ko-dic-2.1.1-20180720/missing --help' for more information\nconfigure: WARNING: 'missing' script is too old or missing\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\nchecking for gawk... no\nchecking for mawk... mawk\nchecking whether make sets $(MAKE)... yes\nchecking whether make supports nested variables... yes\nchecking for mecab-config... /usr/local/bin/mecab-config\nchecking that generated files are newer than configure... done\nconfigure: creating ./config.status\nconfig.status: creating Makefile\n/usr/local/lib\n/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n\n/usr/local/libexec/mecab/mecab-dict-index -d . -o . -f UTF-8 -t UTF-8\nreading ./unk.def ... 13\nemitting double-array: 100% |###########################################| \nreading ./Inflect.csv ... 44820\nreading ./XSN.csv ... 124\nreading ./ETM.csv ... 133\nreading ./Hanja.csv ... 125750\nreading ./Place-address.csv ... 19301\nreading ./VX.csv ... 125\nreading ./Place.csv ... 30303\nreading ./Preanalysis.csv ... 5\nreading ./EC.csv ... 2547\nreading ./NR.csv ... 482\nreading ./MAJ.csv ... 240\nreading ./EF.csv ... 1820\nreading ./NNP.csv ... 2371\nreading ./Symbol.csv ... 16\nreading ./J.csv ... 416\nreading ./XPN.csv ... 83\nreading ./NNB.csv ... 140\nreading ./Place-station.csv ... 1145\nreading ./IC.csv ... 1305\nreading ./MAG.csv ... 14242\nreading ./Person-actor.csv ... 99230\nreading ./XSA.csv ... 19\nreading ./Wikipedia.csv ... 36762\nreading ./VV.csv ... 7331\nreading ./CoinedWord.csv ... 148\nreading ./Person.csv ... 196459\nreading ./Foreign.csv ... 11690\nreading ./NorthKorea.csv ... 3\nreading ./NNG.csv ... 208524\nreading ./XR.csv ... 3637\nreading ./MM.csv ... 453\nreading ./XSV.csv ... 23\nreading ./VCN.csv ... 7\nreading ./VA.csv ... 2360\nreading ./VCP.csv ... 9\nreading ./ETN.csv ... 14\nreading ./EP.csv ... 51\nreading ./NP.csv ... 342\nreading ./NNBC.csv ... 677\nreading ./Group.csv ... 3176\nemitting double-array: 100% |###########################################| \nreading ./matrix.def ... 3822x2693\nemitting matrix      : 100% |###########################################| \n\ndone!\necho To enable dictionary, rewrite /usr/local/etc/mecabrc as \\\"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\\\"\nTo enable dictionary, rewrite /usr/local/etc/mecabrc as \"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\"\nmake[1]: Entering directory '/tmp/mecab-ko-dic-2.1.1-20180720'\nmake[1]: Nothing to be done for 'install-exec-am'.\n /bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\n /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\nmake[1]: Leaving directory '/tmp/mecab-ko-dic-2.1.1-20180720'\nInstall mecab-python\n/tmp /tmp/mecab-ko-dic-2.1.1-20180720\nCloning into 'mecab-python-0.996'...\nUnpacking objects: 100% (17/17), done.\n/tmp/mecab-ko-dic-2.1.1-20180720\nProcessing /tmp/mecab-python-0.996\n\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\nBuilding wheels for collected packages: mecab-python\n  Building wheel for mecab-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp37-cp37m-linux_x86_64.whl size=141818 sha256=f8224281c456f08d8c3637aca638579c5ffafda3456d907e82f6ed0598c31843\n  Stored in directory: /root/.cache/pip/wheels/40/7b/9f/2922869bef86c3354ae7034f7a3647c573ee1997c2dad0290a\n\u001b[33m  WARNING: Built wheel for mecab-python is invalid: Metadata 1.2 mandates PEP 440 version, but '0.996-ko-0.9.2' is not\u001b[0m\nFailed to build mecab-python\nInstalling collected packages: mecab-python\n    Running setup.py install for mecab-python ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: mecab-python was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\nSuccessfully installed mecab-python-0.996-ko-0.9.2\nDone.</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#주요 라이브러리 버전 확인</span>\n\n<span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf <span class=\"token comment\">#NLP 모델 생성</span>\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np <span class=\"token comment\">#데이터 배열 처리</span>\n<span class=\"token keyword\">import</span> matplotlib <span class=\"token keyword\">as</span> plt <span class=\"token comment\">#시각화</span>\n<span class=\"token keyword\">import</span> konlpy\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>__version__<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>__version__<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>plt<span class=\"token punctuation\">.</span>__version__<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>konlpy<span class=\"token punctuation\">.</span>__version__<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">2.8.0\n1.21.5\n3.2.2\n0.6.0</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">!pip install sentencepiece</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Collecting sentencepiece\n  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n\u001b[?25hInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.1.96</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#이 외 라이브러리</span>\n\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt <span class=\"token comment\">#시각화 라이브러리 pyplot</span>\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd <span class=\"token comment\">#데이터 배열</span>\n<span class=\"token keyword\">import</span> sentencepiece <span class=\"token keyword\">as</span> spm <span class=\"token comment\">#우리가 사용할 Tokenizer</span>\n\n<span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>colab <span class=\"token keyword\">import</span> files \n<span class=\"token keyword\">import</span> io <span class=\"token comment\"># kolab 데이터 경로 라이브러리</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># LSTM 라이브러리</span>\n\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Embedding<span class=\"token punctuation\">,</span> Dense<span class=\"token punctuation\">,</span> LSTM</code></pre></div>\n<h1 id=\"2-game-1\" style=\"position:relative;\"><a href=\"#2-game-1\" aria-label=\"2 game 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. GAME</h1>\n<h2 id=\"2-1-데이터-읽어오기\" style=\"position:relative;\"><a href=\"#2-1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BD%EC%96%B4%EC%98%A4%EA%B8%B0\" aria-label=\"2 1 데이터 읽어오기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-1. 데이터 읽어오기</h2>\n<p>깃허브에서는 NLP 학습을 위해 일반 이용자들이 다양한 말뭉치를 제공해주고 있다.</p>\n<p>그 중 <a href=\"https://github.com/e9t/nsmc/\">여기</a> 에서는 네이버의 영화 리뷰에 대한 말뭉치를 제시한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>colab <span class=\"token keyword\">import</span> drive\ndrive<span class=\"token punctuation\">.</span>mount<span class=\"token punctuation\">(</span><span class=\"token string\">'/content/drive'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Mounted at /content/drive</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>colab <span class=\"token keyword\">import</span> files\n\nuploaded <span class=\"token operator\">=</span> files<span class=\"token punctuation\">.</span>upload<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><input type=\"file\" id=\"files-c238279a-3be2-4583-a158-523e8391c9ce\" name=\"files[]\" multiple disabled\nstyle=\"border:none\" />\n<output id=\"result-c238279a-3be2-4583-a158-523e8391c9ce\">\nUpload widget is only available when the cell has been executed in the\ncurrent browser session. Please rerun this cell to enable.\n</output></p>\n <script src=\"/nbextensions/google.colab/files.js\"></script> \n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Saving ratings_train.txt to ratings_train.txt\nSaving ratings_test.txt to ratings_test.txt</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_data<span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_table<span class=\"token punctuation\">(</span>io<span class=\"token punctuation\">.</span>StringIO<span class=\"token punctuation\">(</span>uploaded<span class=\"token punctuation\">[</span><span class=\"token string\">'ratings_train.txt'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntrain_data<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <div id=\"df-f70194ae-0c56-4494-bfd2-e4f316891ca1\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f70194ae-0c56-4494-bfd2-e4f316891ca1')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n<p>&#x3C;svg xmlns=”<a href=\"http://www.w3.org/2000/svg\">http://www.w3.org/2000/svg</a>” height=“24px”viewBox=“0 0 24 24”\nwidth=“24px”>\n<path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n<path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n</svg>\n</button></p>\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  &lt;script>\n    const buttonEl =\n      document.querySelector('#df-f70194ae-0c56-4494-bfd2-e4f316891ca1 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-f70194ae-0c56-4494-bfd2-e4f316891ca1');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                 [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook&lt;/a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  &lt;/script>\n&lt;/div></code></pre></div>\n  </div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_table<span class=\"token punctuation\">(</span>io<span class=\"token punctuation\">.</span>StringIO<span class=\"token punctuation\">(</span>uploaded<span class=\"token punctuation\">[</span><span class=\"token string\">'ratings_test.txt'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntest_data<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n  <div id=\"df-a9a4f778-8733-4490-b8d0-1d8b358002c8\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6270596</td>\n      <td>굳 ㅋ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9274899</td>\n      <td>GDNTOPCLASSINTHECLUB</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8544678</td>\n      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6825595</td>\n      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6723715</td>\n      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9a4f778-8733-4490-b8d0-1d8b358002c8')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n<p>&#x3C;svg xmlns=”<a href=\"http://www.w3.org/2000/svg\">http://www.w3.org/2000/svg</a>” height=“24px”viewBox=“0 0 24 24”\nwidth=“24px”>\n<path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n<path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n</svg>\n</button></p>\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  &lt;script>\n    const buttonEl =\n      document.querySelector('#df-a9a4f778-8733-4490-b8d0-1d8b358002c8 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-a9a4f778-8733-4490-b8d0-1d8b358002c8');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                 [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook&lt;/a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  &lt;/script>\n&lt;/div></code></pre></div>\n  </div>\n<h2 id=\"2-2데이터-전처리\" style=\"position:relative;\"><a href=\"#2-2%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\" aria-label=\"2 2데이터 전처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-2.데이터 전처리</h2>\n<h3 id=\"2-2-1-tokenizer-생성\" style=\"position:relative;\"><a href=\"#2-2-1-tokenizer-%EC%83%9D%EC%84%B1\" aria-label=\"2 2 1 tokenizer 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-2-1. Tokenizer 생성</h3>\n<p>우리가 단어사전을 만들기 위해 사용할<br>\n데이터는 따로 있다. 더 크고 방대한<br>\n한국어 자료를 사용해 더 명료하고 명확한<br>\n한국어 단어 사전을 만들 것이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">corpus_path <span class=\"token operator\">=</span> <span class=\"token string\">'/content/drive/MyDrive/Colab_Notebooks/Aiffel/data/korean-english-park.train.ko'</span>\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>corpus_path<span class=\"token punctuation\">,</span> <span class=\"token string\">\"r\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    raw <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>splitlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Data Size:\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Example:\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> sen <span class=\"token keyword\">in</span> raw<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token number\">100</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">20</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span> <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\">>\"</span><span class=\"token punctuation\">,</span> sen<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Data Size: 94123\nExample:\n>> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n>> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n>> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n>> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n>> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.</code></pre></div>\n<p>이전에 이 자료를 탐색 및 분석했기 때문에 EDA 과정은 생략하고 바로 전처리에 들어가도록 하겠다.</p>\n<p>전처리 과정은</p>\n<blockquote>\n<ol>\n<li>중복데이터 삭제</li>\n</ol>\n</blockquote>\n<ol start=\"2\">\n<li><code class=\"language-text\">max_len</code> = 150</li>\n<li><code class=\"language-text\">min_len</code> = 10</li>\n<li><code class=\"language-text\">padding</code> 처리</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">cleaned_corpus <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 1.중복제거</span>\n\nmax_len <span class=\"token operator\">=</span> <span class=\"token number\">150</span> <span class=\"token comment\"># 2번</span>\nmin_len <span class=\"token operator\">=</span> <span class=\"token number\">10</span> <span class=\"token comment\">#3번</span>\n\n<span class=\"token comment\"># 길이 조건에 맞는 문장 선택</span>\nfiltered_corpus <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>s <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> cleaned_corpus <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> max_len<span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> min_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># 분포도로 시각화.</span>\nsentence_length <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> sen <span class=\"token keyword\">in</span> filtered_corpus<span class=\"token punctuation\">:</span>\n    sentence_length<span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>sen<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n\nplt<span class=\"token punctuation\">.</span>bar<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> sentence_length<span class=\"token punctuation\">,</span> width<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"Sentence Length Distribution\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  # Remove the CWD from sys.path while we load stuff.</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 381px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACtElEQVQ4y4WU20sUcRTHj9nFysJL3lr3ShJERH9B0KME4rtbDyWbQ+Cd3VUMiZ4iX3rqISJCg0TM1t3BNUhTMzKM9YpSOG67ju6klTu7O4vub+bEb3bWVl3oB4czc+acD99zfvP7AQCAxWIBRNxnAFAAAHoAOKc9GwGgGAB0AFCuWW6GOjhutVqLBEHoFkXx/fb2tjccDntDodAHv//HJ38gOBHk+TGOW5nieX58bW1tIhgMfgwEApOCIIyKouilNZFIZITn+WcUeKqmpsYcj8c3McN6OsYh88qHcYL/XdFodJUCc2pra8slSVqhQUVRdoms0HLyhdsiFe0sOX2vn3QNL6uxBJGJoigHbZfWiqI4rwJtNlt5LBbza0CZ+p2EjNe7RrG4yYUGuxsv3R9C/o+kKpEVBbVcTK8RRXGRAo9VVVVVSJKkAoksqx+n/b9R1zqIJocHLW0sFjQM4EP3IiZzFBWWEUh3pqOjoySlMAXsdC1gfv0AWpysCj3fOohXHwyjEI4fUnlQ4YnKysqLKSDNpUWXO717CmnLZqcHCxvf4t3u6T2VGYHpCrUc+fkEp7ZIIUbHP6Ngvd2NU9wvNTGRbObwDKurqy+kFO4kiHzt0QiWNLvQ5ExCjGkqi5tdeOPJ+N78tHnuAwJTV6cLi5FVGhxZCiXKWlyyyeEhRrtbNtjdxJDmLU4Pyat/Iz/2qr+RnBy7ktCAC5R31HrzlkmS4hs0yPR8xYJGF5rbWDQ6WK3ddM+iycliacsg3nk5jdxmLP3HXgGGYeA2U1+4JWy8Wwr8XDS39Pl0Db2zhqY+n76hd06/z7+e1/ycsblv5oytZ/ZK+8BsU/fkTP/n70v8+no/aCsbAE7SHadHMRsgj55xACgCgCMAUAoAWQBQpr3TiyHHdDaLXha5AJBPc78NvYC//kz3Mfiv3r4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/11ca6f3c58f745096e074987426920f1/2add2/output_25_1.png\"\n        srcset=\"/static/11ca6f3c58f745096e074987426920f1/e9ff0/output_25_1.png 180w,\n/static/11ca6f3c58f745096e074987426920f1/f21e7/output_25_1.png 360w,\n/static/11ca6f3c58f745096e074987426920f1/2add2/output_25_1.png 381w\"\n        sizes=\"(max-width: 381px) 100vw, 381px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>해당 데이터를 SentencePiece 라이브러리를 통해</p>\n<p>Tokenize 시킨다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">temp_file <span class=\"token operator\">=</span> <span class=\"token string\">'/content/drive/MyDrive/Colab_Notebooks/Aiffel/data/korean-english-park.train.ko.temp'</span>\n\nvocab_size <span class=\"token operator\">=</span> <span class=\"token number\">8000</span>\n\nmodel_type <span class=\"token operator\">=</span> <span class=\"token string\">'bpe'</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>temp_file<span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> row <span class=\"token keyword\">in</span> filtered_corpus<span class=\"token punctuation\">:</span>   <span class=\"token comment\"># 이전 스텝에서 정제했던 corpus를 활용합니다.</span>\n        f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token string\">'\\n'</span><span class=\"token punctuation\">)</span>\n\nspm<span class=\"token punctuation\">.</span>SentencePieceTrainer<span class=\"token punctuation\">.</span>Train<span class=\"token punctuation\">(</span>\n    <span class=\"token string-interpolation\"><span class=\"token string\">f'--input=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>temp_file<span class=\"token punctuation\">}</span></span><span class=\"token string\"> --model_prefix=korean_spm --vocab_size=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>vocab_size<span class=\"token punctuation\">}</span></span><span class=\"token string\"> --model_type=</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>model_type<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span>    <span class=\"token comment\">#korean_spm 에 저장</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">#위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.</span>\n\n\n!ls <span class=\"token operator\">-</span>l korean_spm<span class=\"token operator\">*</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">-rw-r--r-- 1 root root 371954 Mar 23 03:11 korean_spm.model\n-rw-r--r-- 1 root root 117142 Mar 23 03:11 korean_spm.vocab</code></pre></div>\n<p>위 코드를 실행하면 정상적으로 SentencePiece 모델 학습이 완료된다. 이후에는</p>\n<p>korean_spm.model 파일과<br>\nkorean_spm.vocab vocabulary 파일이 root에 생성된다.</p>\n<p>다음은 이렇게 학습한 model 데이터의 활용이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">s <span class=\"token operator\">=</span> spm<span class=\"token punctuation\">.</span>SentencePieceProcessor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ns<span class=\"token punctuation\">.</span>Load<span class=\"token punctuation\">(</span><span class=\"token string\">'korean_spm.model'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SentencePiece를 활용한 sentence -> encoding</span>\ntokensIDs <span class=\"token operator\">=</span> s<span class=\"token punctuation\">.</span>EncodeAsIds<span class=\"token punctuation\">(</span><span class=\"token string\">'아버지가방에들어가신다.'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokensIDs<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SentencePiece를 활용한 sentence -> encoded pieces</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">.</span>SampleEncodeAsPieces<span class=\"token punctuation\">(</span><span class=\"token string\">'아버지가방에들어가신다.'</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SentencePiece를 활용한 encoding -> sentence 복원</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">.</span>DecodeIds<span class=\"token punctuation\">(</span>tokensIDs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[1243, 11, 302, 7, 3608, 11, 287, 38, 3]\n['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n아버지가방에들어가신다.</code></pre></div>\n<p><code class=\"language-text\">EncodeAsIds</code> = 글자를 벡터화 리스트로 반환</p>\n<p><code class=\"language-text\">SampleEncodeAsPieces</code> = 글자를 나눈 방법을 리스트로 반환</p>\n<p><code class=\"language-text\">DecodeIds</code> = 벡터화 리스트를 글자로 변환</p>\n<p>해당 SentencePiece 를 영화 리뷰 데이터로 Tokenize 하기 위한 코드는 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># s = spm.SentencePieceProcessor()</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">sp_tokenize</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> corpus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    tensor <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">for</span> sen <span class=\"token keyword\">in</span> corpus<span class=\"token punctuation\">:</span>\n        tensor<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">.</span>EncodeAsIds<span class=\"token punctuation\">(</span>sen<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 문장이 벡터화된 리스트로 변환되어 tensor 에 입력</span>\n\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"./korean_spm.vocab\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n        vocab <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    word_index <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n    index_word <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n\n    <span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> line <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        word <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\"\\t\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n        word_index<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>idx<span class=\"token punctuation\">:</span>word<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#word_to_index 저장</span>\n        index_word<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>word<span class=\"token punctuation\">:</span>idx<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#index_to)word 저장</span>\n\n    tensor <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>sequence<span class=\"token punctuation\">.</span>pad_sequences<span class=\"token punctuation\">(</span>tensor<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'post'</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># tensor 문장 중 가장 긴 문장을 기준으로 패딩</span>\n\n    <span class=\"token keyword\">return</span> tensor<span class=\"token punctuation\">,</span> word_index<span class=\"token punctuation\">,</span> index_word</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>\n<h3 id=\"2-2-2-학습-데이터-전처리\" style=\"position:relative;\"><a href=\"#2-2-2-%ED%95%99%EC%8A%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\" aria-label=\"2 2 2 학습 데이터 전처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-2-2. 학습 데이터 전처리</h3>\n<p>이제는 학습시키기 위해 영화리뷰 데이터를 전처리하도록 한다.</p>\n<ol>\n<li>오타 등 필요없는 문자 제거</li>\n<li>중복값 제거</li>\n<li>결측값 제거</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">\n<span class=\"token keyword\">def</span> <span class=\"token function\">preprocessing</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">,</span> test_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span>\n    test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#한글이 아닌 문자를 공백으로 변환</span>\n\n    train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'^ +'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span>\n    train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>nan<span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n    test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'^ +'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#긴 공백을 공백으로 변경</span>\n    test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>nan<span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 공백을 NaN 으로 변경</span>\n\n    train_data<span class=\"token punctuation\">.</span>drop_duplicates<span class=\"token punctuation\">(</span>subset<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> \n    test_data<span class=\"token punctuation\">.</span>drop_duplicates<span class=\"token punctuation\">(</span>subset<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#중복값 제거</span>\n\n    train_data <span class=\"token operator\">=</span> train_data<span class=\"token punctuation\">.</span>dropna<span class=\"token punctuation\">(</span>how <span class=\"token operator\">=</span> <span class=\"token string\">'any'</span><span class=\"token punctuation\">)</span> \n    test_data <span class=\"token operator\">=</span> test_data<span class=\"token punctuation\">.</span>dropna<span class=\"token punctuation\">(</span>how <span class=\"token operator\">=</span> <span class=\"token string\">'any'</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#결측치 제거</span>\n\n    <span class=\"token keyword\">return</span> train_data<span class=\"token punctuation\">,</span> test_data\n  \ntrain<span class=\"token punctuation\">,</span> test <span class=\"token operator\">=</span> preprocessing<span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">,</span> test_data<span class=\"token punctuation\">)</span>\n\ntrain<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  This is separate from the ipykernel package so we can avoid doing imports until\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n  after removing the cwd from sys.path.\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n  \n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n  if __name__ == '__main__':</code></pre></div>\n  <div id=\"df-c3f4c367-1db1-48cb-8ade-ac1bb06e3012\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙   진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼   솔직히 재미는 없다  평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3f4c367-1db1-48cb-8ade-ac1bb06e3012')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n<p>&#x3C;svg xmlns=”<a href=\"http://www.w3.org/2000/svg\">http://www.w3.org/2000/svg</a>” height=“24px”viewBox=“0 0 24 24”\nwidth=“24px”>\n<path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n<path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n</svg>\n</button></p>\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  &lt;script>\n    const buttonEl =\n      document.querySelector('#df-c3f4c367-1db1-48cb-8ade-ac1bb06e3012 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-c3f4c367-1db1-48cb-8ade-ac1bb06e3012');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                 [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook&lt;/a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  &lt;/script>\n&lt;/div></code></pre></div>\n  </div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">.</span>isnull<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">.</span><span class=\"token builtin\">any</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">.</span>isnull<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">.</span><span class=\"token builtin\">any</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">False\nFalse</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">144975 48751</code></pre></div>\n<p>길이 분포를 확인하고 적절한 크기로 padding 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 데이터 길이 분포 확인하기</span>\n\nmin_len <span class=\"token operator\">=</span> <span class=\"token number\">999</span>\nmax_len <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nsum_len <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token keyword\">for</span> sen <span class=\"token keyword\">in</span> train<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n    length <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>sen<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token comment\"># 문장 최소 길이 찾기</span>\n    <span class=\"token keyword\">if</span> min_len <span class=\"token operator\">></span> length<span class=\"token punctuation\">:</span> \n        min_len <span class=\"token operator\">=</span> length\n    \n    <span class=\"token comment\"># 문장 최대 길이 찾기</span>\n    <span class=\"token keyword\">if</span> max_len <span class=\"token operator\">&lt;</span> length<span class=\"token punctuation\">:</span> \n        max_len <span class=\"token operator\">=</span> length\n        \n    <span class=\"token comment\"># 전체 문장총 길이</span>\n    sum_len <span class=\"token operator\">+=</span> length\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문장의 최단 길이:\"</span><span class=\"token punctuation\">,</span> min_len<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문장의 최장 길이:\"</span><span class=\"token punctuation\">,</span> max_len<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"문장의 평균 길이:\"</span><span class=\"token punctuation\">,</span> sum_len <span class=\"token operator\">//</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 전체 길이만큼 0벡터 ==> 길이에 따른 문장의 수를 저장하기 위해 먼저 0으로 이루어진 리스트를 만든다!!</span>\nsentence_length <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">for</span> sen <span class=\"token keyword\">in</span> train<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n    sentence_length<span class=\"token punctuation\">[</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>sen<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> <span class=\"token number\">1</span> <span class=\"token comment\"># 0으로 이루어진 벡터에 문장 count를 더한 뒤 넣는다.</span>\n\nplt<span class=\"token punctuation\">.</span>bar<span class=\"token punctuation\">(</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> sentence_length<span class=\"token punctuation\">,</span> width<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 너비는 1.0씩 늘어나도록 설정</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"문장 길이별 분포\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">문장의 최단 길이: 1\n문장의 최장 길이: 145\n문장의 평균 길이: 36\n\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51109 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44600 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54252 missing from current font.\n  font.set_text(s, 0.0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51109 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44600 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n  font.set_text(s, 0, flags=flags)\n/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54252 missing from current font.\n  font.set_text(s, 0, flags=flags)</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 381px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACiklEQVQ4y52UX0jTURTHzzLIShFz2lJztYgM33qoXkIiJAp7CKoXH3qqpBjMdKEPWSFF/7A/VhA9WmB/FN2aLkPDcBSkzU3NUtRttLbl0n6/ze33794T96eYmjPqwuHch3M+53vO5VzQ6/XADiImNABIBYD1AJAFAKuXjS0pKYFAMJgcDAbrOZ7vCIUn7RzHvZpn9kAg0OX1eh0+n88RCoXe8Dz/R0wkEun0+/2PWPW0g8WHtsmiMMELCnaPhPF/TzQaHWfAFUbTWZ0kxMemYhJeaOmXZIUolNJ/MYkBeZ7vV/uuuXpDK8Zj3mlRwZ2XX5O3w9/VijIhqqeULquMUkpmgYNqy4X7ivJlUfAwhQXVbeTw/W4U5YWw5aALgKWlpdAz7E+hkuD1/JjG7efbyJrTjVjb/mVGpUL+OrvFClMLiw5sRUX0jISimGu2kA3lFiy83onjE9E5dUsppAmASUePn8hGInmsrm+YYWomW6psqDU14/5bXRiXlITQeeP4DWSPYr72IB1R9lxpHcI0YxMxVNkw12zFgmo7hiOCmqQQOgddwhYozNi1Z28+UmnsVH0vA8qGypdEf86qsPaHAhyTyBLmeUoYhBCqEBVGZVEhGOH5AXh4swYQ4ylTfDRYXOfAzLIW3Fxlw02VNsw2W7Gotgsfv/fhgJ9DLi5jXCK4eJqT0yL2feVQFmKjMNHbBvculWf1fBpt31FtGdQZG5x5pmeujWXPnXmmp27tmQZn6sl6t6HihXP3RYv7WF1nX8WTd66axg+uOrvbeafV6T5yt+Pj7da+z7Gf4Sb1Y1gJkMSWHgBWaWb8OgDQAICO7XvOWo2ObRQAZAJAMgCkA4B29p4zG8t+maRf7HZNCc/hpT8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/491f77a7b6f65dce1bf58b96c199d6b4/2add2/output_40_2.png\"\n        srcset=\"/static/491f77a7b6f65dce1bf58b96c199d6b4/e9ff0/output_40_2.png 180w,\n/static/491f77a7b6f65dce1bf58b96c199d6b4/f21e7/output_40_2.png 360w,\n/static/491f77a7b6f65dce1bf58b96c199d6b4/2add2/output_40_2.png 381w\"\n        sizes=\"(max-width: 381px) 100vw, 381px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">min_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>s <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> train<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;=</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nmin_list<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">['최고', '졸작', '대박', '아', '점', '버려', '망함', '굳굳', '안습', '잼']</code></pre></div>\n<p>데이터 분포를 고려하여 45이하 데이터를 사용하도록 하겠다.</p>\n<p>원래는 2글자 이하 데이터도 제거하려 하였으나,<br>\n위 데이터를 보고 충분히 유의미하다고 느꼈다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>s <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> train<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;=</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\ntest_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>s <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> test<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;=</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\ntrain_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>train_list<span class=\"token punctuation\">)</span>\ntest_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>test_list<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 길이 40이하인 데이터를 기존 데이터와 병합.</span>\n\nnew_train_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>merge<span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">,</span> train_df<span class=\"token punctuation\">,</span> how<span class=\"token operator\">=</span><span class=\"token string\">'inner'</span><span class=\"token punctuation\">,</span> left_on<span class=\"token operator\">=</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">,</span> right_on<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\nnew_test_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>merge<span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">,</span> test_df<span class=\"token punctuation\">,</span> how<span class=\"token operator\">=</span><span class=\"token string\">'inner'</span><span class=\"token punctuation\">,</span> left_on<span class=\"token operator\">=</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">,</span> right_on<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\ntrain_data <span class=\"token operator\">=</span> new_train_df<span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'document'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\ntest_data <span class=\"token operator\">=</span> new_test_df<span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'document'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_data</code></pre></div>\n  <div id=\"df-6b82ed66-34b6-44e2-bfa5-f1150a3e85f3\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙   진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼   솔직히 재미는 없다  평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7797314</td>\n      <td>원작의 긴장감을 제대로 살려내지못했다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93794</th>\n      <td>6222902</td>\n      <td>인간이 문제지   소는 뭔죄인가</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>93795</th>\n      <td>8549745</td>\n      <td>평점이 너무 낮아서</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93796</th>\n      <td>9311800</td>\n      <td>이게 뭐요  한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>93797</th>\n      <td>2376369</td>\n      <td>청춘 영화의 최고봉 방황과 우울했던 날들의 자화상</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>93798</th>\n      <td>9619869</td>\n      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>93799 rows × 3 columns</p>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b82ed66-34b6-44e2-bfa5-f1150a3e85f3')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n<p>&#x3C;svg xmlns=”<a href=\"http://www.w3.org/2000/svg\">http://www.w3.org/2000/svg</a>” height=“24px”viewBox=“0 0 24 24”\nwidth=“24px”>\n<path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n<path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n</svg>\n</button></p>\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">  &lt;script>\n    const buttonEl =\n      document.querySelector('#df-6b82ed66-34b6-44e2-bfa5-f1150a3e85f3 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-6b82ed66-34b6-44e2-bfa5-f1150a3e85f3');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                 [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '&lt;a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook&lt;/a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  &lt;/script>\n&lt;/div></code></pre></div>\n  </div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"훈련데이터 : \"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"테스트데이터 : \"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">훈련데이터 :  93799\n테스트데이터 :  31631</code></pre></div>\n<p>텍스트 정제 후엔 훈련된 tokenizer 를 이용해<br>\n문장을 vector 화 시킨다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># tensor 화 시킨다</span>\nX_train<span class=\"token punctuation\">,</span>X_train_word_index<span class=\"token punctuation\">,</span> X_train_index_word <span class=\"token operator\">=</span> sp_tokenize<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nX_test<span class=\"token punctuation\">,</span>X_test_word_index<span class=\"token punctuation\">,</span> X_test_index_word <span class=\"token operator\">=</span> sp_tokenize<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># label 데이터 분리</span>\n\ny_train <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\ny_test <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">아 더빙   진짜 짜증나네요 목소리\n[ 141  106 2611  912 4856    4 4856  752   69  554  514 2648    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0]\n\n흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나\n[   4 7663  490 1756  146   14  439 3174 2766 1791  222  408  381   41\n 4189    4   11 7570   29 1311  230   69    0    0    0    0    0    0\n    0    0    0    0    0    0    0]\n\n너무재밓었다그래서보는것을추천한다\n[1328  437    0  266  254  591   95  146   10 1960    5 1011  703  249\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"학습데이터 :\"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"타겟데이터 :\"</span><span class=\"token punctuation\">,</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">학습데이터 : 93799\n타겟데이터 : 93799</code></pre></div>\n<p>93799 개의 텍스트가 35개의 숫자 데이터로 정의되었다.<br>\npadding 까지 씌어졌음을 알 수 있다.</p>\n<h3 id=\"2-2-3-val-분리\" style=\"position:relative;\"><a href=\"#2-2-3-val-%EB%B6%84%EB%A6%AC\" aria-label=\"2 2 3 val 분리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-2-3. val 분리</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\ntrain_input<span class=\"token punctuation\">,</span> val_input<span class=\"token punctuation\">,</span> train_target<span class=\"token punctuation\">,</span> val_target <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>val_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">75039\n18760</code></pre></div>\n<hr>\n<p>결과적으로, 총 데이터는 다음과 같다.</p>\n<p>train_input : 학습시킬 문장의 tensor<br>\ntrain_target :학습시킬 문장의 label</p>\n<p>val_input : 검증할 문장의 tensor<br>\nval_target : 검증할 문장의 label</p>\n<p>X_test : 측정할 문장의 tensor<br>\ny_test : 측정할 문장의 label</p>\n<h2 id=\"2-3모델-학습\" style=\"position:relative;\"><a href=\"#2-3%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5\" aria-label=\"2 3모델 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-3.모델 학습</h2>\n<p>훈련된 데이터를 통해 다양한 학습을 시킬 것이다.</p>\n<ol>\n<li>RNN 모델 사용</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">vocab_size <span class=\"token operator\">=</span> <span class=\"token number\">8000</span>  <span class=\"token comment\"># 어휘 사전의 크기입니다</span>\nword_vector_dim <span class=\"token operator\">=</span> <span class=\"token number\">100</span>  <span class=\"token comment\"># 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. </span>\n\n<span class=\"token comment\"># RNN 방식</span>\n\nmodel <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span> word_vector_dim<span class=\"token punctuation\">,</span> input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   \nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'sigmoid'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최종 출력은 긍정/부정을 나타내는 1dim 입니다.</span>\n\nmodel<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, None, 100)         800000    \n                                                                 \n lstm (LSTM)                 (None, 8)                 3488      \n                                                                 \n dense (Dense)               (None, 8)                 72        \n                                                                 \n dense_1 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 803,569\nTrainable params: 803,569\nNon-trainable params: 0\n_________________________________________________________________</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>\n              loss<span class=\"token operator\">=</span><span class=\"token string\">'binary_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n              \nepochs<span class=\"token operator\">=</span><span class=\"token number\">20</span>  <span class=\"token comment\"># 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. </span>\n\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_input<span class=\"token punctuation\">,</span>\n                    train_target<span class=\"token punctuation\">,</span>\n                    epochs<span class=\"token operator\">=</span>epochs<span class=\"token punctuation\">,</span>\n                    batch_size<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>\n                    validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>val_input<span class=\"token punctuation\">,</span> val_target<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                    verbose<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/20\n1173/1173 [==============================] - 10s 7ms/step - loss: 0.3819 - accuracy: 0.8278 - val_loss: 0.4099 - val_accuracy: 0.8082\nEpoch 2/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.3558 - accuracy: 0.8400 - val_loss: 0.4398 - val_accuracy: 0.8047\nEpoch 3/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.3288 - accuracy: 0.8556 - val_loss: 0.4236 - val_accuracy: 0.8152\nEpoch 4/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.3030 - accuracy: 0.8692 - val_loss: 0.4304 - val_accuracy: 0.8148\nEpoch 5/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.2802 - accuracy: 0.8822 - val_loss: 0.4366 - val_accuracy: 0.8165\nEpoch 6/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.2577 - accuracy: 0.8941 - val_loss: 0.4608 - val_accuracy: 0.8129\nEpoch 7/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.2365 - accuracy: 0.9049 - val_loss: 0.4742 - val_accuracy: 0.8113\nEpoch 8/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.2172 - accuracy: 0.9141 - val_loss: 0.4833 - val_accuracy: 0.8073\nEpoch 9/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.2026 - accuracy: 0.9217 - val_loss: 0.5415 - val_accuracy: 0.8120\nEpoch 10/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1873 - accuracy: 0.9290 - val_loss: 0.5531 - val_accuracy: 0.8073\nEpoch 11/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1758 - accuracy: 0.9345 - val_loss: 0.5657 - val_accuracy: 0.8078\nEpoch 12/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1642 - accuracy: 0.9397 - val_loss: 0.5720 - val_accuracy: 0.8062\nEpoch 13/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1560 - accuracy: 0.9425 - val_loss: 0.5678 - val_accuracy: 0.8077\nEpoch 14/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1477 - accuracy: 0.9469 - val_loss: 0.6246 - val_accuracy: 0.8075\nEpoch 15/20\n1173/1173 [==============================] - 8s 6ms/step - loss: 0.1410 - accuracy: 0.9496 - val_loss: 0.6327 - val_accuracy: 0.8041\nEpoch 16/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.1337 - accuracy: 0.9528 - val_loss: 0.6378 - val_accuracy: 0.8033\nEpoch 17/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1293 - accuracy: 0.9552 - val_loss: 0.6658 - val_accuracy: 0.8033\nEpoch 18/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1234 - accuracy: 0.9578 - val_loss: 0.6399 - val_accuracy: 0.8010\nEpoch 19/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1176 - accuracy: 0.9605 - val_loss: 0.7190 - val_accuracy: 0.8000\nEpoch 20/20\n1173/1173 [==============================] - 7s 6ms/step - loss: 0.1143 - accuracy: 0.9617 - val_loss: 0.6888 - val_accuracy: 0.8025</code></pre></div>\n<ol start=\"2\">\n<li>LSTM</li>\n</ol>\n<p>다음으로는 LSTM 모델을 이용해 학습을 진행했다.<br>\n손실함수와 최적화함수는 tensor flow 권장 사항을 따랐다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># LSTM</span>\n\nmodel <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 임베딩 레이어</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>LSTM<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># LSTM 레이어</span>\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'sigmoid'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 출력 레이어</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'rmsprop'</span><span class=\"token punctuation\">,</span> loss<span class=\"token operator\">=</span><span class=\"token string\">'binary_crossentropy'</span><span class=\"token punctuation\">,</span> metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">epochs<span class=\"token operator\">=</span><span class=\"token number\">20</span>  <span class=\"token comment\"># 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. </span>\n\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_input<span class=\"token punctuation\">,</span>\n                    train_target<span class=\"token punctuation\">,</span>\n                    epochs<span class=\"token operator\">=</span>epochs<span class=\"token punctuation\">,</span>\n                    batch_size<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>\n                    validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>val_input<span class=\"token punctuation\">,</span> val_target<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                    verbose<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/20\n1173/1173 [==============================] - 11s 8ms/step - loss: 0.4747 - acc: 0.7743 - val_loss: 0.4453 - val_acc: 0.8026\nEpoch 2/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.4123 - acc: 0.8123 - val_loss: 0.4038 - val_acc: 0.8118\nEpoch 3/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3873 - acc: 0.8236 - val_loss: 0.4068 - val_acc: 0.8171\nEpoch 4/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3695 - acc: 0.8326 - val_loss: 0.3877 - val_acc: 0.8201\nEpoch 5/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3545 - acc: 0.8420 - val_loss: 0.3878 - val_acc: 0.8239\nEpoch 6/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3404 - acc: 0.8503 - val_loss: 0.3892 - val_acc: 0.8233\nEpoch 7/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3272 - acc: 0.8582 - val_loss: 0.4038 - val_acc: 0.8201\nEpoch 8/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3160 - acc: 0.8649 - val_loss: 0.3799 - val_acc: 0.8292\nEpoch 9/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.3042 - acc: 0.8701 - val_loss: 0.3780 - val_acc: 0.8266\nEpoch 10/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2929 - acc: 0.8774 - val_loss: 0.3885 - val_acc: 0.8285\nEpoch 11/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2812 - acc: 0.8827 - val_loss: 0.4003 - val_acc: 0.8289\nEpoch 12/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2688 - acc: 0.8889 - val_loss: 0.4134 - val_acc: 0.8292\nEpoch 13/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2533 - acc: 0.8970 - val_loss: 0.4477 - val_acc: 0.8150\nEpoch 14/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2401 - acc: 0.9041 - val_loss: 0.4366 - val_acc: 0.8217\nEpoch 15/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2265 - acc: 0.9101 - val_loss: 0.4455 - val_acc: 0.8181\nEpoch 16/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.2133 - acc: 0.9178 - val_loss: 0.4690 - val_acc: 0.8197\nEpoch 17/20\n1173/1173 [==============================] - 9s 7ms/step - loss: 0.2002 - acc: 0.9237 - val_loss: 0.4750 - val_acc: 0.8135\nEpoch 18/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.1866 - acc: 0.9295 - val_loss: 0.5083 - val_acc: 0.8131\nEpoch 19/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.1741 - acc: 0.9356 - val_loss: 0.5606 - val_acc: 0.8115\nEpoch 20/20\n1173/1173 [==============================] - 8s 7ms/step - loss: 0.1614 - acc: 0.9406 - val_loss: 0.5704 - val_acc: 0.8082</code></pre></div>\n<h2 id=\"2-4데이터-평가\" style=\"position:relative;\"><a href=\"#2-4%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8F%89%EA%B0%80\" aria-label=\"2 4데이터 평가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2-4.데이터 평가</h2>\n<p>종합적으로 5번의 시도를 했으며,<br>\nbatch size = 64<br>\nepochs = 20 으로 통일하고</p>\n<p>tokenizer 와 학습 모델에 변화를 주었다.</p>\n<blockquote>\n<ol>\n<li>영화리뷰 데이터로 Sentencepiece Tokenize : RNN</li>\n</ol>\n</blockquote>\n<ol start=\"2\">\n<li>한국어 corpus 데이터로 Sentencepiece Tokenize : RNN (임베딩 차원 20)</li>\n<li>임베딩 차원 100</li>\n<li>LSTM 모델 설정 (임베딩 차원 100)</li>\n<li>SentencePiece 모델 타입 BPE 로 변경</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># tokenizer 를 영화 리뷰 데이터로 제작</span>\n\nresult <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>  y_test<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1524/1524 - 6s - loss: 0.6932 - accuracy: 0.4982 - 6s/epoch - 4ms/step\n[0.6932055950164795, 0.4981846511363983]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 한국어 corpus 사용</span>\n\nresult <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>  y_test<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1524/1524 - 6s - loss: 0.6933 - acc: 0.4982 - 6s/epoch - 4ms/step\n[0.6933001279830933, 0.4981846511363983]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 임베딩 차원 100</span>\n\nresults <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>  y_test<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">989/989 - 3s - loss: 0.6922 - accuracy: 0.8013 - 3s/epoch - 3ms/step\n[0.6921951770782471, 0.8012709021568298]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># LSTM 모델 사용</span>\nresults <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>  y_test<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">989/989 - 4s - loss: 0.5414 - acc: 0.8038 - 4s/epoch - 4ms/step\n[0.5413879156112671, 0.8038316965103149]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># SentencePiece 모델 타입 변경 BPE</span>\n\nresults <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>  y_test<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>results<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">989/989 - 5s - loss: 0.5565 - acc: 0.8088 - 5s/epoch - 5ms/step\n[0.5565109848976135, 0.8087635636329651]</code></pre></div>\n<p>결과적으로</p>\n<p>한국어 corpus 를 이용해 임베딩 차원 100 으로 두고\nLSTM 모델을 사용한 BPE 방식 Tokenizer 가 가장 큰 결과를 얻었다</p>\n<p>Accuracy = 0.808%</p>\n<p>다양한 시도를 통해 알 수 있었던 점</p>\n<ol>\n<li>임베딩차원의 크기가 정확도에 큰 영향을 미친다.</li>\n<li>RNN 보다 LSTM 이 Loss 를 대폭 줄인다.</li>\n</ol>\n<p>그 외의 차이는 미미한 것으로 보인다.<br>\n하지만 모델의 특성상 <code class=\"language-text\">batch_size</code> 나 임베딩차원, 전처리 방식에<br>\n변화를 주었을 때 또 다른 영향을 끼칠 수도 있음이 나의 결론이다.</p>\n<h1 id=\"3-potg\" style=\"position:relative;\"><a href=\"#3-potg\" aria-label=\"3 potg permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. POTG</h1>\n<h2 id=\"3-1-소감\" style=\"position:relative;\"><a href=\"#3-1-%EC%86%8C%EA%B0%90\" aria-label=\"3 1 소감 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3-1. 소감</h2>\n<h4 id=\"-nlp-뿌시기-1단계-통과\" style=\"position:relative;\"><a href=\"#-nlp-%EB%BF%8C%EC%8B%9C%EA%B8%B0-1%EB%8B%A8%EA%B3%84-%ED%86%B5%EA%B3%BC\" aria-label=\" nlp 뿌시기 1단계 통과 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>”👍 NLP 뿌시기 1단계 통과!”</h4>\n<p>드디어 Tokenizer 가 무엇인지 감을 잡은 것 같습니다.<br>\nTokenize 를 하기 위해선 tokenizer 에 들어갈 데이터도 전처리를 해주어야 합니다.  그리고 tokenizer 에 저장되어 있는 word_to_index 데이터로</p>\n<p>저희가 학습할 데이터를 tensor 화 해줍니다.</p>\n<p>tensor 라 함은 우리의 문장 데이터가 숫자로 변환된 리스트 데이터를 의미합니다.</p>\n<p>tensor 는 tokenizer 텍스트 데이터를 토대로 어떤 기준을 통해 글을 나눠야 할지 판별해주는 역할을 합니다.</p>\n<h2 id=\"3-2-어려웠던-점과-극복방안\" style=\"position:relative;\"><a href=\"#3-2-%EC%96%B4%EB%A0%A4%EC%9B%A0%EB%8D%98-%EC%A0%90%EA%B3%BC-%EA%B7%B9%EB%B3%B5%EB%B0%A9%EC%95%88\" aria-label=\"3 2 어려웠던 점과 극복방안 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3-2. 어려웠던 점과 극복방안</h2>\n<p>-1. 불용어 제거</p>\n<p>morphs 에서는 불용어를 제거하는데 (왜냐하면 그냥 단어를 분리만 해서 리턴해주니까)\nSentencepiece 는 모든 단어를 이미 tensor 화 해서 주기 때문에<br>\n불용어를 제거할 수가 없다.</p>\n<p>그렇다면 불용어를 제거하지 않아도 되는가?</p>\n<blockquote>\n<p>어차피 SentencePiece Train 을 하기 위해서 문장을 전처리 후에 넣게 되는데, 그러므로 불용어를 처리하고 넣을 수 잇다.</p>\n</blockquote>\n<p>-2 전처리 ” 데이터<br>\n결측치를 제거해도 min_len 이 0 이 나오는 상황이 생겼는데,\n” 데이터가 남아있기 때문이다. 때문에 판다스 메서드를 통해 이러한 부분을 제거해 주었다.</p>\n<p>-3. 오류<br>\n<code class=\"language-text\"> Another metric with the same name already exists.</code>\n라는 오류문이 <code class=\"language-text\">tp_tokenize</code> 과정에서 자꾸 났는데, stack over flow 검색 결과 keras 가 두 개 설치되어 있어 나타나는 현상이라고 한다. tensorflow 버전 문제라고 판단해 keras 버전을 2.6에서 2.8로 변경하고서는 오류가 나타나지 않았다.</p>\n<p>-4. 차이가 나는 indexing</p>\n<p><img src=\"/882fa8f89f03aadc3173362b70f0a756/image1.png\"></img></p>\n<p>어떤 부분에선가 train 데이터와 test 데이터가 동일하게 전처리되지 않았다. 다시 한번 꼼꼼히 진행하면서(한 코드씩 비교해가면서) 문제를 해결할 수 있었다.</p>\n<p>-5. 결과가 나오지 않는 모델</p>\n<p><img src=\"/67d16008f45d036c65f8239c1f647928/image2.png\"></img></p>\n<p>다양한 모델 변경을 해가면서 이전 test 타입과 이후 test 타입에 차이가 있어 나타난 현상이었다. 새로운 변수명을 주어 해결할 수 있었다.</p>\n<h2 id=\"3-3-추후\" style=\"position:relative;\"><a href=\"#3-3-%EC%B6%94%ED%9B%84\" aria-label=\"3 3 추후 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3-3. 추후</h2>\n<p>NLP 학습의 전체적인 큰 그림을 그려보고 싶다.</p>\n<p>tokenize 를 사용함과 사용하지 않을 때 padding 이나 임베딩에 차이가 있다.<br>\ntokenizer 를 통해 그런 부분이 해소되기 때문이다. 그래서 처음에<br>\n왜 padding 을 두번 해주는지 등 이해가 안되는 부분이 많았다.</p>\n<p>전체적인 그림을 그릴 수 있다면 이런 문제가 해소될 것이다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#contexts\">Contexts</a></p>\n<ul>\n<li><a href=\"#1-ready\">1. READY</a></li>\n<li><a href=\"#2-game\">2. GAME</a></li>\n<li><a href=\"#3-potg-best-play-of-the-game\">3. POTG (best Play Of The Game</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#1-1-%EC%98%A4%EB%8A%98%EC%9D%98-exp%EC%99%80-rubric\">1-1. 오늘의 Exp와 Rubric</a></p>\n</li>\n<li>\n<p><a href=\"#1-2-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">1-2. 사용하는 라이브러리</a></p>\n</li>\n<li>\n<p><a href=\"#2-1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BD%EC%96%B4%EC%98%A4%EA%B8%B0\">2-1. 데이터 읽어오기</a></p>\n</li>\n<li>\n<p><a href=\"#2-2%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\">2-2.데이터 전처리</a></p>\n<ul>\n<li><a href=\"#2-2-1-tokenizer-%EC%83%9D%EC%84%B1\">2-2-1. Tokenizer 생성</a></li>\n<li><a href=\"#2-2-2-%ED%95%99%EC%8A%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\">2-2-2. 학습 데이터 전처리</a></li>\n<li><a href=\"#2-2-3-val-%EB%B6%84%EB%A6%AC\">2-2-3. val 분리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-3%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5\">2-3.모델 학습</a></p>\n</li>\n<li>\n<p><a href=\"#2-4%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8F%89%EA%B0%80\">2-4.데이터 평가</a></p>\n</li>\n<li>\n<p><a href=\"#3-1-%EC%86%8C%EA%B0%90\">3-1. 소감</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#-nlp-%EB%BF%8C%EC%8B%9C%EA%B8%B0-1%EB%8B%A8%EA%B3%84-%ED%86%B5%EA%B3%BC\">”👍 NLP 뿌시기 1단계 통과!”</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#3-2-%EC%96%B4%EB%A0%A4%EC%9B%A0%EB%8D%98-%EC%A0%90%EA%B3%BC-%EA%B7%B9%EB%B3%B5%EB%B0%A9%EC%95%88\">3-2. 어려웠던 점과 극복방안</a></p>\n</li>\n<li>\n<p><a href=\"#3-3-%EC%B6%94%ED%9B%84\">3-3. 추후</a></p>\n</li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","excerpt":"🙄 네이버 영화리뷰 감성분석에 SentencePiece 적용해보기 <NLP기초> Contexts 1. READY 2. GAME 3. POTG (best Play Of The Game 1. Ready 1-1. 오늘의 Exp와 Rubric SentencePiece 는 Google 에서 제공하고 있는 Tokenizer / Detokenizer 이다. Tokenize 란 NLP 에서 중요한 부분인 ‘단어사전 제작’ 을 의미한다. 직관적으로 생각했을 때, 단어사전은 단어별, 형태소별, 혹은 그 사이 어떤 경계를 나누어 만들 수 있다. Sentencepiece 는\nBPE 와 unigram 이라는 두 가지의 분리 방법을 통해 subword tokenizing model 을 제공하고 있다. 최근 pretrained model 은 대부분 SentencePiece 를 Tokenizer 로 설정하는 추세이기에 NLP 분야 tokenizer 의 표준이라고 표현해도 과언이 아니다. 오늘은 이러한 Sent…","frontmatter":{"date":"March 23, 2022","title":"SentencePiece Tokenizer 사용 방법","categories":"NLP","author":"하성민","emoji":"😁"},"fields":{"slug":"/NLP_2/"}},"next":{"id":"6d79397e-6ce5-58cd-bb09-adcf492b30f0","html":"<h1 id=\"\" style=\"position:relative;\"><a href=\"#\" aria-label=\" permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h1>\n<p>우리는 많은 문장들 속에서 살아간다.<br>\n한국어, 영어, 일본어, 중국어,, 영어 배워라 배워라 스트레스 받는것도 다<br>\n언어가 존재하기 때문이다.</p>\n<p>이렇게 일상에서 사용하는 언어를 <strong>자연어(Natural Language)</strong> 라고 부른다.</p>\n<p>반면에 우리가 쓰는 프로그래밍 언어는 <strong>기계어 또는 인공어(Artificial Language)</strong> 라고 부른다.</p>\n<p>(엄연히 따지면 자연어 안에 인공어가 속한다)</p>\n<p>프로그래밍 언어는 자연어보다 훨씬 명료하고 처리하기에 용이하여야 하므로<br>\n자연어와 다르게 문맥과 상관없이 항상 동일한 의미를 가진다.</p>\n<hr>\n<p>우리는 자연어를 처리해야 하기 때문에<br>\n사람이 하는 방식대로는 즉, 문맥을 이해하는 방식으로는<br>\n기계를 학습시키기가 어렵다.</p>\n<p>때문에, 단어들을 숫자로 매핑해 표현하는 머신러닝 기법 을 사용한다.<br>\n그렇다면 각 단어들의 의미를 기계들도 이해하고 연산할 수 있을 것이다.</p>\n<p><a href=\"http://w.elnn.kr/search/\">해당 블로그</a>에서는 그렇게 수치화된 단어들간의<br>\n계산을 시도해 볼 수 있다.</p>\n<p>이와 함께 단어를 어떻게 구분할 것인지(형태소? 단어?) 결정하는<br>\n<code class=\"language-text\">Toknization</code> 기법을 사용한다.</p>\n<h2 id=\"1-잡음이-많은-자연어\" style=\"position:relative;\"><a href=\"#1-%EC%9E%A1%EC%9D%8C%EC%9D%B4-%EB%A7%8E%EC%9D%80-%EC%9E%90%EC%97%B0%EC%96%B4\" aria-label=\"1 잡음이 많은 자연어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 잡음이 많은 자연어</h2>\n<p><img src=\"/fe59587a4cc89abb453737e1622d75b5/image1.png\"></img></p>\n<p>우리의 일상언어는 문맥이 있어야만 이해가 가능하다<br>\n위는 내가 친구들과 게임 약속을 잡을 때 나눈 대화이다.</p>\n<p>우선 어느정도의 오타가 존재하고, 주어 동사는 밥먹듯이 빼먹고 있다.<br>\n디코는 음성채팅 어플리케이션인 Discord의 줄임말이다.</p>\n<p>이러한 변형들이 자연어처리 모델 입장에서는 Noise(잡음) 이 된다.<br>\n따라서 우리가 모델을 학습시키기 위해선 이러한 잡음이 최대한 없는 데이터를 사용한다.</p>\n<p>(예를 들어 소설책, 뉴스기사 등)</p>\n<h3 id=\"잡음의-종류\" style=\"position:relative;\"><a href=\"#%EC%9E%A1%EC%9D%8C%EC%9D%98-%EC%A2%85%EB%A5%98\" aria-label=\"잡음의 종류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>잡음의 종류</h3>\n<ol>\n<li>혼자서 두 세 번 전송하는 경우</li>\n<li>문장길이가 너무 짧거나 긴 경우</li>\n<li>문장 간격이 너무 긴 경우</li>\n<li>욕설 / 오타 가 많은 경우</li>\n</ol>\n<p>너무나도 많지만, 우선 우리가 그나마 정제할 수 있는 노이즈만 뽑자면 다음과 같다.</p>\n<ol>\n<li>문장부호</li>\n<li>대소문자 (영어)</li>\n<li>특수문자</li>\n</ol>\n<p>때문에 NLP 에서는 이렇게 정제할 수 있는 Noise 를\npreprocessing 하는 작업을 거친다.</p>\n<h2 id=\"2-preprocessing\" style=\"position:relative;\"><a href=\"#2-preprocessing\" aria-label=\"2 preprocessing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Preprocessing</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 1. 문장부호 구분</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">pad_punctuation</span><span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">,</span> punc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> punc<span class=\"token punctuation\">:</span>\n        sentence <span class=\"token operator\">=</span> sentence<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">,</span> <span class=\"token string\">\" \"</span> <span class=\"token operator\">+</span> p <span class=\"token operator\">+</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> sentence\n\nsentence <span class=\"token operator\">=</span> <span class=\"token string\">\"안녕, 내 글 읽어줘서 땡큐.\"</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>pad_punctuation<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\".\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"?\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"!\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\",\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># , ? ! . 가 들어간 부분 양쪽에 띄어쓰기 처리</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">안녕 ,  내 글 읽어줘서 땡큐 . </code></pre></div>\n<p>문장부호를 제거하지 않는 이유는 쉼표나 마침표 또한 일정한 의미를 가지고 있다고 판단하기 때문이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 2. 대소문자 처리</span>\n\nsentence <span class=\"token operator\">=</span> <span class=\"token string\">\"First, open the first chapter.\"</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">first, open the first chapter.</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 3. 특수문자 제거</span>\n\n<span class=\"token keyword\">import</span> re <span class=\"token comment\">#단어 정규화 라이브러리</span>\n\nsentence <span class=\"token operator\">=</span> <span class=\"token string\">\"He is a ten-year-old boy.\"</span>\nsentence <span class=\"token operator\">=</span> re<span class=\"token punctuation\">.</span>sub<span class=\"token punctuation\">(</span><span class=\"token string\">\"([^a-zA-Z.,?!])\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">,</span> sentence<span class=\"token punctuation\">)</span> <span class=\"token comment\">#알파벳과 ! ? ,. 이 아닌 모든 문자는 공백으로 변환</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">He is a ten year old boy.</code></pre></div>\n<hr>\n<h2 id=\"3-embeddng\" style=\"position:relative;\"><a href=\"#3-embeddng\" aria-label=\"3 embeddng permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Embeddng</h2>\n<p>text를 정제했다면,  임베딩레이어 (<code class=\"language-text\">Embedding layer</code>)를 통해 단어의 분산 표현 (<code class=\"language-text\">Distributed represntation</code>) 을 구현한다.<br>\n분산표현은 단어의 속성 차원(<code class=\"language-text\">dim</code>) 을 우리가 임의로 지정하고, 그 <code class=\"language-text\">dim</code> 에 맞게 추상적인 속성을 구분해 표현하는 것을 의미한다.</p>\n<p>때문에 적절한 차원 수와 합당한 속성 값들이 들어간다면, 각 단어마다 적절한 의미의 유사도가 나타날 것이다.</p>\n<p>의미의 유사도는 <code class=\"language-text\">코사인 유사도</code> 를 통해 나타낸다.</p>\n<blockquote>\n<p>코사인 유사도 ( Cosine similarity )</p>\n</blockquote>\n<p>두 벡터 간의 코사인 각도를 이용해 구할 수 있는 두 벡터의 유사도, 다시 말 해 두 벡터(방향과 속도) 가 어느정도 유사한 지 찾는 계산</p>\n<p><img src=\"/5c5d5b0bb99a8f9840f66c02c02e0903/image2.png\"></img></p>\n<p>위의 그림처럼 동일한 속력 하에 방향이 반대면 -1, 90도면 0, 같은 방향이면 1 의 값을 가진다.<br>\n코사인 유사도는 -1에서 1 사이의 값을 지닌다.</p>\n<p>A와 B 의 코사인 유사도 계산식은 다음과 같다.</p>\n<p><img src=\"/6f4e5bb15c88af9e0b691b048eb41c8a/image3.png\"></img></p>\n<hr>\n<p>이 때, <code class=\"language-text\">dim</code>설정 후 나온 속성 값들의 모음을 <code class=\"language-text\">단어 사전</code> 이라 칭하는데,<br>\n이 단어사전을 제대로 작성하기 위해선 단어를 어떻게 쪼갤 것인지도 판단해야 한다.</p>\n<p>예를 들어보자</p>\n<p>나는 드디어 그녀와 사귀기로 했다</p>\n<p>라는 말이 있을 때, 어떻게 분리해야 할까?</p>\n<ol>\n<li>나, 는, 드디어, 그녀, 와, 사귀기로, 했다. # 조사 나눔</li>\n<li>나는, 드디어, 그녀와, 사귀기로, 했다. #띄어쓰기 나눔</li>\n<li>나,는, 드디어, 그녀, 와, 사귀기,로, 했,다. #형태소별 나눔</li>\n</ol>\n<p>문장을 나누었을 때 쪼개진 조각조각을 <code class=\"language-text\">Token</code> 이라 부른다.\n이것을 결정하는 방식이 <code class=\"language-text\">Tokenization</code> 이다.</p>\n<p>토큰화를 위한 여러가지 분석기들이 이미 개발되어 있다.</p>\n<p>그 종류별 차이점을 <a href=\"https://iostream.tistory.com/144\">해당 블로그</a> 에서 잘 설명해주고 있다.</p>\n<p>우리도 한 번 써보도록 하겠다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> konlpy<span class=\"token punctuation\">.</span>tag <span class=\"token keyword\">import</span> Hannanum<span class=\"token punctuation\">,</span>Kkma<span class=\"token punctuation\">,</span>Komoran<span class=\"token punctuation\">,</span>Mecab<span class=\"token punctuation\">,</span>Okt\n\ntokenizer_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>Hannanum<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>Kkma<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>Komoran<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>Mecab<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>Okt<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\nkor_text <span class=\"token operator\">=</span> <span class=\"token string\">'제 이름은 하성민입니다. 헬스 트레이닝과 NLP에 흥미가 있습니다. 세상의 우울을 해결하고 싶습니다.'</span>\n\n<span class=\"token keyword\">for</span> tokenizer <span class=\"token keyword\">in</span> tokenizer_list<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'[{}] \\n{}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>__class__<span class=\"token punctuation\">.</span>__name__<span class=\"token punctuation\">,</span> tokenizer<span class=\"token punctuation\">.</span>pos<span class=\"token punctuation\">(</span>kor_text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[Hannanum] \n[('저', 'N'), ('의', 'J'), ('이름', 'N'), ('은', 'J'), ('하성민입니', 'N'), ('이', 'J'), ('다', 'E'), ('.', 'S'), ('헬스', 'N'), ('트레이닝', 'N'), ('과', 'J'), ('NLP', 'F'), ('에', 'J'), ('흥미', 'N'), ('가', 'J'), ('있', 'P'), ('습니다', 'E'), ('.', 'S'), ('세상', 'N'), ('의', 'J'), ('우울', 'N'), ('을', 'J'), ('해결', 'N'), ('하고', 'J'), ('싶', 'P'), ('습니다', 'E'), ('.', 'S')]\n[Kkma] \n[('저', 'NP'), ('의', 'JKG'), ('이름', 'NNG'), ('은', 'JX'), ('하성', 'NNG'), ('민', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('헬스', 'NNG'), ('트레이닝', 'NNG'), ('과', 'JC'), ('NLP', 'OL'), ('에', 'JKM'), ('흥미', 'NNG'), ('가', 'JKS'), ('있', 'VV'), ('습니다', 'EFN'), ('.', 'SF'), ('세상', 'NNG'), ('의', 'JKG'), ('우울', 'NNG'), ('을', 'JKO'), ('해결', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('싶', 'VXA'), ('습니다', 'EFN'), ('.', 'SF')]\n[Komoran] \n[('제', 'XPN'), ('이름', 'NNG'), ('은', 'JX'), ('하성민', 'NNP'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF'), ('헬스', 'NNG'), ('트레이닝', 'NNG'), ('과', 'JC'), ('NLP', 'SL'), ('에', 'JKB'), ('흥미', 'NNG'), ('가', 'JKS'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('세상', 'NNG'), ('의', 'JKG'), ('우울', 'NNG'), ('을', 'JKO'), ('해결', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('싶', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n[Mecab] \n[('제', 'NP+JKG'), ('이름', 'NNG'), ('은', 'JX'), ('하성민', 'NNP'), ('입니다', 'VCP+EF'), ('.', 'SF'), ('헬스', 'NNG'), ('트레이닝', 'NNG'), ('과', 'JC'), ('NLP', 'SL'), ('에', 'JKB'), ('흥미', 'NNG'), ('가', 'JKS'), ('있', 'VA'), ('습니다', 'EF'), ('.', 'SF'), ('세상', 'NNG'), ('의', 'JKG'), ('우울', 'NNG'), ('을', 'JKO'), ('해결', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('싶', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n[Okt] \n[('제', 'Noun'), ('이름', 'Noun'), ('은', 'Josa'), ('하성민', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation'), ('헬스', 'Noun'), ('트레이닝', 'Noun'), ('과', 'Josa'), ('NLP', 'Alpha'), ('에', 'Josa'), ('흥미', 'Noun'), ('가', 'Josa'), ('있습니다', 'Adjective'), ('.', 'Punctuation'), ('세상', 'Noun'), ('의', 'Josa'), ('우울', 'Noun'), ('을', 'Josa'), ('해결', 'Noun'), ('하고', 'Josa'), ('싶습니다', 'Verb'), ('.', 'Punctuation')]</code></pre></div>\n<p>뒤에 따라오는 알파벳은 해당 글자가 명사인지, 고유명사인지, 형용사인지 구분해주는 기호이다.</p>\n<p><a href=\"http://kkma.snu.ac.kr/documents/?doc=postag\">해당 블로그</a>에서 어떤 알파벳이 어떤 형태소를 나타내는지 표시해준다.<br>\n이 기호 또한 분석기마다 조금의 차이는 있지만 거의 비슷하다.</p>\n<p>내가 봤을 때는 Komoran 이 좀 정확한 것 같다.</p>\n<h3 id=\"이-외-embedding-방법\" style=\"position:relative;\"><a href=\"#%EC%9D%B4-%EC%99%B8-embedding-%EB%B0%A9%EB%B2%95\" aria-label=\"이 외 embedding 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>이 외 Embedding 방법</h3>\n<ol>\n<li>BPE</li>\n</ol>\n<p>이처럼 Embedding layer 는 단어갯수 x 차원 수(n_dim) 의 가중치를 생성한다.</p>\n<p>단어갯수가 많을수록 사용되는 메모리가 많다고 볼 수 있다.<br>\n때문에 단어갯수를 줄여 저장하는 BPE (Byte Pair Encoding) 이나 그 응용 심화 단계인 WPM(Word Piece Model) 등이 사용되기도 한다.</p>\n<p>WPM 은 현재 공개되어 있지는 않고, 구글의 SentencePiece 라이브러리를 통해 전처리까지 해주는 고성능의 BPE 를 이용할 수 있다.<br>\n<a href=\"https://github.com/google/sentencepiece\">SentencePiece</a></p>\n<ol start=\"2\">\n<li>soynlp</li>\n</ol>\n<p>한국어 전용 라이브러리 이다. 토크나이저 외 추출, 형태소분석, 전처리도 가능하다.<br>\n특히 미등록단어(사전에 없는 신조어 등) 을 구분해낼 수 있는 능력을 갖추고 있다.</p>\n<p>예를 들어 <code class=\"language-text\">에스파</code> 라는 단어가 들어왔다면,\n에, 에스, 에스파 중 가장 쓰였을 확률이 높은 단어로 결정한다.</p>\n<hr>\n<h2 id=\"4-분산표현-임베딩의-학습종류\" style=\"position:relative;\"><a href=\"#4-%EB%B6%84%EC%82%B0%ED%91%9C%ED%98%84-%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%98-%ED%95%99%EC%8A%B5%EC%A2%85%EB%A5%98\" aria-label=\"4 분산표현 임베딩의 학습종류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 분산표현 임베딩의 학습종류</h2>\n<h3 id=\"1-word2vec\" style=\"position:relative;\"><a href=\"#1-word2vec\" aria-label=\"1 word2vec permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Word2Vec</h3>\n<p>한 문장 안에 있는 단어들끼리에는 유사성이 더 있다 라는 가정에서 시작한 방식이다.\n<code class=\"language-text\">word2Vec</code> 은 CBOW(주변단어로 중간단어 예측) 학습방식과 Skip-gram(중간단어로 주변단어 예측) 학습방식이 존재한다.</p>\n<h3 id=\"2-fasttext\" style=\"position:relative;\"><a href=\"#2-fasttext\" aria-label=\"2 fasttext permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Fasttext</h3>\n<p>하나의 어휘를 n-gram 으로 더 쪼개어 더 정확한 예측을 한다.</p>\n<h3 id=\"3-elmo\" style=\"position:relative;\"><a href=\"#3-elmo\" aria-label=\"3 elmo permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. ELMo</h3>\n<p>동음이의어를 구분하지 못하는 word2Vec 의 단점을 보완했다.\n동음이의어를 구분하려면 그 주변의 문맥을 파악해야 하고, 이러한 특성이 반영된 임베딩을</p>\n<p>Contextualized Word Embedding 이라고 표현한다.</p>\n<p><img src=\"/14676ddd4510b651461ba8a936436cb2/image4.png\"></img></p>\n<p>이러한 방식은 임베딩값이 주변 단어에 따라 가변적인 속성을 띠게 되어\n배(boat, stomach) 가 쓰이는 맥락에 따라 다른 값을 표현한다.</p>\n<hr>\n<h2 id=\"5-정리\" style=\"position:relative;\"><a href=\"#5-%EC%A0%95%EB%A6%AC\" aria-label=\"5 정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. 정리</h2>\n<p>NLP 에서 데이터 전처리와 토큰화는 큰 비중을 차지한다.</p>\n<p>문장에 Noise 를 제거 <code class=\"language-text\">preprocessing</code> 하고,<br>\n제거한 문장을 적절히 <code class=\"language-text\">Tokenize</code> 하고,<br>\n분리한 Token 을 <code class=\"language-text\">word Embedding</code> 한다.</p>\n<p>Tokenize 과정에는 일반적으로 형태소 분석기를 사용한다.<br>\n대표적으로는 koNLP, SentencePiece 라이브러리 등을 사용한다.</p>\n<p>word Embdding 은 대표적으로 Word2Vec, FastText, ELMo 모델을 사용한다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#1-%EC%9E%A1%EC%9D%8C%EC%9D%B4-%EB%A7%8E%EC%9D%80-%EC%9E%90%EC%97%B0%EC%96%B4\">1. 잡음이 많은 자연어</a></p>\n<ul>\n<li><a href=\"#%EC%9E%A1%EC%9D%8C%EC%9D%98-%EC%A2%85%EB%A5%98\">잡음의 종류</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-preprocessing\">2. Preprocessing</a></p>\n</li>\n<li>\n<p><a href=\"#3-embeddng\">3. Embeddng</a></p>\n<ul>\n<li><a href=\"#%EC%9D%B4-%EC%99%B8-embedding-%EB%B0%A9%EB%B2%95\">이 외 Embedding 방법</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#4-%EB%B6%84%EC%82%B0%ED%91%9C%ED%98%84-%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%98-%ED%95%99%EC%8A%B5%EC%A2%85%EB%A5%98\">4. 분산표현 임베딩의 학습종류</a></p>\n<ul>\n<li><a href=\"#1-word2vec\">1. Word2Vec</a></li>\n<li><a href=\"#2-fasttext\">2. Fasttext</a></li>\n<li><a href=\"#3-elmo\">3. ELMo</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#5-%EC%A0%95%EB%A6%AC\">5. 정리</a></p>\n</li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","frontmatter":{"date":"March 15, 2022","title":"NLP 에서 데이터 전처리 및 토큰화란","categories":"NLP","author":"하성민","emoji":"😁"},"fields":{"slug":"/NLP_1/"}},"prev":{"id":"c1177e06-2486-565d-ab1e-fc6ef4f75243","html":"<h1 id=\"vectorize\" style=\"position:relative;\"><a href=\"#vectorize\" aria-label=\"vectorize permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vectorize</h1>\n<h1 id=\"contexts\" style=\"position:relative;\"><a href=\"#contexts\" aria-label=\"contexts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>contexts</h1>\n<ol>\n<li>BoW</li>\n<li>DTM</li>\n</ol>\n<ul>\n<li>코사인 유사도</li>\n</ul>\n<ol start=\"3\">\n<li>TF-IDF 가중치</li>\n<li>LSA</li>\n</ol>\n<ul>\n<li>특잇값 분해</li>\n</ul>\n<ol start=\"5\">\n<li>LDA</li>\n<li>soynlp</li>\n</ol>\n<ul>\n<li>응집확률</li>\n<li>브랜칭 엔트로피</li>\n</ul>\n<p>자연어 처리에서 꼭 필요한<br>\n텍스트 데이터의 Veoctorization 중에서도<br>\n통계와 머신러닝 사용하는 방법의 변천사에 대한 설명이다.</p>\n<p>토큰화하는 방식은\nWord2vec 임베딩 모델이 생기기 전<br>\n정통적인 Vectorize 방식이었다.</p>\n<hr>\n<p>현재는 Word2vec 기술을 사용하지만<br>\n단어를 학습시키기 위한 Vectorize 기술의 초기 개념을 이해하여<br>\nNLP 의 이해를 심화하도록 한다.</p>\n<hr>\n<p>컴퓨터는 단어를 이해하지 못한다.<br>\n때문에 단어를 숫자 데이터로 표현하여야 한다.<br>\n하지만 단순히 단어를 1, 2, 3, 4.. 로 표현한다면<br>\n단어간의 차이를 정확하게 나타내지 못할 것이다</p>\n<p>그러기 위해 vectorize 라는 기술이 생겨났다.<br>\n단어를 적절한 숫자 데이터로 변경하는 기술을 의미한다.</p>\n<p>다음은 이 vectorize 의 문제점과 그 해결책을\n발전 순서대로 나열한 내용이다.</p>\n<h2 id=\"1-bag-of-words-단어표현-방법\" style=\"position:relative;\"><a href=\"#1-bag-of-words-%EB%8B%A8%EC%96%B4%ED%91%9C%ED%98%84-%EB%B0%A9%EB%B2%95\" aria-label=\"1 bag of words 단어표현 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Bag of Words 단어표현 방법</h2>\n<blockquote>\n<p>단어들의 분포로 문서의 특성을 파악하는 기법 ; BoW</p>\n</blockquote>\n<p>한 말뭉치를 정리할때 순서는 무시하고 중복횟수를 중시한다.</p>\n<p>BoW 는 <code class=\"language-text\">Dic</code> 형태로 저장된다.\n각 Key 는 단어, value 는 빈도이다.</p>\n<p>BoW 는 다음의 라이브러리로 만들 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#1번방법 tensorflow 사용</span>\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n<span class=\"token comment\">#fit_on_texts()</span>\n\n\n<span class=\"token comment\">#2번방법 scikit learn 사용</span>\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n<span class=\"token comment\">#fit_transform() :빈도가 저장</span>\n<span class=\"token comment\">#vocabulary_ :단어들의 빈도 index 표현</span>\n<span class=\"token comment\">#해당 라이브러리에서는 빈도가 아닌 index 를 표현</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> Tokenizer\n\nsentence <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"</span><span class=\"token punctuation\">]</span>\n\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>fit_on_texts<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 단어장 생성</span>\nbow <span class=\"token operator\">=</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>word_counts<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 각 단어와 각 단어의 빈도를 bow에 저장</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Bag of Words :\"</span><span class=\"token punctuation\">,</span> bow<span class=\"token punctuation\">)</span> <span class=\"token comment\"># bow 출력</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'단어장(Vocabulary)의 크기 :'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>word_counts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 중복을 제거한 단어들의 개수</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> CountVectorizer\n\nsentence <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"</span><span class=\"token punctuation\">]</span>\n\nvector <span class=\"token operator\">=</span> CountVectorizer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nbow <span class=\"token operator\">=</span> vector<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toarray<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Bag of Words : '</span><span class=\"token punctuation\">,</span> bow<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 코퍼스로부터 각 단어의 빈도수를 기록한다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'각 단어의 인덱스 :'</span><span class=\"token punctuation\">,</span> vector<span class=\"token punctuation\">.</span>vocabulary_<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.</span></code></pre></div>\n<p>BoW의 집합으로 DTM 을 만들 수 있다.</p>\n<hr>\n<h2 id=\"2-dtm--document-term-matrix\" style=\"position:relative;\"><a href=\"#2-dtm--document-term-matrix\" aria-label=\"2 dtm  document term matrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. DTM ; document-Term Matrix</h2>\n<p>문서 단어 행렬</p>\n<p>여러 문서의 BoW 를 하나의 행렬로 구현한 것을 의미한다.\nDTM 은 문서를 row 로 단어를 col 로 가진다.</p>\n<p>단어를 row 로 문서를 col 로 지정한다면\nTDM 이라고 한다.</p>\n<p>가령,  세 개의 문서가 있다면\nDTM 은 다음과 같다.</p>\n<p><img src=\"/a87fb8568689c503c086526294178178/image1.png\"></img></p>\n<p>문서는 row 에, 단어들은 col 에 있다.</p>\n<p>이렇게 하면 각 문서의 BoW 는 없는 단어들도 0 값이 입력되어 구성된다.</p>\n<p>만약 문서가 많아진다면, 0값도 점점 많아질 것이다.</p>\n<p>DTM 을 이용해 kosine 유사도를 구할 수 있다.</p>\n<hr>\n<h3 id=\"kosine-유사도\" style=\"position:relative;\"><a href=\"#kosine-%EC%9C%A0%EC%82%AC%EB%8F%84\" aria-label=\"kosine 유사도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kosine 유사도</h3>\n<p>코사인 유사도는 문장들간의 유사성을 측정하기 위해 만들어진 방법이다.</p>\n<p><a href=\"https://wikidocs.net/24603\">코사인 유사도</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> numpy <span class=\"token keyword\">import</span> dot\n<span class=\"token keyword\">from</span> numpy<span class=\"token punctuation\">.</span>linalg <span class=\"token keyword\">import</span> norm\n\ndoc1 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 문서1 벡터</span>\ndoc2 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 문서2 벡터</span>\ndoc3 <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 문서3 벡터</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cos_sim</span><span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> dot<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">,</span> B<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>norm<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>norm<span class=\"token punctuation\">(</span>B<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'문서 1과 문서2의 유사도 :'</span><span class=\"token punctuation\">,</span>cos_sim<span class=\"token punctuation\">(</span>doc1<span class=\"token punctuation\">,</span> doc2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'문서 1과 문서3의 유사도 :'</span><span class=\"token punctuation\">,</span>cos_sim<span class=\"token punctuation\">(</span>doc1<span class=\"token punctuation\">,</span> doc3<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'문서 2와 문서3의 유사도 :'</span><span class=\"token punctuation\">,</span>cos_sim<span class=\"token punctuation\">(</span>doc2<span class=\"token punctuation\">,</span> doc3<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">문서 1과 문서2의 유사도 : 0.6666666666666667\n문서 1과 문서3의 유사도 : 0.6666666666666667\n문서 2와 문서3의 유사도 : 1.0000000000000002</code></pre></div>\n<p>만약 문장의 길이만을 비교할 수 있는 유클리드 거리로 유사도를 계산한다면<br>\n이게 무슨말일까 싶을 것이다.</p>\n<p>왜냐면 문장은 길이가 비슷하다고 같은 의미를 가지고 있는게 아니지 않는가?</p>\n<p>하지만 사용하는 단어가 유사하다면, 유사성을 가지고 있다고 표현할 수 있다.</p>\n<p>코사인 유사도란, 행렬의 수치 하나당 방향성을 가지고 있다고 가정하고,<br>\n그 방향의 코사인 값을 취해 각 문서가 얼마나 동일한 방향성을 가지고 있는지를 나타내는 방법이다.</p>\n<p>방향이 정반대일때는 -1\n같은 방향일때는 1의 값을 가진다.</p>\n<p>위의 코드를 보면 문서 2와 3의 유사도가 1이 나왔다.\n왜냐하면 [1,0,1,1] 과 [2,0,2,2] 가 크기는 다르지만 방향이 같다고 인식했기<br>\n때문이다.</p>\n<hr>\n<p>DTM 은 두가지 큰 문제점이 있다.</p>\n<ol>\n<li>저장공간을 0이 많이 차지한다.</li>\n<li>단순 빈도 수 만을 고려한다.</li>\n</ol>\n<p>이를 해결하기 위해 TF-IDF 가중치가 나타났다.</p>\n<hr>\n<h2 id=\"3-tf-idf\" style=\"position:relative;\"><a href=\"#3-tf-idf\" aria-label=\"3 tf idf permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. TF-IDF</h2>\n<p>; Term Frequency- Inverse Document Frequency</p>\n<p>단어 빈도 - 역 문서 빈도 를 뜻한다.</p>\n<p>DTM 은 TF 의 행렬이다. 그러므로</p>\n<p>기존 DTM 의 각 TF 에 IDF 를 곱하는 것이 TF-IDF가중치 의 사용법이다</p>\n<p>DF(문서빈도) 는 해당 단어가 나온 문서의 빈도를 의미한다.\n상단 예시에서 <code class=\"language-text\">business</code> 의 DF 는 2이다. 3문서 중 2문서에 출현했기 때문이다.</p>\n<p>그래서 공식을 이용하면 다음과 같다.</p>\n<p><img src=\"/a9773941abe596c2dba700a3ee08bd95/image2.png\"></img>\n문서 1과 2에 나왔으므로</p>\n<p>각 문서 별로 DF-IDF 값이 나온다, 각 문서당 TF 가 다르기 때문이다.</p>\n<p>허나 여기선 똑같은 1이므로</p>\n<p><img src=\"/1b3dc723c5e2a3749ff1084a2eb6e148/image3.png\"></img>\n이 된다.</p>\n<p>직접 TF-IDF 를 사용해보도록 하겠다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> math <span class=\"token keyword\">import</span> log\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\ndocs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n  <span class=\"token string\">'Ib wanna get some candy from Gary'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">'Gary protect Ib to save the art gallary'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">'Mary also likes to see Ib eating the candy'</span><span class=\"token punctuation\">,</span>  \n<span class=\"token punctuation\">]</span>\n\nvocab <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>w <span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> docs <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nvocab<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nN <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>docs<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 총 문서의 수</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'단어장의 크기 :'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">단어장의 크기 : 18\n['Gary', 'Ib', 'Mary', 'also', 'art', 'candy', 'eating', 'from', 'gallary', 'get', 'likes', 'protect', 'save', 'see', 'some', 'the', 'to', 'wanna']</code></pre></div>\n<p>Python 에서는 모든 식에 에러가 나지 않도록 조정이 필요하다.</p>\n<ol>\n<li>IDF 의 분모가 0이 되는 상황 방지 : 분모 +1</li>\n<li>log 의 분모분자가 같아 1이 되어 IDF 가 0이 되는 상황 방지 : IDF + 1</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">tf</span><span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\">#단어빈도</span>\n    <span class=\"token keyword\">return</span> d<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span>\n \n<span class=\"token keyword\">def</span> <span class=\"token function\">idf</span><span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\">#역문서빈도</span>\n    df <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> docs<span class=\"token punctuation\">:</span>\n        df <span class=\"token operator\">+=</span> t <span class=\"token keyword\">in</span> doc    \n    <span class=\"token keyword\">return</span> log<span class=\"token punctuation\">(</span>N<span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>df <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span> <span class=\"token comment\">#분모와 log 에 각각 1 더해줌</span>\n \n<span class=\"token keyword\">def</span> <span class=\"token function\">tfidf</span><span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tf<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span>d<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span> idf<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span> <span class=\"token comment\">#실제 계산</span></code></pre></div>\n<p>이전 문서를 가지고 DTM 을 만들면 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">result <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># 각 문서에 대해서 아래 명령을 수행</span>\n    result<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    d <span class=\"token operator\">=</span> docs<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        t <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span>\n        \n        result<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \ntf_ <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">,</span> columns <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">)</span>\ntf_</code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gary</th>\n      <th>Ib</th>\n      <th>Mary</th>\n      <th>also</th>\n      <th>art</th>\n      <th>candy</th>\n      <th>eating</th>\n      <th>from</th>\n      <th>gallary</th>\n      <th>get</th>\n      <th>likes</th>\n      <th>protect</th>\n      <th>save</th>\n      <th>see</th>\n      <th>some</th>\n      <th>the</th>\n      <th>to</th>\n      <th>wanna</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>각 단어의 IDF 를 구할 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">result <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    t <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span>\n    result<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>idf<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nidf_ <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">,</span> index <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">,</span> columns<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"IDF\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nidf_</code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Gary</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>Ib</th>\n      <td>0.712318</td>\n    </tr>\n    <tr>\n      <th>Mary</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>also</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>art</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>candy</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>eating</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>from</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>gallary</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>get</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>likes</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>protect</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>save</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>see</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>some</th>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>wanna</th>\n      <td>1.405465</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>IDF 는 모든 문서에 등장한 단어가 가장 낮은 값을 가진다.<br>\n그리고 독립적으로 등장한 단어가 1.4 로 가장 높은 값을 가진다.</p>\n<p>위를 보면, 한 번 나오는 단어는 1.4<br>\n두 번 나오는 단어는 1<br>\n세 번 나오는 단어는 0.7 의 값이 나온다.</p>\n<p>각 나온 IDF 값을 각 단어들의 빈도인 TF 와 곱하면 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">result <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    result<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    d <span class=\"token operator\">=</span> docs<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        t <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span>\n        \n        result<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>tfidf<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">,</span>d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntfidf_ <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">,</span> columns <span class=\"token operator\">=</span> vocab<span class=\"token punctuation\">)</span>\ntfidf_</code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gary</th>\n      <th>Ib</th>\n      <th>Mary</th>\n      <th>also</th>\n      <th>art</th>\n      <th>candy</th>\n      <th>eating</th>\n      <th>from</th>\n      <th>gallary</th>\n      <th>get</th>\n      <th>likes</th>\n      <th>protect</th>\n      <th>save</th>\n      <th>see</th>\n      <th>some</th>\n      <th>the</th>\n      <th>to</th>\n      <th>wanna</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.712318</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.405465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.712318</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.712318</td>\n      <td>1.405465</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.405465</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>Gary 의 IDF 를 각 문서의 TF 별로 곱했음을 알 수 있다.</p>\n<hr>\n<p>그러나 이것은 단어의 의미를 모르는 문제를 편법적으로 해소했을 뿐이지<br>\n실제로 단어의 의미를 깨우친 것은 아니다.</p>\n<p>그래서 그 다음에 나온 방법이</p>\n<p>LSA 이다.</p>\n<hr>\n<h2 id=\"4-lsa\" style=\"position:relative;\"><a href=\"#4-lsa\" aria-label=\"4 lsa permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. LSA</h2>\n<p>; Latent Sementic Analysis</p>\n<p>숨은 의미 분석</p>\n<p>이제부터는 문서 속 단어들간의 관계까지 찾아낼 수 있다.</p>\n<p>LSA 는 Singular Value Decompotion 을 이용한다.</p>\n<hr>\n<h3 id=\"singular-value-decompotion-특잇값-분해\" style=\"position:relative;\"><a href=\"#singular-value-decompotion-%ED%8A%B9%EC%9E%87%EA%B0%92-%EB%B6%84%ED%95%B4\" aria-label=\"singular value decompotion 특잇값 분해 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>singular Value Decompotion (특잇값 분해)</h3>\n<p><img src=\"/8b3904a7223cbf32836556240078ae75/image4.png\"></img>\nSVD 특잇값 분해는 고유값 분해의 변형이다.<br>\n원래 고유값 변형은 정방 행렬에서만 가능한데</p>\n<p>행과 열의 개수가 다른 <code class=\"language-text\">m</code>x<code class=\"language-text\">n</code>의 행렬에서 적당히 해주는 분해를 의미한다.</p>\n<p>그러면 이런식으로 아래나 오른쪽이 조금 비는 대각행렬 비스무리가 나온다</p>\n<p><img src=\"/8ab462c5499ae1bb5d0b03a2a0345491/image5.png\"></img></p>\n<p><code class=\"language-text\">truncated SVD </code> 는 그 분해를 가장 윗부분 <code class=\"language-text\">k</code> 개만 하는 것을 의미한다.<br>\ntruncated SVD 는 원본 행렬을 복구할 수 없다.</p>\n<p>LSA 는 우리가 만든 DTM 에 (혹은 TF-IDF 가중치 처리한 DTM 에 )<br>\ntruncated SVD 를 실시하여</p>\n<p>U : 특이값\nS : 특이벡터\nV : 특이값</p>\n<p>세가지 행렬로 분해할 수 있다.</p>\n<p><img src=\"/995fa8ec4ad6dd2ccb22ea7ae461475d/image6.png\"></img>\nU :  문서와 관련된 의미를 표현한 행렬\nS : 단어와 관련된 의미를 표현한 행렬\nV : 각 의미의 중요도를 표현한 행렬</p>\n<p>U 는 m(행) x k 의 행렬이다. 이 때 각 행은 문서 표현 벡터이다.\nV 는 k x n(열) 의 행렬이다. 이 때 각 열은 단어 표현 벡터이다.</p>\n<p>원래 DTM 에서는 단어 벡터의 크기가 m 이었는데 k 로 줄었음을 확인할 수 있다.</p>\n<p>V 는 전체 문서로부터 얻어낸 k 개의 주요 특징 이라고 할 수도 있다.</p>\n<p>우리는 이 특징(topic) 의 수 k 를 하이퍼 파라미터로 설정하고\nTruncated SVD 를 수행할 수 있다.</p>\n<hr>\n<p>k(주제) x n(단어수) 를 가진 V 에서</p>\n<p>각 행(주제) 마다 빈도수가 가장 많은 단어 5개를 꺼내면,<br>\n해당 주제의 요약을 얻을 수가 있다.</p>\n<p>이를 topic modelig 이라고 한다</p>\n<p>topic modeling 은 이 외에도 LDA 가 존재한다.</p>\n<hr>\n<h2 id=\"5-lda--latent-dirichlet-alloaction\" style=\"position:relative;\"><a href=\"#5-lda--latent-dirichlet-alloaction\" aria-label=\"5 lda  latent dirichlet alloaction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. LDA ; Latent Dirichlet Alloaction</h2>\n<p>잠재 디리클레 할당이라는 뜻이다.</p>\n<p>LDA 는 k (topic) 을 이용해</p>\n<p>각 topic 의 단어 분포와 (특정 topic 에 이 단어가 나타날 확률)</p>\n<p>각 문서의 topic 분포를 추정한다.</p>\n<p>주제가 몇개 있을 것임을 가정하고 단어분포를 찾는다는 의미에서 문서생성 과정을 역추정한다고도 말한다.</p>\n<p>LDA 의 과정은 다음과 같다.</p>\n<ol>\n<li>topic 의 수 k 정하기</li>\n<li>모든 단어를 k 개의 topic 중 하나에 allocation</li>\n<li>다음 단어를 넣을 때 확률과 분포에 따라 올바른 곳에 allocation</li>\n</ol>\n<hr>\n<p>LSA : DTM 의 차원을 축소해 근접 단어끼리 topic 으로 묶는다\nLDA : 단어가 특정 topic 에 들어갈 확률을 구해 나눈다.</p>\n<hr>\n<h2 id=\"6-비지도-학습-토크나이저-soynlp\" style=\"position:relative;\"><a href=\"#6-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-soynlp\" aria-label=\"6 비지도 학습 토크나이저 soynlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 비지도 학습 토크나이저 soynlp</h2>\n<p>지금까지의 방식은 모두 띄어쓰기만을 토대로\n단어들간의 분류 및 할당을 수행했다</p>\n<p>하지만 정확한 단어의 유사도를 파악하기 위해선<br>\n애초에 단어를 의미있게 분리해야 하는 작업이<br>\n선행되어야 한다.</p>\n<p>그래서 이 이후에 나온 방식이</p>\n<p>비지도 학습 토크나이저 방식이다.</p>\n<p>특히 우리나라 말은 영어처럼 띄어쓰기 토큰화는 절대 안된다.</p>\n<p>조사가 다 다르고 어미 어간도 다다르기 때문이다.</p>\n<p>사실 영어같은 경우는 띄어쓰기 토큰화를 해도 어느정도 들어맞는다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">en_text <span class=\"token operator\">=</span> <span class=\"token string\">\"My favorite fruit is orange because of salty taste and cheap price.\"</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>en_text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">['My', 'favorite', 'fruit', 'is', 'orange', 'because', 'of', 'salty', 'taste', 'and', 'cheap', 'price.']</code></pre></div>\n<p>하지만 한국어는 절대 안되므로 형태소 분석기를 이용한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> konlpy<span class=\"token punctuation\">.</span>tag <span class=\"token keyword\">import</span> Okt\n\nkor_text <span class=\"token operator\">=</span> <span class=\"token string\">'나 이제는 게임 안하려고, 더 재밌는 걸 찾아버렸지 뭐야. 그것은 바로 인공지능 NLP'</span>\n\n\ntokenizer <span class=\"token operator\">=</span> Okt<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>morphs<span class=\"token punctuation\">(</span>kor_text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">['나', '이제', '는', '게임', '안', '하려고', ',', '더', '재밌는', '걸', '찾아', '버렸지', '뭐', '야', '.', '그것', '은', '바로', '인공', '지능', 'NLP']</code></pre></div>\n<p>하지만 형태소분석기는 새로운 신조어나 고유명사를 알아내기가 힘들다는 단점이 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>morphs<span class=\"token punctuation\">(</span><span class=\"token string\">'어쩔티비 저쩔티비 크크루삥뽕 아무것도 못하쥬'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">['어쩔', '티비', '저', '쩔', '티비', '크', '크루', '삥뽕', '아무', '것', '도', '못하쥬']</code></pre></div>\n<p>크크루삥뽕은 하나의 말이지만 분리된 것을 볼 수가 있다.</p>\n<p>근데 이런것도 감안해서 크크루삥뽕이라는 말이 여러번 나왔을 때 이걸 하나의 고유명사라고 판단까지 해주는</p>\n<p>발전된 형태소 분석기 <code class=\"language-text\">soynlp</code> 가 나왔다.</p>\n<hr>\n<h3 id=\"soynlp-사용-방법\" style=\"position:relative;\"><a href=\"#soynlp-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95\" aria-label=\"soynlp 사용 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>soynlp 사용 방법</h3>\n<p>그것이 가능한 이유는 비지도 학습을 통해 형태소 분석을 하기 때문이다.</p>\n<p><code class=\"language-text\">soynlp</code> 는 내부의 단어 점수표가 있는데, 그 단어점수표는 다음의 두 가지 방식을 활용한다</p>\n<ol>\n<li>응집확률 (cohesion probability)</li>\n<li>브랜칭 엔트로피 (branching entropy)</li>\n</ol>\n<p>soynlp 는 깃허브에서 예제 말뭉치를 제공하고 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n\n<span class=\"token keyword\">import</span> urllib<span class=\"token punctuation\">.</span>request\n\ntxt_filename <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>getenv<span class=\"token punctuation\">(</span><span class=\"token string\">'HOME'</span><span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token string\">'/aiffel/goingDeeper/data/2016-10-20.txt'</span>\nurllib<span class=\"token punctuation\">.</span>request<span class=\"token punctuation\">.</span>urlretrieve<span class=\"token punctuation\">(</span><span class=\"token string\">\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\"</span><span class=\"token punctuation\">,</span>\\\n                            filename<span class=\"token operator\">=</span>txt_filename<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">('/aiffel/aiffel/goingDeeper/data/2016-10-20.txt',\n &lt;http.client.HTTPMessage at 0x7f2fce2959a0>)</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> soynlp <span class=\"token keyword\">import</span> DoublespaceLineCorpus\n\n<span class=\"token comment\"># 말뭉치에 대해서 다수의 문서로 분리</span>\ncorpus <span class=\"token operator\">=</span> DoublespaceLineCorpus<span class=\"token punctuation\">(</span>txt_filename<span class=\"token punctuation\">)</span>\n<span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">30091</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">i <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token keyword\">for</span> document <span class=\"token keyword\">in</span> corpus<span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>document<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>document<span class=\"token punctuation\">)</span>\n    i <span class=\"token operator\">=</span> i<span class=\"token operator\">+</span><span class=\"token number\">1</span>\n  <span class=\"token keyword\">if</span> i <span class=\"token operator\">==</span> <span class=\"token number\">3</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">break</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">19  1990  52 1 22\n오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진</code></pre></div>\n<p><code class=\"language-text\">soynlp</code> 는 비지도 학습 형태소 분석기 이기 때문에 해당 corpus 에 대한 학습을<br>\n선행시켜주어야 한다.</p>\n<p>이 학습 과정에서 응집확률과 브랜칭 엔트로피 단어 점수표를 만든다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> soynlp<span class=\"token punctuation\">.</span>word <span class=\"token keyword\">import</span> WordExtractor\n\nword_extractor <span class=\"token operator\">=</span> WordExtractor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nword_extractor<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span>\nword_score_table <span class=\"token operator\">=</span> word_extractor<span class=\"token punctuation\">.</span>extract<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#응집확률과 브랜칭 엔트로피 사용</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">training was done. used memory 1.141 Gb\nall cohesion probabilities was computed. # words = 223348\nall branching entropies was computed # words = 361598\nall accessor variety was computed # words = 361598</code></pre></div>\n<h3 id=\"응집확률--cohesion-probability\" style=\"position:relative;\"><a href=\"#%EC%9D%91%EC%A7%91%ED%99%95%EB%A5%A0--cohesion-probability\" aria-label=\"응집확률  cohesion probability permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>응집확률  cohesion probability</h3>\n<p>내부 문자열 substring 이 얼마나 모여 자주 등장하는지 판단하는 기준<br>\n이 값이 높을수록 그 주변 글자와 한 단어일 확률이 높아진다.</p>\n<p><img src=\"/1eeea11cf86b9dfd05c9efcfdcb825c7/image7.png\"></img>\n이처럼 바로 이전에 입력된 글자와의 확률을 구해 함께 오는지 안오는지를 구한다.</p>\n<p>다른 곳에서는 ‘반포한강공원’ 과 ‘에’ 의 응집확률이 낮게 나오므로 한 단어 기준이 정해진다.</p>\n<p><code class=\"language-text\">word_score_table[\"string\"].cohesion_forward</code>\n계산시 ‘반포한강공원’ 의 응집확률이 가장 최대치를 이룬다.</p>\n<h3 id=\"브랜칭-엔트로피\" style=\"position:relative;\"><a href=\"#%EB%B8%8C%EB%9E%9C%EC%B9%AD-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\" aria-label=\"브랜칭 엔트로피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>브랜칭 엔트로피</h3>\n<p>주어진 글자에서 다음 글자를 맞추는 데 헷갈리는 정도\n브랜칭 엔트로피는 주어지는 글자가 많을 수록 헷갈림이 적으므로 점점 낮아지게 된다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word_score_table<span class=\"token punctuation\">[</span><span class=\"token string\">\"디스\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>right_branching_entropy</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1.6371694761537934</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word_score_table<span class=\"token punctuation\">[</span><span class=\"token string\">\"디스플\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>right_branching_entropy</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">-0.0</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word_score_table<span class=\"token punctuation\">[</span><span class=\"token string\">\"디스플레\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>right_branching_entropy</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">-0.0</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word_score_table<span class=\"token punctuation\">[</span><span class=\"token string\">\"디스플레이\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>right_branching_entropy</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">3.1400392861792916</code></pre></div>\n<p>이처럼 브랜칭 엔트로피는 하나의 단어가 끝나가면서 점점 낮아졌다가<br>\n단어가 끝나면 조사 등 다른 것이 올 확률이 높아지므로(다시 헷갈려지므로)<br>\n수치가 다시 오른다.</p>\n<p>이 수치의 고저로 단어를 나눌 수 있다.</p>\n<p>결과적으로</p>\n<p>응집확률 과 브랜칭 엔트로피 를 사용해</p>\n<p>토크나이저 한다.</p>\n<p>그런데 soynlp 토크나이저도 두 가지 방식이 있다.</p>\n<ol>\n<li>L Tokenizer</li>\n</ol>\n<p>우리나라는 조사가 대부분 오른쪽에 붙는것을 감안해</p>\n<p>한 단어를 L+R 로 나누어\nL 의 중요도를 더 높인 tokenize</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> soynlp<span class=\"token punctuation\">.</span>tokenizer <span class=\"token keyword\">import</span> LTokenizer\n\nscores <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>word<span class=\"token punctuation\">:</span>score<span class=\"token punctuation\">.</span>cohesion_forward <span class=\"token keyword\">for</span> word<span class=\"token punctuation\">,</span> score <span class=\"token keyword\">in</span> word_score_table<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span>\nl_tokenizer <span class=\"token operator\">=</span> LTokenizer<span class=\"token punctuation\">(</span>scores<span class=\"token operator\">=</span>scores<span class=\"token punctuation\">)</span>\nl_tokenizer<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span><span class=\"token string\">\"국제사회와 우리의 노력들로 범죄를 척결하자\"</span><span class=\"token punctuation\">,</span> flatten<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]</code></pre></div>\n<p>하나의 tuple 안에 L 과 R 이 들어있는 것을 볼 수 있다.</p>\n<ol start=\"2\">\n<li>Max score Tokenizer</li>\n</ol>\n<p>띄어쓰기가 되어있지 않은 문장에서<br>\n가장 점수가 높은 단어의 중요도를 더 높인 tokenize</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> soynlp<span class=\"token punctuation\">.</span>tokenizer <span class=\"token keyword\">import</span> MaxScoreTokenizer\n\nmaxscore_tokenizer <span class=\"token operator\">=</span> MaxScoreTokenizer<span class=\"token punctuation\">(</span>scores<span class=\"token operator\">=</span>scores<span class=\"token punctuation\">)</span>\nmaxscore_tokenizer<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span><span class=\"token string\">\"국제사회와우리의노력들로범죄를척결하자\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']</code></pre></div>\n<p>해당 input 을 보면 띄어쓰기를 하지 않았음에도 충실히 나누게 된 것을 볼 수 있다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#1-bag-of-words-%EB%8B%A8%EC%96%B4%ED%91%9C%ED%98%84-%EB%B0%A9%EB%B2%95\">1. Bag of Words 단어표현 방법</a></p>\n</li>\n<li>\n<p><a href=\"#2-dtm--document-term-matrix\">2. DTM ; document-Term Matrix</a></p>\n<ul>\n<li><a href=\"#kosine-%EC%9C%A0%EC%82%AC%EB%8F%84\">Kosine 유사도</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#3-tf-idf\">3. TF-IDF</a></p>\n</li>\n<li>\n<p><a href=\"#4-lsa\">4. LSA</a></p>\n<ul>\n<li><a href=\"#singular-value-decompotion-%ED%8A%B9%EC%9E%87%EA%B0%92-%EB%B6%84%ED%95%B4\">singular Value Decompotion (특잇값 분해)</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#5-lda--latent-dirichlet-alloaction\">5. LDA ; Latent Dirichlet Alloaction</a></p>\n</li>\n<li>\n<p><a href=\"#6-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-soynlp\">6. 비지도 학습 토크나이저 soynlp</a></p>\n<ul>\n<li><a href=\"#soynlp-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95\">soynlp 사용 방법</a></li>\n<li><a href=\"#%EC%9D%91%EC%A7%91%ED%99%95%EB%A5%A0--cohesion-probability\">응집확률  cohesion probability</a></li>\n<li><a href=\"#%EB%B8%8C%EB%9E%9C%EC%B9%AD-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\">브랜칭 엔트로피</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","frontmatter":{"date":"March 25, 2022","title":"NLP 전처리 필수 Vecorize","categories":"회고","author":"하성민","emoji":"💍💍"},"fields":{"slug":"/NLP_3/"}},"site":{"siteMetadata":{"siteUrl":"https://xman227.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/NLP_2/","nextSlug":"/NLP_1/","prevSlug":"/NLP_3/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}