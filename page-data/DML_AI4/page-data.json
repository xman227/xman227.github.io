{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/DML_AI4/",
    "result": {"data":{"cur":{"id":"485c4944-42ec-5a62-b090-39cf2a45524f","html":"<h1 id=\"span-stylebackground-color-fff5b1딥러닝-네트워크를-구성하는-레이어-이게-뭘까span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff5b1%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EB%A5%BC-%EA%B5%AC%EC%84%B1%ED%95%98%EB%8A%94-%EB%A0%88%EC%9D%B4%EC%96%B4-%EC%9D%B4%EA%B2%8C-%EB%AD%98%EA%B9%8Cspan\" aria-label=\"span stylebackground color fff5b1딥러닝 네트워크를 구성하는 레이어 이게 뭘까span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff5b1'>딥러닝 네트워크를 구성하는 레이어, 이게 뭘까?</span></h1>\n<p>ANN : artificial NN 인공신경망</p>\n<p>딥러닝은 y = Wx + by=Wx+b 에서 최적의 <code class=\"language-text\">레이어 W(Weight)</code>과 <code class=\"language-text\">편향 b</code>를 찾는 과정!</p>\n<p>레이어 로 이루어져 있는데.</p>\n<p>레이어 : 하나의 물체가 여러개의 논리적인 세부 객체로 구성되어 있는 경우, 그 내부 객체를 이르는 말</p>\n<p>Linear</p>\n<p>Convolutional</p>\n<p>Embedding</p>\n<p>Recurrent</p>\n<p>이렇게 있음 레이어들이.</p>\n<p>Fully Connected Layer,<br>\nFeedforward Neural Network,<br>\nMultilayer Perceptrons,<br>\nDense Layer…</p>\n<p>등 다양한 이름으로 불리지만 그 모든 것들은 결국 Linear 레이어에 해당</p>\n<p>선형 대수학의 선형변환 (Linear Transform) 과 동일한 기능을 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n\nbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nboxes <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>     <span class=\"token comment\"># Tensorflow는 Batch를 기반으로 동작하기에,</span>\n                                         <span class=\"token comment\"># 우리는 사각형 2개 세트를 batch_size개만큼</span>\n                                         <span class=\"token comment\"># 만든 후 처리를 하게 됩니다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 연산 준비:\"</span><span class=\"token punctuation\">,</span> boxes<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nfirst_linear <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span> \n<span class=\"token comment\"># units은 출력 차원 수를 의미합니다.</span>\n<span class=\"token comment\"># Weight 행렬 속 실수를 인간의 뇌 속 하나의 뉴런 '유닛' 취급을 하는 거죠!</span>\n\nfirst_out <span class=\"token operator\">=</span> first_linear<span class=\"token punctuation\">(</span>boxes<span class=\"token punctuation\">)</span>\nfirst_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span>first_out<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># (4, 1)을 (4,)로 변환해줍니다.</span>\n                                           <span class=\"token comment\"># (불필요한 차원 축소)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 연산 결과:\"</span><span class=\"token punctuation\">,</span> first_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 Linear Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> first_linear<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n2단계 연산 준비:\"</span><span class=\"token punctuation\">,</span> first_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nsecond_linear <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nsecond_out <span class=\"token operator\">=</span> second_linear<span class=\"token punctuation\">(</span>first_out<span class=\"token punctuation\">)</span>\nsecond_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span>second_out<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"2단계 연산 결과:\"</span><span class=\"token punctuation\">,</span> second_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"2단계 Linear Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> second_linear<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1단계 연산 준비: (64, 4, 2)\n1단계 연산 결과: (64, 4)\n1단계 Linear Layer의 Weight 형태: (2, 1)\n\n2단계 연산 준비: (64, 4)\n2단계 연산 결과: (64,)\n2단계 Linear Layer의 Weight 형태: (4, 1)</code></pre></div>\n<p>축소만 하는 방식</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n\nbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nboxes <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 연산 준비:\"</span><span class=\"token punctuation\">,</span> boxes<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nfirst_linear <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nfirst_out <span class=\"token operator\">=</span> first_linear<span class=\"token punctuation\">(</span>boxes<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 연산 결과:\"</span><span class=\"token punctuation\">,</span> first_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1단계 Linear Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> first_linear<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n2단계 연산 준비:\"</span><span class=\"token punctuation\">,</span> first_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nsecond_linear <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nsecond_out <span class=\"token operator\">=</span> second_linear<span class=\"token punctuation\">(</span>first_out<span class=\"token punctuation\">)</span>\nsecond_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span>second_out<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"2단계 연산 결과:\"</span><span class=\"token punctuation\">,</span> second_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"2단계 Linear Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> second_linear<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n3단계 연산 준비:\"</span><span class=\"token punctuation\">,</span> second_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nthird_linear <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nthird_out <span class=\"token operator\">=</span> third_linear<span class=\"token punctuation\">(</span>second_out<span class=\"token punctuation\">)</span>\nthird_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>squeeze<span class=\"token punctuation\">(</span>third_out<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"3단계 연산 결과:\"</span><span class=\"token punctuation\">,</span> third_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"3단계 Linear Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> third_linear<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\ntotal_params <span class=\"token operator\">=</span> \\\nfirst_linear<span class=\"token punctuation\">.</span>count_params<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> \\\nsecond_linear<span class=\"token punctuation\">.</span>count_params<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> \\\nthird_linear<span class=\"token punctuation\">.</span>count_params<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"총 Parameters:\"</span><span class=\"token punctuation\">,</span> total_params<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1단계 연산 준비: (64, 4, 2)\n1단계 연산 결과: (64, 4, 3)\n1단계 Linear Layer의 Weight 형태: (2, 3)\n\n2단계 연산 준비: (64, 4, 3)\n2단계 연산 결과: (64, 4)\n2단계 Linear Layer의 Weight 형태: (3, 1)\n\n3단계 연산 준비: (64, 4)\n3단계 연산 결과: (64,)\n3단계 Linear Layer의 Weight 형태: (4, 1)\n총 Parameters: 13</code></pre></div>\n<p>한번 증가(2,3) 시켰다가 축소시키는 방식</p>\n<p>파라미터를 늘리면 (2,3) (3,1) (4,1) = 13  <code class=\"language-text\">(use_bias=False)</code> 일때\n더 많은 데이터를 보존할 수는 잇겟지만</p>\n<p><code class=\"language-text\">use_bias</code> 하면 3 + 1 + 1 해서 총 18 파라미터가 된다</p>\n<p>너무 많은 파라미터는 과적합을 초래한다.</p>\n<h3 id=\"convlutional-레이어\" style=\"position:relative;\"><a href=\"#convlutional-%EB%A0%88%EC%9D%B4%EC%96%B4\" aria-label=\"convlutional 레이어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>convlutional 레이어</h3>\n<p>필터(커널)를 만들어서 그 필터만큼의 픽셀값들을 다 곱한다음 더해 다음 레이어로 출력</p>\n<p><img src=\"attachment:image.png\" alt=\"image.png\"></p>\n<p>보통 3x3 사이즈 등의 커널을 만든다</p>\n<p>커널의 이동 사이즈를 stride 라고 부른다</p>\n<p>convolutional 레이어는 필터 개수 x필터 가로 x 필터 세로 로 이루어진 weight 값을 가진다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n\nbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\npic <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token number\">1920</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1080</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"입력 이미지 데이터:\"</span><span class=\"token punctuation\">,</span> pic<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\nconv_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span>\n                                    kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                    strides<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span>\n                                    use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nconv_out <span class=\"token operator\">=</span> conv_layer<span class=\"token punctuation\">(</span>pic<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nConvolution 결과:\"</span><span class=\"token punctuation\">,</span> conv_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Convolution Layer의 Parameter 수:\"</span><span class=\"token punctuation\">,</span> conv_layer<span class=\"token punctuation\">.</span>count_params<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nflatten_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>conv_out<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n1차원으로 펼친 데이터:\"</span><span class=\"token punctuation\">,</span> flatten_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nlinear_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nlinear_out <span class=\"token operator\">=</span> linear_layer<span class=\"token punctuation\">(</span>flatten_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nLinear 결과:\"</span><span class=\"token punctuation\">,</span> linear_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Linear Layer의 Parameter 수:\"</span><span class=\"token punctuation\">,</span> linear_layer<span class=\"token punctuation\">.</span>count_params<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">입력 이미지 데이터: (64, 1920, 1080, 3)\n\nConvolution 결과: (64, 384, 216, 16)\nConvolution Layer의 Parameter 수: 1200\n\n1차원으로 펼친 데이터: (64, 1327104)\n\nLinear 결과: (64, 1)\nLinear Layer의 Parameter 수: 1327104</code></pre></div>\n<h2 id=\"pooling-layer\" style=\"position:relative;\"><a href=\"#pooling-layer\" aria-label=\"pooling layer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>pooling layer</h2>\n<p>컨볼류셔널 레이어는 필터 사이즈에 의존하게 된다.</p>\n<p>근데 필터사이즈를 키우다보면 결국 컨볼루셔널 레이어의 정체성이 약해지는데</p>\n<p>그래서 필터사이즈가 아닌 receptive Field (수용 영역)을 키워야 한다.</p>\n<p>수용 영역:  출력 레이어의 뉴런 하나에 영향을 미치는 입력 뉴런들의 공간 크기\n(그럼 컨볼류셔널에서는 커널사이즈와 같다.)</p>\n<p>맥스풀링도 맥스풀링 사이즈가 나와야 되는 거 아닌가?</p>\n<p>맥스풀링을 하면 수용영역의 크기는 키울 수 잇지만,<br>\n파라미터 사이즈는 늘지 않는다.</p>\n<p>장점</p>\n<ol>\n<li>\n<p>translate invariance</p>\n</li>\n<li>\n<p>Non-linear 함수와 동일한 특징 추출 효과</p>\n</li>\n<li>\n<p>수용 영역 (receptive Field) 극대화 효과</p>\n</li>\n</ol>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li><a href=\"#convlutional-%EB%A0%88%EC%9D%B4%EC%96%B4\">convlutional 레이어</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#pooling-layer\">pooling layer</a></p>\n</li>\n</ul>\n</div>","excerpt":"딥러닝 네트워크를 구성하는 레이어, 이게 뭘까? ANN : artificial NN 인공신경망 딥러닝은 y = Wx + by=Wx+b 에서 최적의 과 를 찾는 과정! 레이어 로 이루어져 있는데. 레이어 : 하나의 물체가 여러개의 논리적인 세부 객체로 구성되어 있는 경우, 그 내부 객체를 이르는 말 Linear Convolutional Embedding Recurrent 이렇게 있음 레이어들이. Fully Connected Layer, Feedforward Neural Network, Multilayer Perceptrons, Dense Layer… 등 다양한 이름으로 불리지만 그 모든 것들은 결국 Linear 레이어에 해당 선형 대수학의 선형변환 (Linear Transform) 과 동일한 기능을 한다. 축소만 하는 방식 한번 증가(2,3) 시켰다가 축소시키는 방식 파라미터를 늘리면 (2,3) (3,1) (4,1) = 13   일때\n더 많은 데이터를 보존할 수는 잇겟지만  하면 3…","frontmatter":{"date":"April 21, 2022","title":"인공지능 기초 레이어 이해하기","categories":"beginner","author":"하성민","emoji":"😁"},"fields":{"slug":"/DML_AI4/"}},"next":{"id":"83c7066c-e6f4-5149-8931-eec9799d05b9","html":"<h1 id=\"span-stylebackground-color-fff5b1미리-학습된-딥러닝-pre-trained--deep-learning-사용처span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff5b1%EB%AF%B8%EB%A6%AC-%ED%95%99%EC%8A%B5%EB%90%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D-pre-trained--deep-learning-%EC%82%AC%EC%9A%A9%EC%B2%98span\" aria-label=\"span stylebackground color fff5b1미리 학습된 딥러닝 pre trained  deep learning 사용처span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff5b1'>미리 학습된 딥러닝 Pre-trained  Deep Learning 사용처</span></h1>\n<p>요즘 핫한 만큼 다양한 연구와 기법이 발전되고 있다</p>\n<p>DNN\n딥 뉴럴 네트워크</p>\n<p>더 좋은 딥 네트워크를 만들기 위해 많은 종류의 네트워크가 생겼다.<br>\n그 중 몇 가지 사전학습된 pre-trained Network 는\nTF 나 Pytorch 등의 프레임워크 차원으로 지원하고 있다.</p>\n<hr>\n<p>우리는 그 많은 모델들을 훑어볼 건데</p>\n<p>특히 ResNet 과 VGG 를 중심적으로 볼 거다</p>\n<h2 id=\"image-net\" style=\"position:relative;\"><a href=\"#image-net\" aria-label=\"image net permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Net</h2>\n<p>2010년 ILSVRC 2010 을 시작으로 대량의 이미지 데이터셋<br>\n만 개가 넘는 카테고리에 100만 장 규모의 사진을 가지고 있다.</p>\n<p>이걸 통해 많은 사람들이 이미지 분류 콘테스트에 나가 네트워크를 형성했다.</p>\n<ol>\n<li>AlexNet</li>\n</ol>\n<p>2011년 이미지넷 챌린지 1등 모델. 논문저자의 이름을 땄다.<br>\nCNN 구조의 확장판이다.<br>\n2개의 GPU 로 병렬연산을 수행하기 위해 병렬구조로 설계되었다.</p>\n<p><a href=\"https://bskyvision.com/421\">자세한 내용</a></p>\n<ul>\n<li>LeNet</li>\n</ul>\n<p>이건 이때 생긴건 아니지만 1998년에 개발한 CNN 알고리즘 이름이다.</p>\n<p>LeNet-5는 인풋, 3개의 컨볼루션 레이어(C1, C3, C5), 2개의 서브샘플링 레이어(S2, S4), 1층의 full-connected 레이어(F6), 아웃풋 레이어로 구성되어 있다. 참고로 C1부터 F6까지 활성화 함수로 tanh을 사용한다.</p>\n<ol start=\"2\">\n<li>VGG (VGG16, VGG19 등)</li>\n</ol>\n<p>2014년 이미지넷 챌린지 준우승 모델<br>\n이름처럼 16, 19개의 층을 이룸.<br>\n병렬구조가 아니다.</p>\n<hr>\n<p>근데 추세를 보면 계속 층이 깊어지는게<br>\n좋다고 하는데,  이게 또</p>\n<p>막 층을키운다고만 좋은게 아니다.</p>\n<p>부작용이 있다.</p>\n<ul>\n<li>vanishing gradient (또는 Exoloding Gradient)</li>\n</ul>\n<p>와 근데 이걸 해결한 것이</p>\n<ol start=\"3\">\n<li>ResNet</li>\n</ol>\n<p>2015년 이미지넷 챌린지 우승 모델\nSkip connection 이라는 구조로 해결\n: 레이어의 입력을 다른 곳에 이어서 Gradient 가 깊게 이어지도록 만드는 구조</p>\n<hr>\n<h2 id=\"이제는-실습으로-만들어보자\" style=\"position:relative;\"><a href=\"#%EC%9D%B4%EC%A0%9C%EB%8A%94-%EC%8B%A4%EC%8A%B5%EC%9C%BC%EB%A1%9C-%EB%A7%8C%EB%93%A4%EC%96%B4%EB%B3%B4%EC%9E%90\" aria-label=\"이제는 실습으로 만들어보자 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>이제는 실습으로 만들어보자</h2>\n<p><a href=\"https://github.com/keras-team/keras-applications/tree/master/keras_applications\">https://github.com/keras-team/keras-applications/tree/master/keras_applications</a></p>\n<p>그냥 여기에 다 담겨있다고 보면 된다</p>\n<p>keras 에서 지원하는 pre-trained model 이 담겨있다.\n굿굿 킹왕짱 굿굿 🚶‍♂️🧓👩👨</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#image-net\">Image Net</a></li>\n<li><a href=\"#%EC%9D%B4%EC%A0%9C%EB%8A%94-%EC%8B%A4%EC%8A%B5%EC%9C%BC%EB%A1%9C-%EB%A7%8C%EB%93%A4%EC%96%B4%EB%B3%B4%EC%9E%90\">이제는 실습으로 만들어보자</a></li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","frontmatter":{"date":"April 21, 2022","title":"pre-trained 모델 가져오는 링크","categories":"DeepML","author":"하성민","emoji":"😁"},"fields":{"slug":"/DML_keras3/"}},"prev":{"id":"c23336a2-9dde-5998-a74d-897defbfb439","html":"<h1 id=\"span-stylebackground-color-fff5b1정규화라고-다같은-정규화가-아니다span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff5b1%EC%A0%95%EA%B7%9C%ED%99%94%EB%9D%BC%EA%B3%A0-%EB%8B%A4%EA%B0%99%EC%9D%80-%EC%A0%95%EA%B7%9C%ED%99%94%EA%B0%80-%EC%95%84%EB%8B%88%EB%8B%A4span\" aria-label=\"span stylebackground color fff5b1정규화라고 다같은 정규화가 아니다span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff5b1'>정규화(라고 다같은 정규화가 아니다)</span></h1>\n<p>Regularization : 정칙화라고 불리며, 오버피팅을 해결하기 위한 방법 중의 하나</p>\n<p>Regularization 기법들은 모델이 train set의 정답을 맞히지 못하도록 오버피팅을 방해(train loss가 증가) 하는 역할을 합니다. 그래서 train loss는 약간 증가하지만 결과적으로, validation loss나 최종적인 test loss를 감소시키려는 목적</p>\n<p>(이건 오버피팅 방지)</p>\n<hr>\n<p>Normalization : 정규화라고 불리며, 이는 데이터의 형태를 좀 더 의미 있게, 혹은 트레이닝에 적합하게 전처리하는 과정</p>\n<p>(이건 전처리)</p>\n<p>예를 들어</p>\n<ol>\n<li>데이터를 z-socre 변환</li>\n<li>0 과 1 사이 값으로 분포 조정</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> load_iris\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd \n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n\niris <span class=\"token operator\">=</span> load_iris<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\niris_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>iris<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">,</span> columns<span class=\"token operator\">=</span>iris<span class=\"token punctuation\">.</span>feature_names<span class=\"token punctuation\">)</span>\ntarget_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>iris<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> columns<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'species'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 0, 1, 2로 되어있는 target 데이터를 </span>\n<span class=\"token comment\"># 알아보기 쉽게 'setosa', 'versicolor', 'virginica'로 바꿉니다 </span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">converter</span><span class=\"token punctuation\">(</span>species<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> species <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token string\">'setosa'</span>\n    <span class=\"token keyword\">elif</span> species <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token string\">'versicolor'</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token string\">'virginica'</span>\n\ntarget_df<span class=\"token punctuation\">[</span><span class=\"token string\">'species'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> target_df<span class=\"token punctuation\">[</span><span class=\"token string\">'species'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span>converter<span class=\"token punctuation\">)</span>\n\niris_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>concat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>iris_df<span class=\"token punctuation\">,</span> target_df<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\niris_df<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">X <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>iris_df<span class=\"token punctuation\">[</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> a <span class=\"token keyword\">in</span> iris_df<span class=\"token punctuation\">.</span>index <span class=\"token keyword\">if</span> iris_df<span class=\"token punctuation\">[</span><span class=\"token string\">'species'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">]</span><span class=\"token operator\">==</span><span class=\"token string\">'virginica'</span><span class=\"token punctuation\">]</span>\nY <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>iris_df<span class=\"token punctuation\">[</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> a <span class=\"token keyword\">in</span> iris_df<span class=\"token punctuation\">.</span>index <span class=\"token keyword\">if</span> iris_df<span class=\"token punctuation\">[</span><span class=\"token string\">'species'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">]</span><span class=\"token operator\">==</span><span class=\"token string\">'virginica'</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>Y<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[6.0, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5.0, 5.1, 5.3, 5.5, 6.7, 6.9, 5.0, 5.7, 4.9, 6.7, 4.9, 5.7, 6.0, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5.0, 5.2, 5.4, 5.1]\n[6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6.0, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6.0, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>Y<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'petal-sepal scatter before normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 336px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADyElEQVQ4y4VUTW8jRRCt2CsQAq0QYCyEIxkkUH5Dbpw4csmFA77APWIv7EqIEz8GbfgIKyGRBC+JVmID68RyPI42cWLHHnvGie2ZsT1jT38WqvbYsCyIlkr9untc/erVa8PBwQFsbW2t9Ho9QEQAgDwAvAYAWQDIJDgHAK8DwFsA8Eay9yYAvJ3MOfrtTc8BKJfLKQC4XSwWP+52u8V6vV5st9s7NF9dXe21Wq3di4uLYqvV2mm327vNZnOn53R22ra9c1g53WtcXv7idOzdYf9m7/Dx4w/Atu2067rguu49pRT+79AaR7E0cBhyVBox5BpHM47HlepnVGaK6FqWdZcxhkIIJoQQcRwLKaUJwpzzJXb9UGilBKIUZ44n2oMJ648iPCyVP1kmrNVq9yihnA8dxzFhlEoh7XMhkCqQnGN7MMHBeIbhNEYphEZEqaXAmlUtmISXl5fgOM4dzjmaQ621kmJZpZLzEpuDCKu2h2fuGIMpxxkzl5iEdLllWQXY3Nx8EQBulUqlr4gV51zSrd44xCCcYTRjeO1PcBYz9CYzvPHHxAq1koZ5HMeashGuVCoFyOVypuWu634uhGFFdPSMcXSCGU65xIvrEVqdAE86I3T96C/mSi0ZEq7VagXIZrPvAMAKaUgMhRBSSamVYFh3A5xMGQ5HIbb6Y3zqBBhNZ0hyCDlnyBhbMjw5OSlAJpN5nxKen59/wUhDrWUQMX3a8bDS9tDxp3jU6OOTxgClVHgThDiNSWtNl1OQ5JJwtVo1DN+lkh3HMSVrRPmkOdTEhkbIJF71J9jsh2Y9imKMhfzvkjOZzHsA8HK5XP6SsRhjxmXdDXTd8XAwnhrxJ9EUh+PI2IjNZTHxr03Z2Nh4yfM86Ha7xjZEn+o5c3xjDxqmWVobTN/MSeHcp1I+a5uFsU9PT42xEbV0/Ehf9Hyjk0qMLRJjJ+Y3mJJzzp/VkDGWThLeTV6H5EJoPu+eTl6NJvH/ianDFEopSclNl73hwCTsdOw7pBcixvMqjSnFAmutn8NKKROIyIixZVnmLafppfxROvqaLCHmZWDiySU27znBNC+aspCDZsuyPgXraT3166Pf4PfS0UfHtfOHnufdHwwG3zUajZ/6/f73QRBs2bb9YDgcfjuZTL6xbfvH6+vrH3zf3+p0Og/oLAiC+47j/Ly9vf0hLEbyb522bRuiKCL8Au0l+6/+Dd8GgNT+/j4s9o6Pjxdnryw3EXGFIvD9dBhG6WSdWltby+bz+Vw+n19dXV3N03p9fZ0uSCcOSbVarVuJdPAn2XYUFtkpGdIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/613dfaf87366d28da81864efc979bad9/d99f2/output_3_0.png\"\n        srcset=\"/static/613dfaf87366d28da81864efc979bad9/e9ff0/output_3_0.png 180w,\n/static/613dfaf87366d28da81864efc979bad9/d99f2/output_3_0.png 336w\"\n        sizes=\"(max-width: 336px) 100vw, 336px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>preprocessing <span class=\"token keyword\">import</span> minmax_scale\n\n<span class=\"token comment\">#sklearn 에서 지원하는 minmax_SCALE 로 0~ 1값으로 조정됨</span>\n\nX_scale <span class=\"token operator\">=</span> minmax_scale<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nY_scale <span class=\"token operator\">=</span> minmax_scale<span class=\"token punctuation\">(</span>Y<span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>X_scale<span class=\"token punctuation\">,</span>Y_scale<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'petal-sepal scatter after normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 330px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 101.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADmklEQVQ4y21U2U4cRxStATkKkfI0MIIo9uCED0keYsOj7SzKLzk/wRPihSdreIAw6WDMNBiQmATJJALP1t01vc7S01stN7o13WRIaKlVp1tVt845dyEkf87PzwkAIPyCEPI4f78ihCwSQqqEkC9n/lcIIU9m9iAuqfMAUMKPWq32reM4OqX0FFfTNE8tyzpzXVc3DOPUtu0TxJTSk8BzG8PA1z/cdk7aPeP9MPAa4+HgvPXx9hcMOIeRj46OfkrTFIqHMfYgBinAG8eQcgnDSQIp4xBMMvWv3TPrRNf1+c3NTWJZ1rMkSdT5aQzGAUC9s1hKwf0w4SmX6tsZxfzGCdNRnEHHsN4ohmjc/v7+j3EcYyTJOZcYHJlxziGOpzjNGEjOoOuOgA4iCMYTvA1Zc8EZ9CmtqUTgW6/Xf8CAGEwIITOULwUIIYCzDLWCPYrh/Y0NzY4HzjiG8QT3q0tRBVgYcH19faFWqxFK6fNcssDTSZopb4QEoIMJcCEhTDn4YQJSytxOUWCOq+M4NbKysrJCCFnQNO1VFEVKMuNcRlEMVhAq428sH/7s+nDVC+Av04csS5UVSCDLMsUQV8uyamRpaQlr6JNGo/ECN3AhpLpWcGg5Y7AGkZLW9UK4tcfgDidKBG7Lg6EtGBMoSi6Xy1+jh57nfVdITpiQ19YAmr0B3DghfDAD2LuiwCWAM4pgGE3Li3F+T7Jt24ohdsGnx8fHLzEgJuTKGMiLjw4kGYM4Y9BxhvA3HanEhHECQRirgAVDKeW/DBcXF58SQj47evfuJXqYpJm8tUfy2vDA9MO8bGIYhKqkALPPWHbPQ6zTOw83NjYWdF0npmk+x06ReZZ7XggXbf9/ncL5tJSm+AHJ2MvooaZp3yMTkEKOolT+0XFVWeBhvCg3v2CkAqWKLVNJwVVJ9n1f9fLh4eGrSVE2jMk0TVTRYtfNdg0GKbx7sGwuLy/nt7e3iWEYz1imspf9p5fvYTyMjAqMCQGANJf85m7aaJr2M9YYni2kIYuCVYGRaS5ztg7V2u/368U8nNvZ2fnm4uq6QWn/wHGcg1ar9bbf7//med6v3W73d/wXBMEeYkpp3XXdg3a7/dY0Tc113b1er3fabDZfFwO7mNafF8MC23EG49Sen8Gls7Ozuz27u7uIcWo9Kg6Qra0tzHaJCWXBXJH9tbW1cqVSqS4vLz8ul8tr1Wq1srq6WtE0bT7fM9fpdApc+gc0WAWZVrhbCQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/d878eef0f4b726da8c48e1f411322ba2/d9ecf/output_4_0.png\"\n        srcset=\"/static/d878eef0f4b726da8c48e1f411322ba2/e9ff0/output_4_0.png 180w,\n/static/d878eef0f4b726da8c48e1f411322ba2/d9ecf/output_4_0.png 330w\"\n        sizes=\"(max-width: 330px) 100vw, 330px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRegression\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np \n\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nY <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>Y<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Iris Dataset을 Linear Regression으로 학습합니다. </span>\nlinear<span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nlinear<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Linear Regression의 기울기와 절편을 확인합니다. </span>\na<span class=\"token punctuation\">,</span> b<span class=\"token operator\">=</span>linear<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">,</span> linear<span class=\"token punctuation\">.</span>intercept_\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"기울기 : %0.2f, 절편 : %0.2f\"</span> <span class=\"token operator\">%</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span>b<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">기울기 : 1.00, 절편 : 1.06</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>Y<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>linear<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">'-b'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'petal-sepal scatter with linear regression'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 336px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD5UlEQVQ4y4VUS28bVRQ+tSseUgUVJEoARzJIoPyG7FixZJMNC7yBfdRKiKLCjh+DWqBpJVCSWnmI0ipRbJPMuKkdJ/Y8Etfxa2LPeObOfRx07thWKEhc6Wq++/B3z3e+cwyICDQMw6DPGwDwPgC8CwAfAMA745kBgJsAMAcAswAwDwBvA8B7V9bX7MYxQKlUSgHAjc3NzS8sy9qp1+sbtm2vVyqVLdu2NyzLelyr1fKO46w5jrPeaDTWXNt6THjvr/LW6elJ/mXTXm/Um/m93Wefgud5aYrStu27Sin8v0F3RnFyrzti+lu3EZ0mx6OK+TUgYooIK5XKN1EUoRCCcc55FEVcCKEnYRoT7HZ9jig4GwmefzLkhzWfdYY+Pt0rfTklNAzjLmOMCGmoKNIYhZRI+5xzVFKiiGPsBD423BgLRYbegClEKQTnaJpGThPW63U4Ozu7HccxKRBKKSUEn8qUQiAqxPPLEZ5ceLj2bID7hzGGjO5LygIFgaZp5mBlZeV1ALheKBR+IMlxHNNryhsGOAgiDCKGXc9HIRm67QDXdwZYbxCR0JGHYaSIjfDBwUEOMplMhiRThCRL30RUQciwNYhwxARaPQ+LVR8f5H08cYOxOYhCSJRSkkNCSonlcjkH8/PzHxJhtVr9liRLKQUmIrD68hKZ5GgcRbjxxMeS1ccRC1FJpa9QAJxzTTiVPDMz8wkApMvl8p0wJJe58PxQnbS6WGp0Mf9ngA932njgXCDjHC/6QwzCSOc1YoxkTyUfHh7mYG5u7iOKsNls3tJOKhTGeU/tvuhjoaTQbQl0+wOstYZaat8PMYxFYpb8D8mzs7MfU6cUS6XvGYtQqlhs7vfU71sedrxQp9QbjLB9GegyYlE0kapxFL1iyvLy8puJKe5txmKsN5RwHVSVZg+Pmp6OJDEr6Y4kzwnWdSrEP3M4Kezj4+ffWVaMXk+JThCoWrOvSeSVwp5g+jFhIo/jWJctnRuGkYNut6t7uVx+fofqkB6LY67iJNkq6ZpIkZuvYjKEJlUGkWtTvH5fE7quc0tJnewoUal18glWSv0LSyn1RERGEZumqXs5TZ2yt1/48cLzkScyKNla5gST1AmmrzYlKZspNk3zKzBfHKe2/ngKu/uFz4vl6mav17vX6XR+OT09/a3dbv/qed59x3Eedbvdn4fD4U+O4zxstVoP+v3+fdd1H9GZ53n3zs/P11ZXVz+DyRj/c6cdx4EgCAi/Rnvj/ZtX8FsAkNre3obJXrFYnJzdmG4i4jWalFPfD9LjdWpxcXEum81mstnswsLCQpbWS0tL9EB6XCEpy7Kuj1MHfwOMqArQb760ogAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/2fc7c4278cbcf1ba6b256f6e838832b6/d99f2/output_6_0.png\"\n        srcset=\"/static/2fc7c4278cbcf1ba6b256f6e838832b6/e9ff0/output_6_0.png 180w,\n/static/2fc7c4278cbcf1ba6b256f6e838832b6/d99f2/output_6_0.png 336w\"\n        sizes=\"(max-width: 336px) 100vw, 336px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#L1 regularization은 Lasso로 import 합니다.</span>\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> Lasso\n\nL1 <span class=\"token operator\">=</span> Lasso<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nL1<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">)</span>\na<span class=\"token punctuation\">,</span> b<span class=\"token operator\">=</span>L1<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">,</span> L1<span class=\"token punctuation\">.</span>intercept_\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"기울기 : %0.2f, 절편 : %0.2f\"</span> <span class=\"token operator\">%</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span>b<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>Y<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>L1<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">'-b'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'petal-sepal scatter with L1 regularization(Lasso)'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">기울기 : 0.00, 절편 : 6.59</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 336px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD/ElEQVQ4y3VUW28bVRCexBUIgQCphIBIwCCB8hvyB+CNl7ziB+A9ah+ASognHvJTopZLqGiVJlUbqRVqi+s0ydqJY3sv3nidxPZ6413be66D5ngdcRFHGs3nOWe/c2bmG8Pa2hrYtj0DAO8BwDsA8AYAvA8ArwPAuwDwFgDMZf5lALgKAB9ksbezM3OZXQXXdWeJZH19/Wvbth81m83NWq320HGce47j3Pc8b8t13U3yvu/fazQa206jvtNsNreevKjcd113q+V7203XeXz3zu+fged5uSAIoNPpfKO1xun6P0wrZsr4XsJRasSEa4xTiXtW5UtAxFlEBMuyvkvTFKWUTAgh0jQVUkpjhDnnl7jdT4RWSiBKUQ1C4XVj1omG+KRY+uKSsFwu32CMESEtnZGjVAopzoVApRRKzrHZjbE7GGMySlEKQc+XWgosWwcFQ9hoNCAIguucczSbWmspxWWKSkrjne4QD/wQq+0B9kccx8xcYgjpcsuyCrC6ukqdu1IsFn9gjF7FpVJCR3GCg9EYR4xhJ4oxZQz7wzF2ogEqukxLFJwhY6lWSkrKYm9vrwALCwsLlPLpafvaeCyQsmYM9UXM0TsfY5RIPPQvsOREWLQv0DsbIvExjjgeKzLNOVVCYaVSLsD8/PyHADBTrZZvtFop1utCeq7Uns3xwZ8RVqoM98pDfFwa4PbTCI+qY3QdgbYjsV5n2Ggw3WhI2W4zPDzcL8Dc3NwnRHh8fPytlFRDLZOU6eN2iOUgxLPBCF80O7jb7FI1sRcnyCbnUGtBpukbavrBwYF54UeUchAE14SgAyif2T191IoIY5JKdM9jtDuJaUyUpDjmkyZJqchMU0gB5XLZvPBjAHh1d3f3e2pKyristSNdC0LsDkaolcR4OMLeYGhkxNIU6WIyakSapppafNmUlZWVV8IwhFarZWRDiqF8qkHfyIMWfYzZtNCZiVJwolMp/ymbqbArlYoRNtUj6A91/bRv6qQyYYtM2Jn4DSZyzrmRLe2bGjLGchnhdPQkF0JzxigVnU2NFkL8BzPGjCmlJJHv7+8XIOx1DeHJiX+d6oWI6SRLyhPFFGtq6b+wUsoYIjJ6sWVZZpZzNCnPis9/PI8SFJM0qNgmzSk285xh8tOmTMtB3rKsr8A6qs0+fPQHPC0+/7xUPn4QhuHNbrf7s23bdzqdzi9RFN3yff92r9f7KY7jdd/3fzs7O/u13+/fOjk5uU17URTdDIJgc2Nj41OYLkobAHK+78NwOCT8EsWy+Jt/w/RPPruzswPTWKlUmu69dhlExBmyqN/PJckwl/2eXVpams/n8wv5fH5xcXExT7+Xl5fpglymkFnP865kpYO/AHq4BOl4qIQaAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/c1c3d0dc2cedc5cab88e3cfd8fca76c0/d99f2/output_7_1.png\"\n        srcset=\"/static/c1c3d0dc2cedc5cab88e3cfd8fca76c0/e9ff0/output_7_1.png 180w,\n/static/c1c3d0dc2cedc5cab88e3cfd8fca76c0/d99f2/output_7_1.png 336w\"\n        sizes=\"(max-width: 336px) 100vw, 336px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이게 Lasso 방식이고</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">#L2 regularization은 Ridge로 import 합니다. </span>\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> Ridge\n\nL2 <span class=\"token operator\">=</span> Ridge<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nL2<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">)</span>\na<span class=\"token punctuation\">,</span> b <span class=\"token operator\">=</span> L2<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">,</span> L2<span class=\"token punctuation\">.</span>intercept_\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"기울기 : %0.2f, 절편 : %0.2f\"</span> <span class=\"token operator\">%</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span>b<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>Y<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span>L2<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">'-b'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'petal-sepal scatter with L2 regularization(Ridge)'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'petal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'sepal length (cm)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">기울기 : 0.93, 절편 : 1.41</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 336px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD60lEQVQ4y3VUTW8bVRS9sStQBQIkMAaRgEEC9TfkD7BgwSZbsgD2UbOAVkKsWOSnRC2koRIlSSNqqdCEynHjZJwmcTzjsZ2xnYlnJvbYnnlfF93nsQUFnvR0z1zPO3PPvecZVlZWwDTNGQB4DwDeAYDXAeB9AHgNAN4FgLcAIJPE6wDwJgB8kOTeTs7Q2Q91rlarpYhkdXX1a9M0H9fr9Y1KpfLIsqxNy7K2bdveqtVqmxTr9fpWtVp9aFXP8oR39o+2Lcvaci+aG2a1snNvbe0zsG077TgOuK77jVIKJ+v/MK1+LHXsDpiOjTaidS7x5Oz4S0DEFCKCYRi3oihCIUTMOedRFHEhhN6EGWNT7HRDjig4CsHzu31eej6KO36Iu4XiF1PCcrl8O45jIqSlEnIUUiLlGafzEjlj2Or18bwT496zGF2XK0QllORoHB4uasJqtQqO4ywzpiUIpZSSgk8lSil0tL0BPm95+PDpFRZLAocjnryu60DDMBZhaWnpZQC4VigUvqeqGGNCcK68XohBOMJwFGPb6yMXEba9Ieb/CLFWIyKOjMU4GkWK2EhFqVRahNnZ2VmS3Gq1bnKuq6Jy1Chm6PgjHDKJtneFT4werm728LQejgdFL0qJUsoESiyXy4uQzWbJPzPUQ6qQcy6kEEqwGE03wP4owr3SEPO7fTxxAozYCAXnyIXQvY3jeFrhwcHBImQymU+I8PT09NuYMbKI8MNYnXU83D3xMb8T4YMdF/ebrq6IpjmMmK6RFHHOqeWC8CENJZvNfkSSHce5ydlY8n6zq/LFAA8OEP2eROeqh+bFWOrVIMKIi2RY/yE5k8l8DACvFIvPvhMiwlHExK+/X6ntHR+9cKhb2guH2O0NtI3icVv0JplR9MJQFhYWroehB+32+bLnMaxUlAh7qEzXx+NWT1eih5XcFrLWuCgc+1QINYaJbSbGPjk5ut1qMxRMCbc/UBXH132SibGJdILpMGEiZ4z9s4dxHKeJ8Ojo6BbnEZlYMMYVG09PJbdGUfNfxDRh2lJKQeR6yl73UhM2m41lKXSzo7FKPnZvgpVS/8JSSr0RMaaKDcPQdzlNN+VpYe+HiyDUd5W+lnhyivV9TjDFyVAm7aBoGMZXYBxXUo8eP4E/C3ufF8unv3med+fy8vIn0zR/cV13LQiCu41G43632/2x3++vNhqNnzudzj3f9+82m8379FsQBHccx9lYX1//FCaLZANAutFowGAwIPwS5ZL8G3/D9E+eyufzMMkVi8XJb69Ok4g4Qzvw/XQYDtLJc+rGjRvZXC43m8vl5ubm5nL0PD8/Tx9IJw5J2bZ9LWkd/AUa/wOfIeMgQQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/6ffbbbf6f87c1e36c5b43fd3ea8ac4d5/d99f2/output_9_1.png\"\n        srcset=\"/static/6ffbbbf6f87c1e36c5b43fd3ea8ac4d5/e9ff0/output_9_1.png 180w,\n/static/6ffbbbf6f87c1e36c5b43fd3ea8ac4d5/d99f2/output_9_1.png 336w\"\n        sizes=\"(max-width: 336px) 100vw, 336px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이게 Ridge 방법<br>\n기존 방법보다 축은 위로 쫌 이동했지만</p>\n<p>기울기가 좀 줄었다</p>\n<p>이 두 방식은 Regularization\n다시말해 오버피팅을 방지한 것이다</p>\n<hr>\n<p>, L1 Regularization을 사용할 때는 X가 2차원 이상인 여러 컬럼 값이 있는 데이터일 때 실제 효과를 볼 수 있습니다.</p>\n<p>x가 1차원이었던 iris 꽃잎길이데이터같은 따분한거 쓰지말고</p>\n<p>어른의 데이터인 wine dataset을 볼까?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> load_wine\n\nwine <span class=\"token operator\">=</span> load_wine<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nwine_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>wine<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">,</span> columns<span class=\"token operator\">=</span>wine<span class=\"token punctuation\">.</span>feature_names<span class=\"token punctuation\">)</span>\ntarget_df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>wine<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> columns<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'Y'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">wine_df<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">target_df<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRegression\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error<span class=\"token punctuation\">,</span> mean_squared_error\n\n<span class=\"token comment\"># 데이터를 준비하고</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>wine_df<span class=\"token punctuation\">,</span> target_df<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">101</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 모델을 훈련시킵니다.</span>\nmodel <span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트를 해볼까요?</span>\nmodel<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\npred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트 결과는 이렇습니다!</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result of linear regression\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Absolute Error:'</span><span class=\"token punctuation\">,</span> mean_absolute_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Squared Error:'</span><span class=\"token punctuation\">,</span> mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Root Squared Error:'</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\\n coefficient linear regression\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">result of linear regression\nMean Absolute Error: 0.25128973939722626\nMean Squared Error: 0.1062458740952556\nMean Root Squared Error: 0.32595379134971814\n\n\n coefficient linear regression\n[[-8.09017190e-02  4.34817880e-02 -1.18857931e-01  3.65705449e-02\n  -4.68014203e-04  1.41423581e-01 -4.54107854e-01 -5.13172664e-01\n   9.69318443e-02  5.34311136e-02 -1.27626604e-01 -2.91381844e-01\n  -5.72238959e-04]]</code></pre></div>\n<p>선형회귀로 문제를 풀고</p>\n<p>계수(coefficient)</p>\n<p>절대 오차 ( mean absolute error)</p>\n<p>제곱 오차 ( mean squared error)</p>\n<p>평균 제곱값 오차 (root mean squared error)</p>\n<p>를 출력</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRegression\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error<span class=\"token punctuation\">,</span> mean_squared_error\n\n<span class=\"token comment\"># 데이터를 준비하고</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>wine_df<span class=\"token punctuation\">,</span> target_df<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">101</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 모델을 훈련시킵니다.</span>\nmodel <span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트를 해볼까요?</span>\nmodel<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\npred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트 결과는 이렇습니다!</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result of linear regression\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Absolute Error:'</span><span class=\"token punctuation\">,</span> mean_absolute_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Squared Error:'</span><span class=\"token punctuation\">,</span> mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Root Squared Error:'</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\\n coefficient linear regression\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">result of linear regression\nMean Absolute Error: 0.25128973939722626\nMean Squared Error: 0.1062458740952556\nMean Root Squared Error: 0.32595379134971814\n\n\n coefficient linear regression\n[[-8.09017190e-02  4.34817880e-02 -1.18857931e-01  3.65705449e-02\n  -4.68014203e-04  1.41423581e-01 -4.54107854e-01 -5.13172664e-01\n   9.69318443e-02  5.34311136e-02 -1.27626604e-01 -2.91381844e-01\n  -5.72238959e-04]]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> Lasso\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error<span class=\"token punctuation\">,</span> mean_squared_error\n\n<span class=\"token comment\"># 모델을 준비하고 훈련시킵니다.</span>\nL1 <span class=\"token operator\">=</span> Lasso<span class=\"token punctuation\">(</span>alpha<span class=\"token operator\">=</span><span class=\"token number\">0.05</span><span class=\"token punctuation\">)</span>\nL1<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트를 해봅시다.</span>\npred <span class=\"token operator\">=</span> L1<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 모델 성능은 얼마나 좋을까요?</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result of Lasso\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Absolute Error:'</span><span class=\"token punctuation\">,</span> mean_absolute_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Squared Error:'</span><span class=\"token punctuation\">,</span> mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Mean Root Squared Error:'</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\n\\n coefficient of Lasso\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>L1<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">result of Lasso\nMean Absolute Error: 0.24233731936122138\nMean Squared Error: 0.0955956894578189\nMean Root Squared Error: 0.3091855259513597\n\n\n coefficient of Lasso\n[-0.          0.01373795 -0.          0.03065716  0.00154719 -0.\n -0.34143614 -0.          0.          0.06755943 -0.         -0.14558153\n -0.00089635]</code></pre></div>\n<p>coefficient 부분을 보시면 Linear Regression과 L1 Regularization의 차이가 좀 더 두드러짐</p>\n<p>inear Regression에서는 모든 컬럼의 가중치를 탐색하여 구하는 반면, L1 Regularization에서는 총 13개 중 7개를 제외한 나머지의 값들이 모두 0임</p>\n<h2 id=\"l2-norm--ridge\" style=\"position:relative;\"><a href=\"#l2-norm--ridge\" aria-label=\"l2 norm  ridge permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L2 norm  Ridge</h2>\n<p><img src=\"attachment:image.png\" alt=\"image.png\"></p>\n<p>보면 L1 은 걍 절댓값만 씌우고</p>\n<p>L2 는 제곱을 해,,! 그 차이야,,!!</p>\n<p><img src=\"attachment:image-2.png\" alt=\"image-2.png\"></p>\n<p>L2 는 제곱을 하기 때문에 저렇게 원의 형태가 나와</p>\n<p>하지만 L1 은 사각형이지</p>\n<p>제곱이라 수렴 속도도 빠름 더 가까운 길을 찾을 수 있잖아</p>\n<p>정리하면, L1 Regularization은 가중치가 적은 벡터에 해당하는 계수를 0으로 보내면서 차원 축소와 비슷한 역할을 하는 것이 특징이며, L2 Regularization은 0이 아닌 0에 가깝게 보내지만 제곱 텀이 있기 때문에 L1 Regularization보다는 수렴 속도가 빠르다는 장점</p>\n<p>예를 들어, A=[1,1,1,1,1]A=[1,1,1,1,1] , B=[5,0,0,0,0]B=[5,0,0,0,0] 의 경우<br>\nL1-norm은 같지만, L2-norm은 같지 않습니다.<br>\n즉, 제곱 텀에서 결과에 큰 영향을 미치는 값은 더 크게,<br>\n결과에 영향이 적은 값들은 더 작게 보내면서 수렴 속도가 빨라지는 것입니다.</p>\n<h4 id=\"그러므로-데이터에-따라-적절한-regularization-방법을-활용하는-것이-좋습니다\" style=\"position:relative;\"><a href=\"#%EA%B7%B8%EB%9F%AC%EB%AF%80%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90-%EB%94%B0%EB%9D%BC-%EC%A0%81%EC%A0%88%ED%95%9C-regularization-%EB%B0%A9%EB%B2%95%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%98%EB%8A%94-%EA%B2%83%EC%9D%B4-%EC%A2%8B%EC%8A%B5%EB%8B%88%EB%8B%A4\" aria-label=\"그러므로 데이터에 따라 적절한 regularization 방법을 활용하는 것이 좋습니다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>그러므로, 데이터에 따라 적절한 Regularization 방법을 활용하는 것이 좋습니다.</h4>\n<h2 id=\"근데-그래서-norm-이란게-뭘까\" style=\"position:relative;\"><a href=\"#%EA%B7%BC%EB%8D%B0-%EA%B7%B8%EB%9E%98%EC%84%9C-norm-%EC%9D%B4%EB%9E%80%EA%B2%8C-%EB%AD%98%EA%B9%8C\" aria-label=\"근데 그래서 norm 이란게 뭘까 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>근데 그래서 Norm 이란게 뭘까…?</h2>\n<ol>\n<li>vector norm</li>\n<li>matrix norm</li>\n</ol>\n<p>Norm이라는 개념은 벡터뿐만 아니라 함수, 행렬에 대해서 크기를 구하는 것으로, 딥러닝을 배우는 과정에서는 주로 벡터, 좀 더 어렵게는 행렬의 Norm 정도만 알면 됩니다.</p>\n<p><img src=\"attachment:image.png\" alt=\"image.png\"></p>\n<ol>\n<li>vector</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\np<span class=\"token operator\">=</span><span class=\"token number\">5</span>\n\nnorm_x<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>norm<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token builtin\">ord</span><span class=\"token operator\">=</span>p<span class=\"token punctuation\">)</span>\n\nmaking_norm <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span>p<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token operator\">**</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span>p<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result of numpy package norm function : %0.5f \"</span><span class=\"token operator\">%</span>norm_x<span class=\"token punctuation\">)</span> \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result of making norm : %0.5f \"</span><span class=\"token operator\">%</span>making_norm<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">result of numpy package norm function : 10.00008 \nresult of making norm : 10.00008 </code></pre></div>\n<ol start=\"2\">\n<li>matrix</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">A<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\ninf_norm_A<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>norm<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">,</span> <span class=\"token builtin\">ord</span><span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>inf<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result inf norm of A :\"</span><span class=\"token punctuation\">,</span> inf_norm_A<span class=\"token punctuation\">)</span>\none_norm_A<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>norm<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">,</span> <span class=\"token builtin\">ord</span><span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"result one norm of A :\"</span><span class=\"token punctuation\">,</span> one_norm_A<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">result inf norm of A : 18.0\nresult one norm of A : 14.0</code></pre></div>\n<h2 id=\"dropout-은-뭔데\" style=\"position:relative;\"><a href=\"#dropout-%EC%9D%80-%EB%AD%94%EB%8D%B0\" aria-label=\"dropout 은 뭔데 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dropout 은 뭔데?</h2>\n<p><a href=\"https://jmlr.org/papers/v15/srivastava14a.html\">Dropout 논문</a></p>\n<p>Dropout 은 Regularization 으로 오버피팅을 막는 정칙화 이다.</p>\n<p>fully connected layer에서 오버피팅이 생기는 경우에 주로 Dropout layer를 추가합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">from</span> tensorflow <span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n\nfashion_mnist <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>fashion_mnist\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'=3'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">=3</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>test_images<span class=\"token punctuation\">,</span> test_labels<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> fashion_mnist<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclass_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'T-shirt/top'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Trouser'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Pullover'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Dress'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Coat'</span><span class=\"token punctuation\">,</span>\n               <span class=\"token string\">'Sandal'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Shirt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Sneaker'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Bag'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Ankle boot'</span><span class=\"token punctuation\">]</span>\n\ntrain_images <span class=\"token operator\">=</span> train_images <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span>\ntest_images <span class=\"token operator\">=</span> test_images <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 아래의 실습과 같습니다.</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory<span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.4060 - accuracy: 0.4571\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1796 - accuracy: 0.5296\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1358 - accuracy: 0.5459\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1104 - accuracy: 0.5530\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0902 - accuracy: 0.5628</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 이번에는 dropout layer가 없습니다. </span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.5047 - accuracy: 0.8254\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3759 - accuracy: 0.8648\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3370 - accuracy: 0.8777\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3138 - accuracy: 0.8853\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2967 - accuracy: 0.8910</code></pre></div>\n<p>보면 드랍아웃을 0.9로 주면 정확도가 56% 이다</p>\n<p>아무것도 안한게 89% 인데 ㅋ 어이없어 저렇겐 쓰지마</p>\n<p>근데 오버피팅 줄일때 써봐</p>\n<h3 id=\"overfitting-줄이는-법\" style=\"position:relative;\"><a href=\"#overfitting-%EC%A4%84%EC%9D%B4%EB%8A%94-%EB%B2%95\" aria-label=\"overfitting 줄이는 법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>overfitting 줄이는 법</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">X_train<span class=\"token punctuation\">,</span> X_valid<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_valid <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">101</span><span class=\"token punctuation\">)</span>\nX_train <span class=\"token operator\">=</span> X_train <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span>\nX_valid <span class=\"token operator\">=</span> X_valid <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span>\n\n<span class=\"token comment\">#Dense layer만으로 만들어 낸 classification 모델입니다.</span>\nmodel <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory<span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>X_valid<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/200\n117/117 [==============================] - 1s 5ms/step - loss: 2.0499 - accuracy: 0.5479 - val_loss: 1.6354 - val_accuracy: 0.5983\nEpoch 2/200\n117/117 [==============================] - 0s 4ms/step - loss: 1.3857 - accuracy: 0.6154 - val_loss: 1.1556 - val_accuracy: 0.6817\nEpoch 3/200\n117/117 [==============================] - 0s 4ms/step - loss: 1.0562 - accuracy: 0.6812 - val_loss: 0.9326 - val_accuracy: 0.7333\nEpoch 4/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.8842 - accuracy: 0.7192 - val_loss: 0.8094 - val_accuracy: 0.7483\nEpoch 5/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.7847 - accuracy: 0.7343 - val_loss: 0.7366 - val_accuracy: 0.7650\nEpoch 6/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.7235 - accuracy: 0.7471 - val_loss: 0.6882 - val_accuracy: 0.7717\nEpoch 7/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.7565 - val_loss: 0.6565 - val_accuracy: 0.7850\nEpoch 8/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.6518 - accuracy: 0.7663 - val_loss: 0.6299 - val_accuracy: 0.7917\nEpoch 9/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6271 - accuracy: 0.7739 - val_loss: 0.6132 - val_accuracy: 0.7933\nEpoch 10/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.7824 - val_loss: 0.5906 - val_accuracy: 0.7950\nEpoch 11/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5883 - accuracy: 0.7897 - val_loss: 0.5751 - val_accuracy: 0.7967\nEpoch 12/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.7971 - val_loss: 0.5648 - val_accuracy: 0.8017\nEpoch 13/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.8016 - val_loss: 0.5587 - val_accuracy: 0.8000\nEpoch 14/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5459 - accuracy: 0.8068 - val_loss: 0.5408 - val_accuracy: 0.8033\nEpoch 15/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.8118 - val_loss: 0.5273 - val_accuracy: 0.8000\nEpoch 16/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5256 - accuracy: 0.8148 - val_loss: 0.5238 - val_accuracy: 0.8117\nEpoch 17/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.8192 - val_loss: 0.5124 - val_accuracy: 0.8117\nEpoch 18/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.8216 - val_loss: 0.5084 - val_accuracy: 0.8117\nEpoch 19/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.8243 - val_loss: 0.5048 - val_accuracy: 0.8133\nEpoch 20/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.8270 - val_loss: 0.4947 - val_accuracy: 0.8183\nEpoch 21/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4868 - accuracy: 0.8289 - val_loss: 0.4901 - val_accuracy: 0.8167\nEpoch 22/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4807 - accuracy: 0.8313 - val_loss: 0.4836 - val_accuracy: 0.8233\nEpoch 23/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.8335 - val_loss: 0.4753 - val_accuracy: 0.8250\nEpoch 24/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.8356 - val_loss: 0.4692 - val_accuracy: 0.8267\nEpoch 25/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.8366 - val_loss: 0.4710 - val_accuracy: 0.8283\nEpoch 26/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.8384 - val_loss: 0.4609 - val_accuracy: 0.8267\nEpoch 27/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.8388 - val_loss: 0.4640 - val_accuracy: 0.8250\nEpoch 28/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.8412 - val_loss: 0.4571 - val_accuracy: 0.8333\nEpoch 29/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.8428 - val_loss: 0.4522 - val_accuracy: 0.8333\nEpoch 30/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.8435 - val_loss: 0.4497 - val_accuracy: 0.8317\nEpoch 31/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.8450 - val_loss: 0.4491 - val_accuracy: 0.8317\nEpoch 32/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.8456 - val_loss: 0.4417 - val_accuracy: 0.8333\nEpoch 33/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8473 - val_loss: 0.4395 - val_accuracy: 0.8367\nEpoch 34/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8474 - val_loss: 0.4377 - val_accuracy: 0.8333\nEpoch 35/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8487 - val_loss: 0.4360 - val_accuracy: 0.8317\nEpoch 36/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8485 - val_loss: 0.4331 - val_accuracy: 0.8367\nEpoch 37/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.8499 - val_loss: 0.4331 - val_accuracy: 0.8350\nEpoch 38/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.8499 - val_loss: 0.4268 - val_accuracy: 0.8333\nEpoch 39/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8517 - val_loss: 0.4282 - val_accuracy: 0.8367\nEpoch 40/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8526 - val_loss: 0.4243 - val_accuracy: 0.8400\nEpoch 41/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8534 - val_loss: 0.4206 - val_accuracy: 0.8350\nEpoch 42/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4177 - accuracy: 0.8538 - val_loss: 0.4232 - val_accuracy: 0.8333\nEpoch 43/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4164 - accuracy: 0.8543 - val_loss: 0.4232 - val_accuracy: 0.8383\nEpoch 44/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4138 - accuracy: 0.8559 - val_loss: 0.4183 - val_accuracy: 0.8317\nEpoch 45/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4125 - accuracy: 0.8556 - val_loss: 0.4137 - val_accuracy: 0.8300\nEpoch 46/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4106 - accuracy: 0.8564 - val_loss: 0.4153 - val_accuracy: 0.8317\nEpoch 47/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8566 - val_loss: 0.4131 - val_accuracy: 0.8350\nEpoch 48/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4067 - accuracy: 0.8581 - val_loss: 0.4158 - val_accuracy: 0.8400\nEpoch 49/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8577 - val_loss: 0.4130 - val_accuracy: 0.8333\nEpoch 50/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8588 - val_loss: 0.4109 - val_accuracy: 0.8367\nEpoch 51/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4019 - accuracy: 0.8594 - val_loss: 0.4092 - val_accuracy: 0.8317\nEpoch 52/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8599 - val_loss: 0.4081 - val_accuracy: 0.8350\nEpoch 53/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8603 - val_loss: 0.4027 - val_accuracy: 0.8333\nEpoch 54/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8607 - val_loss: 0.4011 - val_accuracy: 0.8367\nEpoch 55/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8609 - val_loss: 0.4045 - val_accuracy: 0.8367\nEpoch 56/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3946 - accuracy: 0.8608 - val_loss: 0.3998 - val_accuracy: 0.8383\nEpoch 57/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8622 - val_loss: 0.3959 - val_accuracy: 0.8367\nEpoch 58/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3920 - accuracy: 0.8624 - val_loss: 0.3958 - val_accuracy: 0.8383\nEpoch 59/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8628 - val_loss: 0.3966 - val_accuracy: 0.8350\nEpoch 60/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8634 - val_loss: 0.3964 - val_accuracy: 0.8417\nEpoch 61/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8642 - val_loss: 0.3943 - val_accuracy: 0.8333\nEpoch 62/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.8643 - val_loss: 0.3931 - val_accuracy: 0.8450\nEpoch 63/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3848 - accuracy: 0.8643 - val_loss: 0.3928 - val_accuracy: 0.8433\nEpoch 64/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8642 - val_loss: 0.3893 - val_accuracy: 0.8350\nEpoch 65/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8652 - val_loss: 0.3940 - val_accuracy: 0.8367\nEpoch 66/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8659 - val_loss: 0.3843 - val_accuracy: 0.8367\nEpoch 67/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3800 - accuracy: 0.8664 - val_loss: 0.3837 - val_accuracy: 0.8383\nEpoch 68/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8665 - val_loss: 0.3838 - val_accuracy: 0.8367\nEpoch 69/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8672 - val_loss: 0.3854 - val_accuracy: 0.8367\nEpoch 70/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8673 - val_loss: 0.3827 - val_accuracy: 0.8383\nEpoch 71/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3752 - accuracy: 0.8684 - val_loss: 0.3791 - val_accuracy: 0.8367\nEpoch 72/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3738 - accuracy: 0.8686 - val_loss: 0.3844 - val_accuracy: 0.8467\nEpoch 73/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3740 - accuracy: 0.8677 - val_loss: 0.3817 - val_accuracy: 0.8467\nEpoch 74/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3729 - accuracy: 0.8684 - val_loss: 0.3797 - val_accuracy: 0.8417\nEpoch 75/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8687 - val_loss: 0.3812 - val_accuracy: 0.8467\nEpoch 76/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8693 - val_loss: 0.3757 - val_accuracy: 0.8483\nEpoch 77/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3689 - accuracy: 0.8693 - val_loss: 0.3788 - val_accuracy: 0.8450\nEpoch 78/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8705 - val_loss: 0.3774 - val_accuracy: 0.8483\nEpoch 79/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3680 - accuracy: 0.8703 - val_loss: 0.3738 - val_accuracy: 0.8433\nEpoch 80/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8711 - val_loss: 0.3756 - val_accuracy: 0.8450\nEpoch 81/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8716 - val_loss: 0.3760 - val_accuracy: 0.8450\nEpoch 82/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8708 - val_loss: 0.3768 - val_accuracy: 0.8467\nEpoch 83/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3628 - accuracy: 0.8719 - val_loss: 0.3718 - val_accuracy: 0.8467\nEpoch 84/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8729 - val_loss: 0.3743 - val_accuracy: 0.8450\nEpoch 85/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3608 - accuracy: 0.8727 - val_loss: 0.3700 - val_accuracy: 0.8417\nEpoch 86/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8727 - val_loss: 0.3771 - val_accuracy: 0.8467\nEpoch 87/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.8727 - val_loss: 0.3671 - val_accuracy: 0.8450\nEpoch 88/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3577 - accuracy: 0.8743 - val_loss: 0.3657 - val_accuracy: 0.8450\nEpoch 89/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3569 - accuracy: 0.8745 - val_loss: 0.3677 - val_accuracy: 0.8500\nEpoch 90/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8743 - val_loss: 0.3750 - val_accuracy: 0.8483\nEpoch 91/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3563 - accuracy: 0.8748 - val_loss: 0.3626 - val_accuracy: 0.8400\nEpoch 92/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3541 - accuracy: 0.8749 - val_loss: 0.3644 - val_accuracy: 0.8433\nEpoch 93/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8752 - val_loss: 0.3652 - val_accuracy: 0.8467\nEpoch 94/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8754 - val_loss: 0.3644 - val_accuracy: 0.8483\nEpoch 95/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3514 - accuracy: 0.8757 - val_loss: 0.3615 - val_accuracy: 0.8500\nEpoch 96/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8755 - val_loss: 0.3620 - val_accuracy: 0.8550\nEpoch 97/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8756 - val_loss: 0.3613 - val_accuracy: 0.8467\nEpoch 98/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8766 - val_loss: 0.3613 - val_accuracy: 0.8483\nEpoch 99/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.8769 - val_loss: 0.3630 - val_accuracy: 0.8533\nEpoch 100/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3472 - accuracy: 0.8775 - val_loss: 0.3580 - val_accuracy: 0.8483\nEpoch 101/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8775 - val_loss: 0.3561 - val_accuracy: 0.8433\nEpoch 102/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.8774 - val_loss: 0.3581 - val_accuracy: 0.8533\nEpoch 103/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3458 - accuracy: 0.8783 - val_loss: 0.3562 - val_accuracy: 0.8467\nEpoch 104/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8774 - val_loss: 0.3581 - val_accuracy: 0.8417\nEpoch 105/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3444 - accuracy: 0.8783 - val_loss: 0.3597 - val_accuracy: 0.8517\nEpoch 106/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8784 - val_loss: 0.3579 - val_accuracy: 0.8567\nEpoch 107/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3440 - accuracy: 0.8782 - val_loss: 0.3545 - val_accuracy: 0.8550\nEpoch 108/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3414 - accuracy: 0.8794 - val_loss: 0.3543 - val_accuracy: 0.8467\nEpoch 109/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8798 - val_loss: 0.3526 - val_accuracy: 0.8533\nEpoch 110/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8797 - val_loss: 0.3543 - val_accuracy: 0.8483\nEpoch 111/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3400 - accuracy: 0.8797 - val_loss: 0.3533 - val_accuracy: 0.8517\nEpoch 112/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8805 - val_loss: 0.3552 - val_accuracy: 0.8533\nEpoch 113/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8809 - val_loss: 0.3574 - val_accuracy: 0.8533\nEpoch 114/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3399 - accuracy: 0.8790 - val_loss: 0.3517 - val_accuracy: 0.8550\nEpoch 115/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8805 - val_loss: 0.3508 - val_accuracy: 0.8583\nEpoch 116/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 0.8808 - val_loss: 0.3524 - val_accuracy: 0.8533\nEpoch 117/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3347 - accuracy: 0.8813 - val_loss: 0.3503 - val_accuracy: 0.8500\nEpoch 118/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8811 - val_loss: 0.3475 - val_accuracy: 0.8517\nEpoch 119/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8820 - val_loss: 0.3498 - val_accuracy: 0.8517\nEpoch 120/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3327 - accuracy: 0.8825 - val_loss: 0.3527 - val_accuracy: 0.8567\nEpoch 121/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8834 - val_loss: 0.3501 - val_accuracy: 0.8567\nEpoch 122/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8826 - val_loss: 0.3531 - val_accuracy: 0.8550\nEpoch 123/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8824 - val_loss: 0.3479 - val_accuracy: 0.8533\nEpoch 124/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3311 - accuracy: 0.8824 - val_loss: 0.3496 - val_accuracy: 0.8583\nEpoch 125/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8831 - val_loss: 0.3545 - val_accuracy: 0.8550\nEpoch 126/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8835 - val_loss: 0.3524 - val_accuracy: 0.8533\nEpoch 127/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8837 - val_loss: 0.3505 - val_accuracy: 0.8583\nEpoch 128/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 0.8839 - val_loss: 0.3498 - val_accuracy: 0.8583\nEpoch 129/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8844 - val_loss: 0.3493 - val_accuracy: 0.8550\nEpoch 130/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8839 - val_loss: 0.3470 - val_accuracy: 0.8583\nEpoch 131/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8845 - val_loss: 0.3463 - val_accuracy: 0.8583\nEpoch 132/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8844 - val_loss: 0.3508 - val_accuracy: 0.8567\nEpoch 133/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3260 - accuracy: 0.8839 - val_loss: 0.3498 - val_accuracy: 0.8617\nEpoch 134/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3241 - accuracy: 0.8850 - val_loss: 0.3430 - val_accuracy: 0.8600\nEpoch 135/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8855 - val_loss: 0.3427 - val_accuracy: 0.8600\nEpoch 136/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8852 - val_loss: 0.3445 - val_accuracy: 0.8650\nEpoch 137/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8857 - val_loss: 0.3410 - val_accuracy: 0.8600\nEpoch 138/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8857 - val_loss: 0.3469 - val_accuracy: 0.8633\nEpoch 139/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3208 - accuracy: 0.8859 - val_loss: 0.3421 - val_accuracy: 0.8583\nEpoch 140/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8858 - val_loss: 0.3437 - val_accuracy: 0.8583\nEpoch 141/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8862 - val_loss: 0.3454 - val_accuracy: 0.8633\nEpoch 142/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.8865 - val_loss: 0.3467 - val_accuracy: 0.8617\nEpoch 143/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8865 - val_loss: 0.3463 - val_accuracy: 0.8667\nEpoch 144/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8865 - val_loss: 0.3384 - val_accuracy: 0.8667\nEpoch 145/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8873 - val_loss: 0.3429 - val_accuracy: 0.8683\nEpoch 146/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8885 - val_loss: 0.3391 - val_accuracy: 0.8583\nEpoch 147/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3158 - accuracy: 0.8880 - val_loss: 0.3397 - val_accuracy: 0.8617\nEpoch 148/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3147 - accuracy: 0.8887 - val_loss: 0.3447 - val_accuracy: 0.8617\nEpoch 149/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8885 - val_loss: 0.3387 - val_accuracy: 0.8633\nEpoch 150/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8889 - val_loss: 0.3411 - val_accuracy: 0.8633\nEpoch 151/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8884 - val_loss: 0.3403 - val_accuracy: 0.8667\nEpoch 152/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8894 - val_loss: 0.3358 - val_accuracy: 0.8617\nEpoch 153/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.8892 - val_loss: 0.3364 - val_accuracy: 0.8667\nEpoch 154/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8899 - val_loss: 0.3369 - val_accuracy: 0.8683\nEpoch 155/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8893 - val_loss: 0.3444 - val_accuracy: 0.8683\nEpoch 156/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.8894 - val_loss: 0.3356 - val_accuracy: 0.8667\nEpoch 157/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8895 - val_loss: 0.3314 - val_accuracy: 0.8650\nEpoch 158/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3097 - accuracy: 0.8901 - val_loss: 0.3373 - val_accuracy: 0.8650\nEpoch 159/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3094 - accuracy: 0.8899 - val_loss: 0.3380 - val_accuracy: 0.8633\nEpoch 160/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3099 - accuracy: 0.8893 - val_loss: 0.3372 - val_accuracy: 0.8633\nEpoch 161/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3090 - accuracy: 0.8902 - val_loss: 0.3355 - val_accuracy: 0.8633\nEpoch 162/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3077 - accuracy: 0.8906 - val_loss: 0.3374 - val_accuracy: 0.8567\nEpoch 163/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8902 - val_loss: 0.3312 - val_accuracy: 0.8667\nEpoch 164/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.8908 - val_loss: 0.3356 - val_accuracy: 0.8717\nEpoch 165/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8901 - val_loss: 0.3358 - val_accuracy: 0.8683\nEpoch 166/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3060 - accuracy: 0.8913 - val_loss: 0.3341 - val_accuracy: 0.8667\nEpoch 167/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3043 - accuracy: 0.8917 - val_loss: 0.3319 - val_accuracy: 0.8617\nEpoch 168/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8916 - val_loss: 0.3283 - val_accuracy: 0.8617\nEpoch 169/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3039 - accuracy: 0.8924 - val_loss: 0.3327 - val_accuracy: 0.8667\nEpoch 170/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8919 - val_loss: 0.3290 - val_accuracy: 0.8733\nEpoch 171/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8923 - val_loss: 0.3332 - val_accuracy: 0.8717\nEpoch 172/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3019 - accuracy: 0.8930 - val_loss: 0.3301 - val_accuracy: 0.8683\nEpoch 173/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3019 - accuracy: 0.8924 - val_loss: 0.3324 - val_accuracy: 0.8650\nEpoch 174/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8934 - val_loss: 0.3306 - val_accuracy: 0.8667\nEpoch 175/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8930 - val_loss: 0.3310 - val_accuracy: 0.8617\nEpoch 176/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2996 - accuracy: 0.8929 - val_loss: 0.3307 - val_accuracy: 0.8650\nEpoch 177/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.8933 - val_loss: 0.3284 - val_accuracy: 0.8650\nEpoch 178/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8935 - val_loss: 0.3299 - val_accuracy: 0.8633\nEpoch 179/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2994 - accuracy: 0.8937 - val_loss: 0.3336 - val_accuracy: 0.8700\nEpoch 180/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2975 - accuracy: 0.8942 - val_loss: 0.3276 - val_accuracy: 0.8650\nEpoch 181/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.8940 - val_loss: 0.3343 - val_accuracy: 0.8683\nEpoch 182/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2969 - accuracy: 0.8949 - val_loss: 0.3326 - val_accuracy: 0.8683\nEpoch 183/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.8946 - val_loss: 0.3276 - val_accuracy: 0.8600\nEpoch 184/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2963 - accuracy: 0.8944 - val_loss: 0.3316 - val_accuracy: 0.8750\nEpoch 185/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2952 - accuracy: 0.8941 - val_loss: 0.3219 - val_accuracy: 0.8700\nEpoch 186/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2958 - accuracy: 0.8951 - val_loss: 0.3301 - val_accuracy: 0.8650\nEpoch 187/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8948 - val_loss: 0.3262 - val_accuracy: 0.8700\nEpoch 188/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2939 - accuracy: 0.8950 - val_loss: 0.3255 - val_accuracy: 0.8667\nEpoch 189/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8961 - val_loss: 0.3340 - val_accuracy: 0.8667\nEpoch 190/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8954 - val_loss: 0.3298 - val_accuracy: 0.8667\nEpoch 191/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8945 - val_loss: 0.3254 - val_accuracy: 0.8600\nEpoch 192/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8964 - val_loss: 0.3252 - val_accuracy: 0.8650\nEpoch 193/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.8963 - val_loss: 0.3253 - val_accuracy: 0.8700\nEpoch 194/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8963 - val_loss: 0.3237 - val_accuracy: 0.8700\nEpoch 195/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8959 - val_loss: 0.3282 - val_accuracy: 0.8667\nEpoch 196/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8960 - val_loss: 0.3261 - val_accuracy: 0.8683\nEpoch 197/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.8966 - val_loss: 0.3250 - val_accuracy: 0.8750\nEpoch 198/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8969 - val_loss: 0.3268 - val_accuracy: 0.8717\nEpoch 199/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.8974 - val_loss: 0.3265 - val_accuracy: 0.8700\nEpoch 200/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.2900 - accuracy: 0.8965 - val_loss: 0.3265 - val_accuracy: 0.8633</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># loss 값을 plot 해보겠습니다.</span>\ny_vloss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\ny_loss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vloss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_loss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'upper right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Loss graph without dropout layer'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7klEQVQ4y32Uz28TVxDHn6ASEkW9IXHoLRwQEvQMhxxStUDVgIAL/0FQfnBIlL+jaitV6oFLxAou9IKURGuTYJw4dSqL4liV4gbbDZs0trPL2l7vvt9fNOsk5ACs9NHMm519b958R8suXfzmBGPsy2d/LHz9IrN6YaNSPu84zuXp6elvJycnr42Pj9+Ympr6fuz+2A9pbGry2sSDiRuzs7PfzczMXM/lchc3NzfPl8vlC8vLy+dYWKmdBMB2q/Vne/UQieBhkiQ9IUS33+9H5HPOuzxJupxi78Jer/G212g0euWNcm9ra6tXq9XC7e1t1Ov1hwzACdqw02o///9NH8Zaa42FtRbGmCNLABZxpNDeVfA8D9VqFbVajXzLOUez2XzM5ufnv2CMnd7f9zM7//gQUkgppSGEEPbApqS+FEYbaZRSKQe5UmuNVqv16KjCbqebabwKYKxR5hMVHo8djxtjFAAEQeCwOI5PUoVRHGT/LQXYaUQK0BBCgnMxIOEQnEMKAcEFpJQpgtYDlFKKKnTY0NDQOaowTiI3iQ1eLnRUe5cDoJ5pHH9sGrWpbyygtIUyliwZ+L7vsJGRkTOMsVNB8C5D6Z2Aqz9zEVbmA2zkfVRWfWwVdhBWPcStAJHfA+/GMEkfUBFg+oQCONrtlkM9TAnDMGu0gbFK0cZ0+s62RL3SxWZxH8WFNgqPayj89heKP6+i9MtL/P3rMsq/5/Hqp4yqPn2N5m7TYaVSiVRmVCEpRf2QQlmllAWkHVzuUAZr0yMBG0aw3p61b/dg//O03GsaNJtth3melw52p9Nx6ROttTQHj1LaaDWwA18bJZWxdBUQ6sBqSR32/f0PYxMEwQtSigb0IyqmHMYO39MEHE7DQOX2EzY8PExj85Xrulcrlcro2traj4VCYZTI5/M3i8Vius5ms7dzudwt13XvrKys3FxcXLxL66Wlpdv5fH7Udd17c3NzV9j6+jq18Eicj6G1ppSz9BP5XB7xHjFQo2rcvFL8AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/7659e0a64c70ee324e2c8cf47f322541/7bc0b/output_37_0.png\"\n        srcset=\"/static/7659e0a64c70ee324e2c8cf47f322541/e9ff0/output_37_0.png 180w,\n/static/7659e0a64c70ee324e2c8cf47f322541/f21e7/output_37_0.png 360w,\n/static/7659e0a64c70ee324e2c8cf47f322541/7bc0b/output_37_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># accuracy 값을 plot 해보겠습니다.</span>\ny_vacc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_accuracy'</span><span class=\"token punctuation\">]</span>\ny_acc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_acc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vacc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_acc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'lower right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Accuracy graph without dropout layer'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/klEQVQ4y32U32sUVxTHL5FCS7HQh4JgEARBfAvkSSgItraFGiX2pX9BXrIlJOgfYV98Kn0SIrjYYl62GtzMTJK1kZ1sW8lq0tWa7HZ1dxnd3dkfk5nZmXvvuV+5k2QNRXrhw/fMzOGcc8+ce9n4+PgIY+xj0zRHi8Xi6Ww2e2ZmZubC7Ozsl1qnp6e/SaVSX2vVzM3NfTE1NfVtKvXDV5qrV6+d39h4csq2C2csyzrGOOdHALBWq3UvjmOEYdiLosiLotjz/cAP/NAPg4GXEGoNvUE42I2jgRdHwS7XxEFX8AEcx7nJAIzogJ7nLQOAEFKBJACNAKDwvsUB7HKgHwO9CMoXgOt27rB0Ov0BY+wj13VNwXUAyQMJcrqgWhPq7y2u1nMe5R+0yP6tQXamToXMKyrceU72rS1a/7lA9k8FvrVQglNzbusKE7y+Z+rMzx61hP3jQ2xcz2LzhoFaOofO/TX07i6ht2Cgt2QjePgH5J+PgdIm8M9T4MWmQLOOTquVZmNjY58yxj4Jop5V+b2Kv359JqjbBsIO4LtA7INkBIIE7TeC9psR7TMARAyg2Wym2ejx4yd0hX7HNUqLFe2sffc6qACpIYKUEiQkJOdQB7YQoMQWQhHBdd00Ozl64hhj7MM3z6tG/YUHQVLwmEMIAc7f6WFb63++C52wpbeM/t5frj6tmX3HB+ncOruUifOBDm3Oh+8OBU8CttvtNMvfLR5hjI1UijWr5/iQSgjOudK7OKyJLaXiUiqovUVEB3Cl1F6Fj1YbOiCrVvqG11fJ2JCkZEkphyqVorjXp9h5Q9WXL+nfSoXq9To1Gg2tvNFo6MG+PRxs53U3F4YCcRwNe6ZPzgFCcHTbAV7XA5TLO9jZ3kG5XMb29jaq1ao+YTrgL+zcuc91hUdXVoyzpdLmRD5vX8zn8xOatbW1S4VCIXm2LGtyeTV3+X526UpudfVSJpP5zjTNy4uLi5OWZU0YhvH9/Pz8Wba+vqx3PBzw9yGl1C6f6Uvk//w0bwHIlJYIkoZ5JQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/14349f49de542fb4d16446c01a574836/7bc0b/output_38_0.png\"\n        srcset=\"/static/14349f49de542fb4d16446c01a574836/e9ff0/output_38_0.png 180w,\n/static/14349f49de542fb4d16446c01a574836/f21e7/output_38_0.png 360w,\n/static/14349f49de542fb4d16446c01a574836/7bc0b/output_38_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이렇게 200번 epochs 하면 어느순간(loss 한 100부터 accuracy 한 25부터,,)부터 train loss 는 계속 떨어지지만 val loss 는 더이상 움직이지 않는다…</p>\n<p>ㅜㅜ 넘해</p>\n<p>이럴때 드랍아웃으로 오버피팅 방지</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다. </span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory<span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>X_valid<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/200\n117/117 [==============================] - 1s 7ms/step - loss: 2.0802 - accuracy: 0.4763 - val_loss: 1.7029 - val_accuracy: 0.5300\nEpoch 2/200\n117/117 [==============================] - 0s 3ms/step - loss: 1.4692 - accuracy: 0.5660 - val_loss: 1.2217 - val_accuracy: 0.6350\nEpoch 3/200\n117/117 [==============================] - 0s 4ms/step - loss: 1.1566 - accuracy: 0.6223 - val_loss: 1.0075 - val_accuracy: 0.7083\nEpoch 4/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.9942 - accuracy: 0.6671 - val_loss: 0.8747 - val_accuracy: 0.7533\nEpoch 5/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.8909 - accuracy: 0.6978 - val_loss: 0.7917 - val_accuracy: 0.7600\nEpoch 6/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.8228 - accuracy: 0.7125 - val_loss: 0.7359 - val_accuracy: 0.7567\nEpoch 7/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.7727 - accuracy: 0.7269 - val_loss: 0.6952 - val_accuracy: 0.7767\nEpoch 8/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.7379 - accuracy: 0.7364 - val_loss: 0.6664 - val_accuracy: 0.7833\nEpoch 9/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.7098 - accuracy: 0.7456 - val_loss: 0.6425 - val_accuracy: 0.7917\nEpoch 10/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.7520 - val_loss: 0.6225 - val_accuracy: 0.7917\nEpoch 11/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6682 - accuracy: 0.7576 - val_loss: 0.6038 - val_accuracy: 0.7967\nEpoch 12/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6491 - accuracy: 0.7655 - val_loss: 0.5924 - val_accuracy: 0.7967\nEpoch 13/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.7712 - val_loss: 0.5801 - val_accuracy: 0.7933\nEpoch 14/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.6212 - accuracy: 0.7761 - val_loss: 0.5685 - val_accuracy: 0.7950\nEpoch 15/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.7821 - val_loss: 0.5560 - val_accuracy: 0.7967\nEpoch 16/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.7863 - val_loss: 0.5497 - val_accuracy: 0.8050\nEpoch 17/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5861 - accuracy: 0.7915 - val_loss: 0.5371 - val_accuracy: 0.8017\nEpoch 18/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5760 - accuracy: 0.7923 - val_loss: 0.5313 - val_accuracy: 0.8033\nEpoch 19/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5669 - accuracy: 0.7980 - val_loss: 0.5252 - val_accuracy: 0.7983\nEpoch 20/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5606 - accuracy: 0.7998 - val_loss: 0.5152 - val_accuracy: 0.8067\nEpoch 21/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5500 - accuracy: 0.8034 - val_loss: 0.5098 - val_accuracy: 0.8117\nEpoch 22/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5430 - accuracy: 0.8059 - val_loss: 0.5044 - val_accuracy: 0.8133\nEpoch 23/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5370 - accuracy: 0.8086 - val_loss: 0.4973 - val_accuracy: 0.8117\nEpoch 24/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5300 - accuracy: 0.8109 - val_loss: 0.4926 - val_accuracy: 0.8133\nEpoch 25/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5257 - accuracy: 0.8133 - val_loss: 0.4880 - val_accuracy: 0.8167\nEpoch 26/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5208 - accuracy: 0.8151 - val_loss: 0.4803 - val_accuracy: 0.8217\nEpoch 27/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5144 - accuracy: 0.8182 - val_loss: 0.4767 - val_accuracy: 0.8183\nEpoch 28/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5101 - accuracy: 0.8194 - val_loss: 0.4730 - val_accuracy: 0.8200\nEpoch 29/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.8202 - val_loss: 0.4692 - val_accuracy: 0.8267\nEpoch 30/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.5016 - accuracy: 0.8223 - val_loss: 0.4649 - val_accuracy: 0.8250\nEpoch 31/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4949 - accuracy: 0.8246 - val_loss: 0.4627 - val_accuracy: 0.8217\nEpoch 32/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.8260 - val_loss: 0.4581 - val_accuracy: 0.8200\nEpoch 33/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4880 - accuracy: 0.8282 - val_loss: 0.4540 - val_accuracy: 0.8267\nEpoch 34/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.8270 - val_loss: 0.4477 - val_accuracy: 0.8250\nEpoch 35/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.8301 - val_loss: 0.4479 - val_accuracy: 0.8267\nEpoch 36/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4784 - accuracy: 0.8317 - val_loss: 0.4423 - val_accuracy: 0.8317\nEpoch 37/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4746 - accuracy: 0.8326 - val_loss: 0.4429 - val_accuracy: 0.8283\nEpoch 38/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4712 - accuracy: 0.8336 - val_loss: 0.4392 - val_accuracy: 0.8350\nEpoch 39/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4680 - accuracy: 0.8346 - val_loss: 0.4364 - val_accuracy: 0.8333\nEpoch 40/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4642 - accuracy: 0.8361 - val_loss: 0.4352 - val_accuracy: 0.8333\nEpoch 41/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4625 - accuracy: 0.8360 - val_loss: 0.4318 - val_accuracy: 0.8367\nEpoch 42/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4589 - accuracy: 0.8378 - val_loss: 0.4305 - val_accuracy: 0.8317\nEpoch 43/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4571 - accuracy: 0.8379 - val_loss: 0.4267 - val_accuracy: 0.8317\nEpoch 44/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.8400 - val_loss: 0.4250 - val_accuracy: 0.8317\nEpoch 45/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8407 - val_loss: 0.4234 - val_accuracy: 0.8350\nEpoch 46/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4514 - accuracy: 0.8398 - val_loss: 0.4185 - val_accuracy: 0.8350\nEpoch 47/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4457 - accuracy: 0.8415 - val_loss: 0.4165 - val_accuracy: 0.8333\nEpoch 48/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8418 - val_loss: 0.4174 - val_accuracy: 0.8367\nEpoch 49/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.8442 - val_loss: 0.4194 - val_accuracy: 0.8317\nEpoch 50/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.8431 - val_loss: 0.4114 - val_accuracy: 0.8383\nEpoch 51/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4384 - accuracy: 0.8454 - val_loss: 0.4127 - val_accuracy: 0.8350\nEpoch 52/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.8446 - val_loss: 0.4082 - val_accuracy: 0.8400\nEpoch 53/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.8454 - val_loss: 0.4059 - val_accuracy: 0.8383\nEpoch 54/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8479 - val_loss: 0.4040 - val_accuracy: 0.8367\nEpoch 55/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8481 - val_loss: 0.4016 - val_accuracy: 0.8367\nEpoch 56/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4268 - accuracy: 0.8489 - val_loss: 0.4070 - val_accuracy: 0.8400\nEpoch 57/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8487 - val_loss: 0.4012 - val_accuracy: 0.8367\nEpoch 58/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8498 - val_loss: 0.3998 - val_accuracy: 0.8383\nEpoch 59/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8508 - val_loss: 0.3976 - val_accuracy: 0.8333\nEpoch 60/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4232 - accuracy: 0.8501 - val_loss: 0.3967 - val_accuracy: 0.8383\nEpoch 61/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8499 - val_loss: 0.3960 - val_accuracy: 0.8350\nEpoch 62/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8510 - val_loss: 0.3976 - val_accuracy: 0.8367\nEpoch 63/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4183 - accuracy: 0.8520 - val_loss: 0.3940 - val_accuracy: 0.8400\nEpoch 64/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4154 - accuracy: 0.8527 - val_loss: 0.3922 - val_accuracy: 0.8350\nEpoch 65/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4156 - accuracy: 0.8531 - val_loss: 0.3937 - val_accuracy: 0.8383\nEpoch 66/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4129 - accuracy: 0.8537 - val_loss: 0.3896 - val_accuracy: 0.8400\nEpoch 67/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4136 - accuracy: 0.8532 - val_loss: 0.3890 - val_accuracy: 0.8417\nEpoch 68/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8547 - val_loss: 0.3869 - val_accuracy: 0.8383\nEpoch 69/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4077 - accuracy: 0.8564 - val_loss: 0.3861 - val_accuracy: 0.8467\nEpoch 70/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8553 - val_loss: 0.3848 - val_accuracy: 0.8417\nEpoch 71/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4047 - accuracy: 0.8579 - val_loss: 0.3820 - val_accuracy: 0.8433\nEpoch 72/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4054 - accuracy: 0.8558 - val_loss: 0.3827 - val_accuracy: 0.8400\nEpoch 73/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4042 - accuracy: 0.8575 - val_loss: 0.3828 - val_accuracy: 0.8400\nEpoch 74/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8579 - val_loss: 0.3800 - val_accuracy: 0.8433\nEpoch 75/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8583 - val_loss: 0.3794 - val_accuracy: 0.8450\nEpoch 76/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4005 - accuracy: 0.8568 - val_loss: 0.3768 - val_accuracy: 0.8450\nEpoch 77/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8576 - val_loss: 0.3765 - val_accuracy: 0.8467\nEpoch 78/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3980 - accuracy: 0.8592 - val_loss: 0.3773 - val_accuracy: 0.8467\nEpoch 79/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8593 - val_loss: 0.3762 - val_accuracy: 0.8517\nEpoch 80/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8594 - val_loss: 0.3729 - val_accuracy: 0.8450\nEpoch 81/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8586 - val_loss: 0.3714 - val_accuracy: 0.8433\nEpoch 82/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8619 - val_loss: 0.3736 - val_accuracy: 0.8467\nEpoch 83/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8623 - val_loss: 0.3728 - val_accuracy: 0.8433\nEpoch 84/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3913 - accuracy: 0.8614 - val_loss: 0.3716 - val_accuracy: 0.8483\nEpoch 85/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8621 - val_loss: 0.3704 - val_accuracy: 0.8433\nEpoch 86/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.8621 - val_loss: 0.3699 - val_accuracy: 0.8450\nEpoch 87/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3875 - accuracy: 0.8614 - val_loss: 0.3710 - val_accuracy: 0.8483\nEpoch 88/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3863 - accuracy: 0.8630 - val_loss: 0.3666 - val_accuracy: 0.8417\nEpoch 89/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3847 - accuracy: 0.8626 - val_loss: 0.3684 - val_accuracy: 0.8467\nEpoch 90/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3837 - accuracy: 0.8634 - val_loss: 0.3681 - val_accuracy: 0.8450\nEpoch 91/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3835 - accuracy: 0.8646 - val_loss: 0.3655 - val_accuracy: 0.8517\nEpoch 92/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3840 - accuracy: 0.8640 - val_loss: 0.3673 - val_accuracy: 0.8467\nEpoch 93/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3825 - accuracy: 0.8646 - val_loss: 0.3628 - val_accuracy: 0.8500\nEpoch 94/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8639 - val_loss: 0.3630 - val_accuracy: 0.8450\nEpoch 95/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8657 - val_loss: 0.3628 - val_accuracy: 0.8517\nEpoch 96/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.8660 - val_loss: 0.3638 - val_accuracy: 0.8517\nEpoch 97/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3784 - accuracy: 0.8671 - val_loss: 0.3629 - val_accuracy: 0.8533\nEpoch 98/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3768 - accuracy: 0.8661 - val_loss: 0.3609 - val_accuracy: 0.8567\nEpoch 99/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8662 - val_loss: 0.3623 - val_accuracy: 0.8550\nEpoch 100/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8666 - val_loss: 0.3586 - val_accuracy: 0.8550\nEpoch 101/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8674 - val_loss: 0.3573 - val_accuracy: 0.8417\nEpoch 102/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8675 - val_loss: 0.3605 - val_accuracy: 0.8550\nEpoch 103/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3722 - accuracy: 0.8695 - val_loss: 0.3577 - val_accuracy: 0.8533\nEpoch 104/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.8669 - val_loss: 0.3558 - val_accuracy: 0.8517\nEpoch 105/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8672 - val_loss: 0.3577 - val_accuracy: 0.8550\nEpoch 106/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8689 - val_loss: 0.3540 - val_accuracy: 0.8533\nEpoch 107/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3681 - accuracy: 0.8686 - val_loss: 0.3565 - val_accuracy: 0.8533\nEpoch 108/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8687 - val_loss: 0.3529 - val_accuracy: 0.8500\nEpoch 109/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8696 - val_loss: 0.3523 - val_accuracy: 0.8533\nEpoch 110/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3671 - accuracy: 0.8700 - val_loss: 0.3537 - val_accuracy: 0.8550\nEpoch 111/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8699 - val_loss: 0.3520 - val_accuracy: 0.8550\nEpoch 112/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3648 - accuracy: 0.8715 - val_loss: 0.3499 - val_accuracy: 0.8583\nEpoch 113/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.8721 - val_loss: 0.3496 - val_accuracy: 0.8600\nEpoch 114/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3636 - accuracy: 0.8721 - val_loss: 0.3508 - val_accuracy: 0.8467\nEpoch 115/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8709 - val_loss: 0.3480 - val_accuracy: 0.8550\nEpoch 116/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3616 - accuracy: 0.8717 - val_loss: 0.3514 - val_accuracy: 0.8583\nEpoch 117/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8720 - val_loss: 0.3462 - val_accuracy: 0.8533\nEpoch 118/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3616 - accuracy: 0.8716 - val_loss: 0.3518 - val_accuracy: 0.8567\nEpoch 119/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3597 - accuracy: 0.8719 - val_loss: 0.3479 - val_accuracy: 0.8583\nEpoch 120/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.8726 - val_loss: 0.3491 - val_accuracy: 0.8583\nEpoch 121/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3588 - accuracy: 0.8727 - val_loss: 0.3466 - val_accuracy: 0.8583\nEpoch 122/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3576 - accuracy: 0.8732 - val_loss: 0.3459 - val_accuracy: 0.8517\nEpoch 123/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3544 - accuracy: 0.8743 - val_loss: 0.3451 - val_accuracy: 0.8583\nEpoch 124/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3557 - accuracy: 0.8739 - val_loss: 0.3430 - val_accuracy: 0.8550\nEpoch 125/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3555 - accuracy: 0.8745 - val_loss: 0.3430 - val_accuracy: 0.8600\nEpoch 126/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3546 - accuracy: 0.8729 - val_loss: 0.3442 - val_accuracy: 0.8583\nEpoch 127/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.8746 - val_loss: 0.3435 - val_accuracy: 0.8600\nEpoch 128/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3551 - accuracy: 0.8736 - val_loss: 0.3405 - val_accuracy: 0.8550\nEpoch 129/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3532 - accuracy: 0.8750 - val_loss: 0.3415 - val_accuracy: 0.8567\nEpoch 130/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3521 - accuracy: 0.8753 - val_loss: 0.3397 - val_accuracy: 0.8583\nEpoch 131/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3499 - accuracy: 0.8756 - val_loss: 0.3405 - val_accuracy: 0.8550\nEpoch 132/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8753 - val_loss: 0.3395 - val_accuracy: 0.8617\nEpoch 133/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.8758 - val_loss: 0.3397 - val_accuracy: 0.8600\nEpoch 134/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3509 - accuracy: 0.8758 - val_loss: 0.3412 - val_accuracy: 0.8567\nEpoch 135/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.8770 - val_loss: 0.3386 - val_accuracy: 0.8583\nEpoch 136/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.8758 - val_loss: 0.3384 - val_accuracy: 0.8583\nEpoch 137/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8777 - val_loss: 0.3378 - val_accuracy: 0.8583\nEpoch 138/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.8774 - val_loss: 0.3392 - val_accuracy: 0.8617\nEpoch 139/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3469 - accuracy: 0.8769 - val_loss: 0.3361 - val_accuracy: 0.8583\nEpoch 140/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3464 - accuracy: 0.8778 - val_loss: 0.3370 - val_accuracy: 0.8567\nEpoch 141/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3433 - accuracy: 0.8785 - val_loss: 0.3343 - val_accuracy: 0.8600\nEpoch 142/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3455 - accuracy: 0.8782 - val_loss: 0.3373 - val_accuracy: 0.8600\nEpoch 143/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3445 - accuracy: 0.8778 - val_loss: 0.3357 - val_accuracy: 0.8583\nEpoch 144/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.8786 - val_loss: 0.3356 - val_accuracy: 0.8617\nEpoch 145/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8781 - val_loss: 0.3364 - val_accuracy: 0.8650\nEpoch 146/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.8782 - val_loss: 0.3342 - val_accuracy: 0.8600\nEpoch 147/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3422 - accuracy: 0.8781 - val_loss: 0.3342 - val_accuracy: 0.8650\nEpoch 148/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3424 - accuracy: 0.8787 - val_loss: 0.3343 - val_accuracy: 0.8550\nEpoch 149/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3397 - accuracy: 0.8788 - val_loss: 0.3328 - val_accuracy: 0.8600\nEpoch 150/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8798 - val_loss: 0.3332 - val_accuracy: 0.8633\nEpoch 151/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8788 - val_loss: 0.3333 - val_accuracy: 0.8567\nEpoch 152/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3389 - accuracy: 0.8793 - val_loss: 0.3336 - val_accuracy: 0.8650\nEpoch 153/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.8806 - val_loss: 0.3335 - val_accuracy: 0.8600\nEpoch 154/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8803 - val_loss: 0.3319 - val_accuracy: 0.8567\nEpoch 155/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3381 - accuracy: 0.8811 - val_loss: 0.3323 - val_accuracy: 0.8617\nEpoch 156/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3382 - accuracy: 0.8801 - val_loss: 0.3308 - val_accuracy: 0.8617\nEpoch 157/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8808 - val_loss: 0.3269 - val_accuracy: 0.8617\nEpoch 158/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.8804 - val_loss: 0.3304 - val_accuracy: 0.8683\nEpoch 159/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3361 - accuracy: 0.8809 - val_loss: 0.3298 - val_accuracy: 0.8583\nEpoch 160/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8813 - val_loss: 0.3296 - val_accuracy: 0.8650\nEpoch 161/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8816 - val_loss: 0.3282 - val_accuracy: 0.8700\nEpoch 162/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8817 - val_loss: 0.3293 - val_accuracy: 0.8600\nEpoch 163/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3334 - accuracy: 0.8810 - val_loss: 0.3317 - val_accuracy: 0.8650\nEpoch 164/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8828 - val_loss: 0.3252 - val_accuracy: 0.8633\nEpoch 165/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8819 - val_loss: 0.3271 - val_accuracy: 0.8700\nEpoch 166/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8823 - val_loss: 0.3270 - val_accuracy: 0.8633\nEpoch 167/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3322 - accuracy: 0.8825 - val_loss: 0.3263 - val_accuracy: 0.8650\nEpoch 168/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8830 - val_loss: 0.3270 - val_accuracy: 0.8600\nEpoch 169/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8824 - val_loss: 0.3241 - val_accuracy: 0.8617\nEpoch 170/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8829 - val_loss: 0.3249 - val_accuracy: 0.8650\nEpoch 171/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8823 - val_loss: 0.3261 - val_accuracy: 0.8650\nEpoch 172/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3277 - accuracy: 0.8836 - val_loss: 0.3255 - val_accuracy: 0.8650\nEpoch 173/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8824 - val_loss: 0.3264 - val_accuracy: 0.8633\nEpoch 174/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3284 - accuracy: 0.8830 - val_loss: 0.3247 - val_accuracy: 0.8650\nEpoch 175/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3266 - accuracy: 0.8836 - val_loss: 0.3226 - val_accuracy: 0.8683\nEpoch 176/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8835 - val_loss: 0.3229 - val_accuracy: 0.8633\nEpoch 177/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8841 - val_loss: 0.3252 - val_accuracy: 0.8633\nEpoch 178/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3229 - accuracy: 0.8872 - val_loss: 0.3233 - val_accuracy: 0.8650\nEpoch 179/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8845 - val_loss: 0.3215 - val_accuracy: 0.8667\nEpoch 180/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3264 - accuracy: 0.8844 - val_loss: 0.3245 - val_accuracy: 0.8617\nEpoch 181/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8846 - val_loss: 0.3210 - val_accuracy: 0.8633\nEpoch 182/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.8843 - val_loss: 0.3230 - val_accuracy: 0.8667\nEpoch 183/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3243 - accuracy: 0.8842 - val_loss: 0.3240 - val_accuracy: 0.8650\nEpoch 184/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8845 - val_loss: 0.3199 - val_accuracy: 0.8700\nEpoch 185/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8856 - val_loss: 0.3225 - val_accuracy: 0.8733\nEpoch 186/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8844 - val_loss: 0.3208 - val_accuracy: 0.8650\nEpoch 187/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8859 - val_loss: 0.3167 - val_accuracy: 0.8667\nEpoch 188/200\n117/117 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8849 - val_loss: 0.3201 - val_accuracy: 0.8633\nEpoch 189/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8848 - val_loss: 0.3202 - val_accuracy: 0.8717\nEpoch 190/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8855 - val_loss: 0.3192 - val_accuracy: 0.8633\nEpoch 191/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8861 - val_loss: 0.3214 - val_accuracy: 0.8650\nEpoch 192/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8857 - val_loss: 0.3180 - val_accuracy: 0.8700\nEpoch 193/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8859 - val_loss: 0.3191 - val_accuracy: 0.8617\nEpoch 194/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8858 - val_loss: 0.3170 - val_accuracy: 0.8683\nEpoch 195/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8862 - val_loss: 0.3222 - val_accuracy: 0.8667\nEpoch 196/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8860 - val_loss: 0.3167 - val_accuracy: 0.8700\nEpoch 197/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8880 - val_loss: 0.3194 - val_accuracy: 0.8700\nEpoch 198/200\n117/117 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8865 - val_loss: 0.3178 - val_accuracy: 0.8633\nEpoch 199/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8877 - val_loss: 0.3137 - val_accuracy: 0.8667\nEpoch 200/200\n117/117 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8876 - val_loss: 0.3202 - val_accuracy: 0.8650</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># loss 값을 plot 해보겠습니다. </span>\ny_vloss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\ny_loss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vloss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_loss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'upper right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Loss graph with dropout layer'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC9UlEQVQ4y32US2sbVxTHL0mh0IbuCll05yYE0k2XKdQLp5S21ClpNv0AKV4YhMEQ6MYfwl1lEUJN1VdIQykxzkix60qWI8WGVHKaxjEzQp666DUaaaSZ+/6XM37ENKEXfpx7z5w78z8Phr17/p0TjLHX793+5a2fvls/V35YObOxsXFmbm7u/ZmZmQ+mpqY+yWQyH05PT380Ozt78eqXVyevfXVtIpPJfDw/P3/Bdd23q9Xq2Vqtdm5lZeU0E83WSQCstev/uvukg4QnYZIkEed8EMdxFMfxMEmSASeEGIx6YTSs+wPP86JarRbt7OxEruuGjUYDnufdYABO0Av7YXh/r9qBscZaa0EYY44sAVjEQ4XWnoTv+9je3obrurS3nHM0m83v2eLi4iuMsdc67U6u/sCHkFJKIY2U0gghyFqyB3sjpDDaKKPUPuSjK1prtFqtb48UCjHIeat1BD2jAAOtX1R4XPVxvzF0BwiCIMviOD5JCvtylA8euVjLbitKDpAQgkNyDp4cwCU4F5BSpgghDlFKKVKYZWNjY6fTGg6GDtWofuuBKt58ioanwDV5/rv2a0lYa1KM0YoUd7vdLJuYmDjFGHu11+vllKZgpcLKY9Su/461rx+i/M2fqP78F2r3fTz5Y4S93QTDPkccCQwjgThWSIRW2mi02+0s1TAlDMO8NQZSKnUkph9APKtjt9yAt/wM7t0tPLr9FKUf6yjd+RtrdztYyw1QzEXq8eYIzX9aWba5uUldZqSQOiWFUFJISwhtrDrML8VaQFvApNZCWqO4FcORHA0Ems12lvm+nw52v993qA5aa2kOllbKGK2NkiqFEkutJrc2ihKlQGskfa/b7TwfmyAIfqNO0YC+pIsph77jzw/tfpfbP7Dx8XEamzccx3lva2trcn19/dNSqTRJFAqFS+VyOT3n8/nLq6urnzmO83mxWLy0tLR0hc7Ly8uXC4XCpOM4XywsLFxglUqFSnjUnJehtaaQN+kn8n9xxL9MAaP24DqSKAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/3c940df8a06089ad50c2fbe8c6ea158f/7bc0b/output_41_0.png\"\n        srcset=\"/static/3c940df8a06089ad50c2fbe8c6ea158f/e9ff0/output_41_0.png 180w,\n/static/3c940df8a06089ad50c2fbe8c6ea158f/f21e7/output_41_0.png 360w,\n/static/3c940df8a06089ad50c2fbe8c6ea158f/7bc0b/output_41_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># accuracy 값을 plot 해보겠습니다. </span>\ny_vacc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_accuracy'</span><span class=\"token punctuation\">]</span>\ny_acc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_acc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vacc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_acc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'lower right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Accuracy graph with dropout layer'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADEElEQVQ4y41Uz2sbRxQeHAItJYUcCjkYH5pTbi6GkFx6aGlDwU5Ic8kfYIMPFj7Yf4Rv/XXpxcFQixoX0pjEppU2yNiyskqckjo2DWvJklP9sCWvvNrV7mrnzcxXZv2jaumhDx7fx7zZb99782bY0NBQH2PsPcMw+k3TvGZZ1oczMzM3JiYmbk1OTn42PT39yejo6PDU1NSniUTiViKR+HxsbGxYr4+Pj38xO/vgo+3tnav6W8MwrjDO+QUArNlsPo6iCEEQON1u142iyA2CwPM7vh/4gRcEoRsGXdfvBG7gh14YhG7H87041vGdKAxRr9dnGYA+Lei67lMAEEIoISSkkIAiAAr/w5QEYNutH1kymbzIGHu31bLTxAlcEAeEVFCy6ULW6qQqliOt5w35cuVPmfv5rTQf12T+YUmaD17JjR/eyOy3z/jW4jZqlcN5nWHs7bab1r/ijkdbWQe5+SI2v8vit68y2PxmDdb3Bg4XMzh+sgbHeI72ShbOShbus9fomL8TWXtoNZtJNjg4eJkx9r4TeEa4Y2H16xdkvfbQsT0ItwWQCyACIOPixWkT5CnXSABp3mg0kqy/v39AZ8i7TurNwx3UDuLGxUYKIAkQSRAJSCIIziH/wQmCiJSUsG07ya4ODFxhjL1zXNpPFV62ICAo6nIQETj/G3u5xn/FSQiBpi757JTtQilds1yQUiRI6NOON5/hOef8fK1HPBY8OjpKsvyieYEx1ld+VTLst+04Q8650lX0YsyFUFwIBXViUsoz50qpkwzXftrSgqy0fZByG13dZi6EjE2cEo1CKRk5bRnVD2V5f1+W9vZkpVKR1WpVI69Wq3qw589LLv1xsOo0QpCIEEUnPdM358yJOI6PfBxUfBSLBRR2CygWi9jd3UW5XNY3TAsusOvXP9YZXnr0yLi5ubkzYpq54Y2N3EgulxtZX1+/nc/nhzU3DOPu08zqnSe//PrlaiZze2lp6V46nb6zvLx81zCMkVQqdX9ubu4mW1hY0xXHw02E80HvdSGE3vKBfkT+K97rfwFv9ZYsQOGykwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/07db8a2400447ea8b0cf8954c55862e6/7bc0b/output_42_0.png\"\n        srcset=\"/static/07db8a2400447ea8b0cf8954c55862e6/e9ff0/output_42_0.png 180w,\n/static/07db8a2400447ea8b0cf8954c55862e6/f21e7/output_42_0.png 360w,\n/static/07db8a2400447ea8b0cf8954c55862e6/7bc0b/output_42_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>진짜 조금 바뀌긴 했지만 차이가 있긴 있다</p>\n<p>사실 더 복잡한 네트워크나, 더 어려운 데이터의 경우에는 이러한 오버피팅이 더 자주 있는 일이므로, Dropout layer를 추가하는 경우가 많습니다. 하지만 이 또한 확률 값이 파라미터로 들어가므로, 어떠한 값을 선택하느냐는 데이터와 네트워크에 따라 달린 일입니다.</p>\n<h2 id=\"batch-normalization\" style=\"position:relative;\"><a href=\"#batch-normalization\" aria-label=\"batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Batch Normalization</h2>\n<h3 id=\"요건-기울기소실-이랑-기울기포화-문제를-해결한다\" style=\"position:relative;\"><a href=\"#%EC%9A%94%EA%B1%B4-%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%86%8C%EC%8B%A4-%EC%9D%B4%EB%9E%91-%EA%B8%B0%EC%9A%B8%EA%B8%B0%ED%8F%AC%ED%99%94-%EB%AC%B8%EC%A0%9C%EB%A5%BC-%ED%95%B4%EA%B2%B0%ED%95%9C%EB%8B%A4\" aria-label=\"요건 기울기소실 이랑 기울기포화 문제를 해결한다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>요건 기울기소실 이랑 기울기포화 문제를 해결한다.</h3>\n<p><a href=\"https://arxiv.org/pdf/1502.03167.pdf\">논문</a></p>\n<p>쉽게 말하면 미니배치에서</p>\n<p>평균이랑 분산을 구해가지고</p>\n<p>x에서 평균뺀거를 분산으로 나눈 값으로 정규화 하는거</p>\n<p>특히 중요한 부분은 분모에 엡실론(0.001)) 가 추가된 것이다 이 부분으로</p>\n<p>가중치 소실, 포화를 막을 수 있다. 그래서 이걸로\n오버피팅으로 학습이 잘되지 않는 것을 막을 수 있닫.</p>\n<p>fashion_mnist 데이터로 가져가보자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">from</span> tensorflow <span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n\nfashion_mnist <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>fashion_mnist\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'=3'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">=3</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>test_images<span class=\"token punctuation\">,</span> test_labels<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> fashion_mnist<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclass_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'T-shirt/top'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Trouser'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Pullover'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Dress'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Coat'</span><span class=\"token punctuation\">,</span>\n               <span class=\"token string\">'Sandal'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Shirt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Sneaker'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Bag'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Ankle boot'</span><span class=\"token punctuation\">]</span>\n\ntrain_images <span class=\"token operator\">=</span> train_images <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span>\ntest_images <span class=\"token operator\">=</span> test_images <span class=\"token operator\">/</span> <span class=\"token number\">255.0</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n\nX_train<span class=\"token punctuation\">,</span> X_valid<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_valid <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>train_images<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">101</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory<span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">2048</span><span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>X_valid<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/20\n21/21 [==============================] - 1s 23ms/step - loss: 1.2103 - accuracy: 0.6096 - val_loss: 0.7485 - val_accuracy: 0.7401\nEpoch 2/20\n21/21 [==============================] - 0s 10ms/step - loss: 0.6556 - accuracy: 0.7789 - val_loss: 0.5983 - val_accuracy: 0.8006\nEpoch 3/20\n21/21 [==============================] - 0s 10ms/step - loss: 0.5541 - accuracy: 0.8160 - val_loss: 0.5364 - val_accuracy: 0.8184\nEpoch 4/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.5061 - accuracy: 0.8299 - val_loss: 0.5008 - val_accuracy: 0.8274\nEpoch 5/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.4775 - accuracy: 0.8394 - val_loss: 0.4801 - val_accuracy: 0.8369\nEpoch 6/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.4555 - accuracy: 0.8452 - val_loss: 0.4612 - val_accuracy: 0.8456\nEpoch 7/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.4391 - accuracy: 0.8502 - val_loss: 0.4470 - val_accuracy: 0.8479\nEpoch 8/20\n21/21 [==============================] - 0s 8ms/step - loss: 0.4251 - accuracy: 0.8559 - val_loss: 0.4369 - val_accuracy: 0.8503\nEpoch 9/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.4144 - accuracy: 0.8578 - val_loss: 0.4295 - val_accuracy: 0.8530\nEpoch 10/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.4059 - accuracy: 0.8595 - val_loss: 0.4182 - val_accuracy: 0.8566\nEpoch 11/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.3950 - accuracy: 0.8646 - val_loss: 0.4125 - val_accuracy: 0.8588\nEpoch 12/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3900 - accuracy: 0.8662 - val_loss: 0.4139 - val_accuracy: 0.8543\nEpoch 13/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3826 - accuracy: 0.8678 - val_loss: 0.4036 - val_accuracy: 0.8598\nEpoch 14/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3731 - accuracy: 0.8719 - val_loss: 0.3979 - val_accuracy: 0.8619\nEpoch 15/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3696 - accuracy: 0.8723 - val_loss: 0.3929 - val_accuracy: 0.8624\nEpoch 16/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3614 - accuracy: 0.8761 - val_loss: 0.3878 - val_accuracy: 0.8650\nEpoch 17/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3560 - accuracy: 0.8774 - val_loss: 0.3865 - val_accuracy: 0.8647\nEpoch 18/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3514 - accuracy: 0.8776 - val_loss: 0.3858 - val_accuracy: 0.8644\nEpoch 19/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.8803 - val_loss: 0.3774 - val_accuracy: 0.8686\nEpoch 20/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3385 - accuracy: 0.8828 - val_loss: 0.3780 - val_accuracy: 0.8672</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># loss 값을 plot 해보겠습니다. </span>\ny_vloss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\ny_loss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vloss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_loss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'upper right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Loss graph without batch normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADBklEQVQ4y3WSS28TVxTHrwAJqQVV6qpddBWpLKCrrLNoSguBLAqi9BtEWXhhrCzoh+muVugmD2XRyDbJhiBFURIXN1aaJrZjj03sGXvG4/jOff+rMxYgVfRIR//7ODP3dx5senr6CmPs00Kh8NWrV399vby8/E0mk7mXzWbvLiwsPMzlct+RZrPZ7xcXF+ee//L822dLz+7mcrl7mUxmfmlp6YfNzc3bp6ent7a3t79kSsqrAJjv+xudtoCUIuKcx5zzS875SAgRJ0kSSynjRIiYR8N4VG/FjfPzuFKpxGdnZ3GtVgubzSYajcavDMAV+uEwHr30AwBwzhhDC1hr4ZzDh70BH2v0Ogqe5+Hk5AS1Wg2tVstJKdHr9ZbZHy9eXGOMfRJ22sWgPQZPjJJSWKUUBZFaUq21FUKkqrSwZHRnjKFzRQ8HQfDbe0Ix6hcHf/cwvIQGLKydkFEgkZJN9oDRE2Kt9TtNF1EU5RkX4ioRDkbDUvI2QHPP05xLOGgYp6GMBqVD3yRCQCmNJBHQqSZQSpFqawwR5tnU1NQXRBjHowK9Mm4PdP0wgN8WCL0Y+pJPiJyDcwRCtBNCh4kZICUMiXB2dvYGY+x6GIZFrQ0cnFZaIfQTdOsRmm981F+fo73fRrfcQq/cxMWfHoJ/+vBPLsBbXfBWR5tggKDby1MNU4+iqES1MsZoSoNIHNUSVEsNY4FoaBBGFv0LjrgzQlAfoHMcovEm0N3aJXoXQZ7t7+9TlxkRUtGpwFJKwnRSkFK3FY2S01pQAs5YRdnSinrklDVEgH6/n2ee56WDPRwOCy6tk1M0Gs65dCTISGlP59Y6qxXdIx2bdHxkmhLCMPwwNoPBYJs6SS7Sbqq0u++czj+mFEfxlJ3v+7+zmZkZGpubq6urMzs7O48PDg7mtra2fiyXyw+KxeKjo6Oj+6VS6VGlUplbW1t7uru7O7+xsfFTtVq9v7Ky8vPh4eGD9fX1J8fHx/PVavUOC4KASvi+Of/18XicKmPsc8bYZ/8XR763t8f+BXp2qO3PX9GyAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/6a1f7c39ee994f85d98d4d22baf532e5/7bc0b/output_50_0.png\"\n        srcset=\"/static/6a1f7c39ee994f85d98d4d22baf532e5/e9ff0/output_50_0.png 180w,\n/static/6a1f7c39ee994f85d98d4d22baf532e5/f21e7/output_50_0.png 360w,\n/static/6a1f7c39ee994f85d98d4d22baf532e5/7bc0b/output_50_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>일반적인 Dense FC layer 를 쓰면 저렇게 val loss가 7.5 쯤에서 멈춘다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># accuracy 값을 plot 해보겠습니다. </span>\ny_vacc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_accuracy'</span><span class=\"token punctuation\">]</span>\ny_acc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_acc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vacc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_acc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'lower right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Accuracy graph without batch normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADCklEQVQ4y32TS2sbVxTHDw6BloaUdhXSkkWyCmRh8NaFQBOITQJJyeMLeCG8sWywv013EXVxqRZd2XJtMHZtk+AYFJWoUjS2HuMZzVOjGc3cx7mn3LEtki56hzu/++LM+f/vGZiampoAgK92dna+W19fv7e0tPTjwsLCQ81CoTAzPz8/UywWH2guLi4+KBQKsysrK/eLxeJDvT83N/dkeXn5/uHh4d3t7e0bwDm/QkTgOM4fWZYRYyxI0zRK0zSO4zjJ0kyPI8ZYNBqNoixjUZKMIsZ4znweJwFnnGzb/hmIaEIHjKLoTyIipZQSQmqSIqTzpi7e6rP5J8wHYRj+AqVS6SoAfOn7XkUIQVJJLpEhKqmGg1QN+jGGdoRDN0GnM0CzFeHphxB7zSG2ay52ah4ab7rc/rtPlmm/1hnCeYbDiv4KGzLRbQzJbMR01s7I+uDT2XuH+nWfvLpFccuixDgj1nVoZLqUnfmU2KGQiaDA9UowOTn5DQBcD+PBZty2yXhriUGQUppkxAUjRTJ/tDyhBCEp4ihzM5gUJEkRk1xoO1zPLcH3N2/e0hmmXn/D/celESdB+TFFUkpCiSS1p6hIcEEKkbQ12j7Ni3Wh1QVBUII7t2/fAIAvQs/bsC1JiFKwjOWHOedj6uCMsTER8dO50JfoeV5pfMuuN6g4DuoMxGWA/JIuqAPodfyfgL7vl2B/37oCABOGEVTCUJeLEIwxJYRQl+ScKymlYpwroTAfa1P13nmZCa4l5xm22z0dEMJwsIGodP1xIQQqpVBKibppKiLM/AC51Ufj9BQNw8BOp4O9Xg+73S43TZMsy3o9lhwE/va5X4L0H6PlaUmXXaIg34nJ6ib08WOTGo0GtVqtnCcnJ5QkCZmm+StMT0/rDK+Vy+Ufdnd3f3r37mhma2vr6fHx8WylUnlWq9UebW5uPntfrc789nv55c5f+4/L5fKLo6OjR6urq6/29vZm19bWntfr9cfVavUeNJtNrXhc4P/tSZLkBIBvAeDry/UwDD87h4hwcHAA/wLoLKAxXQohggAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/3efbcc1914c76a9b1265a4d0a9e655c9/7bc0b/output_52_0.png\"\n        srcset=\"/static/3efbcc1914c76a9b1265a4d0a9e655c9/e9ff0/output_52_0.png 180w,\n/static/3efbcc1914c76a9b1265a4d0a9e655c9/f21e7/output_52_0.png 360w,\n/static/3efbcc1914c76a9b1265a4d0a9e655c9/7bc0b/output_52_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"배치-노말라이제이션-쓰면\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EC%B9%98-%EB%85%B8%EB%A7%90%EB%9D%BC%EC%9D%B4%EC%A0%9C%EC%9D%B4%EC%85%98-%EC%93%B0%EB%A9%B4\" aria-label=\"배치 노말라이제이션 쓰면 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배치 노말라이제이션 쓰면</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\">#여기에 batchnormalization layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다.</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>BatchNormalization<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span><span class=\"token string\">'adam'</span><span class=\"token punctuation\">,</span>loss<span class=\"token operator\">=</span><span class=\"token string\">'sparse_categorical_crossentropy'</span><span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nhistory<span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">2048</span><span class=\"token punctuation\">,</span> validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>X_valid<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/20\n21/21 [==============================] - 1s 25ms/step - loss: 0.8808 - accuracy: 0.7005 - val_loss: 1.0613 - val_accuracy: 0.6525\nEpoch 2/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.5167 - accuracy: 0.8267 - val_loss: 0.8411 - val_accuracy: 0.7488\nEpoch 3/20\n21/21 [==============================] - 0s 10ms/step - loss: 0.4500 - accuracy: 0.8469 - val_loss: 0.7390 - val_accuracy: 0.7935\nEpoch 4/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.4127 - accuracy: 0.8584 - val_loss: 0.6659 - val_accuracy: 0.8183\nEpoch 5/20\n21/21 [==============================] - 0s 10ms/step - loss: 0.3869 - accuracy: 0.8680 - val_loss: 0.6321 - val_accuracy: 0.8328\nEpoch 6/20\n21/21 [==============================] - 0s 9ms/step - loss: 0.3640 - accuracy: 0.8746 - val_loss: 0.5769 - val_accuracy: 0.8439\nEpoch 7/20\n21/21 [==============================] - 0s 10ms/step - loss: 0.3465 - accuracy: 0.8790 - val_loss: 0.5427 - val_accuracy: 0.8486\nEpoch 8/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.3321 - accuracy: 0.8842 - val_loss: 0.5140 - val_accuracy: 0.8510\nEpoch 9/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.3164 - accuracy: 0.8903 - val_loss: 0.4943 - val_accuracy: 0.8492\nEpoch 10/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.3045 - accuracy: 0.8935 - val_loss: 0.4571 - val_accuracy: 0.8611\nEpoch 11/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.2944 - accuracy: 0.8968 - val_loss: 0.4447 - val_accuracy: 0.8588\nEpoch 12/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2839 - accuracy: 0.9001 - val_loss: 0.4193 - val_accuracy: 0.8649\nEpoch 13/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.2733 - accuracy: 0.9047 - val_loss: 0.4101 - val_accuracy: 0.8683\nEpoch 14/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2639 - accuracy: 0.9089 - val_loss: 0.4099 - val_accuracy: 0.8638\nEpoch 15/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2580 - accuracy: 0.9097 - val_loss: 0.3807 - val_accuracy: 0.8750\nEpoch 16/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2502 - accuracy: 0.9125 - val_loss: 0.3694 - val_accuracy: 0.8753\nEpoch 17/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2434 - accuracy: 0.9149 - val_loss: 0.3822 - val_accuracy: 0.8688\nEpoch 18/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.2365 - accuracy: 0.9167 - val_loss: 0.3545 - val_accuracy: 0.8790\nEpoch 19/20\n21/21 [==============================] - 0s 6ms/step - loss: 0.2273 - accuracy: 0.9210 - val_loss: 0.3586 - val_accuracy: 0.8746\nEpoch 20/20\n21/21 [==============================] - 0s 7ms/step - loss: 0.2213 - accuracy: 0.9223 - val_loss: 0.3413 - val_accuracy: 0.8785</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># loss 값을 plot 해보겠습니다. </span>\ny_vloss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span>\ny_loss <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vloss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_loss<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set Loss\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'upper right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Loss graph with batch normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADI0lEQVQ4y3VSTUtkRxQtZgKBfBDIahbZOYsMZJVZZdEEJBhHXGTCJPkJImhrthMmP8F/kF2akI12ELKwW4MLFyLaQtMiKm2/fs+2+/X76tf93qvPe0K1M2MIzIXLqbpVVJ177mFPn375gDH24e5u/bOjnZ3PTxuNx69+fVVaX1//ZmlpaWF1dXVuZWXl27W1tbnl5eX5l7+8/Lr8c3muXC7bs8WNjY2vLi8vHzebzSf7+/uPmBDyIQA29Ifb/OYGXIgkz/O04HycZdmEcz4uiiIVQthamiejdHztpo7jpM1mM726uhq32+2k2+2i0+n8xgA8sA92e8XuxAmBYkIKABkDYwyICFpr2DBGI88U/J6E53m4uLhAu92G67okhIDv+3+wra2/32OMfdD345p/y6GdjuR5bqQxRnBOUkojhDBKKcM5n6JU3NiwZ1prW5f24yAIfn/LME3T2jAEUBRKX3dAWQZjmVpmrxlapkSAVnd7pdQbnC6SJKmwPM8fWoZxHNZ9X4FLo2SRQTpdiK4LOR5DSAVFBC4lpJTgnEMrBV4UUFKiKApl5QmCoMJmZmYeWYZFMd5JU4LnkdLGTH+mSQrq3cB0u6AwgppkMIbumb3uQBkzLcRxXGGzs7MfMcbej+O4RqQRRVq12xL9vsJorCEUQRQFkEYwfQ/wOlCda9DQhxz0YYIhRJJYYRGGYcVqOM0kSepmyswo21aSGNzeKjgdDdfV8EOg1zcYBgZZpiDjMfJBCD4IIT1XIfQR+YMKOz4+tlNmlqEV3QoshCAiRVoLMkZRUUgaJYriSFAUKXIcQd4tyLnR5PZAjqukHehgEFaY53lTY49Gox07eiKS1hrGkFFWzWloY51JpP6D1qbSWMhzKTkHoii+t00URf9YsW3aKdq2rVnf5F1dQMo7vN/bexxW/+Fw+CcrlUrWNh9vbW2VDg4Ovj85OXm2t7f33enp6UKtVnvearXm6/X682az+axarf54eHi4uL29/cPZ2dn85ubmT41GY6Fa/evF+fn5Yqt19gULgsBK+HY4/88sy6bIGPuUMfbJu+7ZPDo6Yv8CUj2pel0/ITIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/baa29ed3f08a5d4b724717acc1de9ef2/7bc0b/output_55_0.png\"\n        srcset=\"/static/baa29ed3f08a5d4b724717acc1de9ef2/e9ff0/output_55_0.png 180w,\n/static/baa29ed3f08a5d4b724717acc1de9ef2/f21e7/output_55_0.png 360w,\n/static/baa29ed3f08a5d4b724717acc1de9ef2/7bc0b/output_55_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># accuracy 값을 plot 해보겠습니다. </span>\ny_vacc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_accuracy'</span><span class=\"token punctuation\">]</span>\ny_acc <span class=\"token operator\">=</span> history<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span>\nx_len <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_acc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_vacc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Validation-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x_len<span class=\"token punctuation\">,</span> y_acc<span class=\"token punctuation\">,</span> marker<span class=\"token operator\">=</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> c<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span><span class=\"token string\">\"Train-set accuracy\"</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token string\">'lower right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>grid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylim<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'Accurcy graph with batch normalization'</span><span class=\"token punctuation\">)</span> \nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 386px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADGElEQVQ4y31Tz2skRRgtsgiKouhJIboHEQ96COYaDy64ZsIeXPHHX7C72Q0TWAzkb8nBm4Mj0Z2Dp6THCJJoTkNkjGScSSbzq9MzPf1jOtPd1VX11ZPq7ITVgx88XtfXxev3vq5ii4uLc4yxF3d2dt48OTl5Z2tr64P19fWPNzc3P1pbW1teXX1YKBaLt4vF9duPHq19srGxcevx469vPXjwsHDv3mrh/v3VlXL5x/dqtfq7llV9gwkhbgBgruv+xDkH5zxMkiRKU345nU6nSZJecp5GUmYR50kkBI8yHkeaskhkcUTKrKeBJoHRaPgNAzBnBKMo+llrmNJaS0NPAXABZBJIuEamAC8E3ABwXGA4BvoOdDABxl74HSuVSs8xxl7wPM8ikhBCCH8iyR5q6p7GutuYUOckpH7r8gp/DMk5tilq2RQ2uhT93aPgr7aIO0O4F863xmGOOI6sNAV6A0i7ESBuDpB2LsC7DvSFDXgjYGgDySUw8YEsheYJIDh0GkuQRBgEJbawsPAqY+zlMPSqzogwPetL7XSheAIhM0hocKUgNJBKBaEIXEhI0uBZBqEU0iyTpDXG43GJzc/Pv2UchjHfdZsB4LlS5IMElFIgRVBS5Q0lpRlwzqbk07WUV43AOLx58+3XGWPP93rRbthyoLWSWZblm4UQ12zETX/GRPTsWhphz/NK13857PUtCkIQIGcCRmzGRsD06X8Efd8vsYtu5wZjbC4Yjy0TRarcoZZS6hkLIbRSSmdCaKkpfzZTMe9MSZlP6cph37aNIJtE0a75itZaSGlmrEkpRaYMa4C4H5BwRtTudKjdblOv16PBYED9fl/Ytg3HuTo2eWTf938x0QzMjTHxTKQZFEn47hROP8bpaQvNZhNnZ2c5n5+fI45j2Lb9PVtaWjIOX6pUKh/u7+9/VqvVCnt7e58eHR2tWJZ19/j4eLlard79s14v/PCk8uWvv/1+p1KpfFGr1ZbL5fJXBwcHK9vb2583Go079Xr9fdZqtUzi6wP+X8RxnDNj7DXG2CuzfhiG/9pHROzw8JD9A/hQoR0ACR8rAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/d8558b14bc2e985dfb95c978261e3a12/7bc0b/output_56_0.png\"\n        srcset=\"/static/d8558b14bc2e985dfb95c978261e3a12/e9ff0/output_56_0.png 180w,\n/static/d8558b14bc2e985dfb95c978261e3a12/f21e7/output_56_0.png 360w,\n/static/d8558b14bc2e985dfb95c978261e3a12/7bc0b/output_56_0.png 386w\"\n        sizes=\"(max-width: 386px) 100vw, 386px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#l2-norm--ridge\">L2 norm  Ridge</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#%EA%B7%B8%EB%9F%AC%EB%AF%80%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90-%EB%94%B0%EB%9D%BC-%EC%A0%81%EC%A0%88%ED%95%9C-regularization-%EB%B0%A9%EB%B2%95%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%98%EB%8A%94-%EA%B2%83%EC%9D%B4-%EC%A2%8B%EC%8A%B5%EB%8B%88%EB%8B%A4\">그러므로, 데이터에 따라 적절한 Regularization 방법을 활용하는 것이 좋습니다.</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EA%B7%BC%EB%8D%B0-%EA%B7%B8%EB%9E%98%EC%84%9C-norm-%EC%9D%B4%EB%9E%80%EA%B2%8C-%EB%AD%98%EA%B9%8C\">근데 그래서 Norm 이란게 뭘까…?</a></p>\n</li>\n<li>\n<p><a href=\"#dropout-%EC%9D%80-%EB%AD%94%EB%8D%B0\">Dropout 은 뭔데?</a></p>\n<ul>\n<li><a href=\"#overfitting-%EC%A4%84%EC%9D%B4%EB%8A%94-%EB%B2%95\">overfitting 줄이는 법</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#batch-normalization\">Batch Normalization</a></p>\n<ul>\n<li><a href=\"#%EC%9A%94%EA%B1%B4-%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%86%8C%EC%8B%A4-%EC%9D%B4%EB%9E%91-%EA%B8%B0%EC%9A%B8%EA%B8%B0%ED%8F%AC%ED%99%94-%EB%AC%B8%EC%A0%9C%EB%A5%BC-%ED%95%B4%EA%B2%B0%ED%95%9C%EB%8B%A4\">요건 기울기소실 이랑 기울기포화 문제를 해결한다.</a></li>\n<li><a href=\"#%EB%B0%B0%EC%B9%98-%EB%85%B8%EB%A7%90%EB%9D%BC%EC%9D%B4%EC%A0%9C%EC%9D%B4%EC%85%98-%EC%93%B0%EB%A9%B4\">배치 노말라이제이션 쓰면</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"April 21, 2022","title":"정규화 정칙화 차이","categories":"DeepML","author":"하성민","emoji":"😁"},"fields":{"slug":"/DML_norm/"}},"site":{"siteMetadata":{"siteUrl":"https://xman227.github.io","comments":{"utterances":{"repo":"xman227/blog_comments"}}}}},"pageContext":{"slug":"/DML_AI4/","nextSlug":"/DML_keras3/","prevSlug":"/DML_norm/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}