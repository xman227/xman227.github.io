{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/NLP_4/",
    "result": {"data":{"cur":{"id":"6f2cd6e3-4d52-59b8-8a55-7fe83eb9f0c7","html":"<h1 id=\"span-stylebackground-color-fff5b1임베딩이란span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff5b1%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%B4%EB%9E%80span\" aria-label=\"span stylebackground color fff5b1임베딩이란span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff5b1'>임베딩이란..?</span></h1>\n<p>단어를 표현하기 위해서는 적어도 1차원에서는 안된다 (의미가 담기기엔 작다)</p>\n<p>그래서 벡터의 <strong>특정 차원을 직접</strong> 만들어 의미를 직접 mapping 해야 하고,<br>\n이를 희소 표현 (Sparse Representation) 이라고 한다.</p>\n<hr>\n<p>반면에 그냥 차원은 일정하게 256차원 이렇게 정해놓고</p>\n<p>유사한 맥락에서 자주 나오는 단어들은 의미가 비슷하다고 판단하는 방식을\n분포 가설 (distribution hypothesis) 이라고 한다. 그리고 이 가설을 통해 분산표현 (distribution Representation) 이라고 한다.</p>\n<p>맥락이라 함은 단어 좌우에 함께 위치하는 단어를 의미한다.</p>\n<hr>\n<p>분산표현 은 희소표현 과 달리 단어 간 유사도를 구할 수 있다.</p>\n<p>embedding 레이어라는 것은</p>\n<p>이 단어의 분산표현을 구현하기 위한 레이어!!!!!!!!!!!!!!!!!!!!!!</p>\n<p>우리가 단어를 n 개 쓸거야~ k차원으로 구현해조~ 하면</p>\n<p>컴퓨터가 n x k 형태의 분산표현 사전을 만든다.</p>\n<p>이게 weihght 이 되는 거고 파라미터가 된다.</p>\n<hr>\n<p>이 임베딩을 훈련시키기 위해</p>\n<p>word2vec , FastText, Glove, ELMo 등이 있는 거임 방법들이</p>\n<hr>\n<h2 id=\"임베딩-레이어는-컴퓨터가-알아먹는-단어사전이다\" style=\"position:relative;\"><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EB%8A%94-%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B0%80-%EC%95%8C%EC%95%84%EB%A8%B9%EB%8A%94-%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84%EC%9D%B4%EB%8B%A4\" aria-label=\"임베딩 레이어는 컴퓨터가 알아먹는 단어사전이다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>임베딩 레이어는 컴퓨터가 알아먹는 단어사전이다.</h2>\n<p>weight 은</p>\n<ol>\n<li>단어의 개수</li>\n<li>임베딩 사이즈</li>\n</ol>\n<p>로 정의된다.</p>\n<p>임베딩 레이어는 input 데이터를 분산 데이터로 연결해주니 LUT 룩업 테이블<br>\n이라고도 한다.</p>\n<p>그것은 원-핫 인코딩 이라고도 하는데</p>\n<hr>\n<p>원핫 인코딩 자체는 sparse 표현이지만</p>\n<p>embedding 이랑 함께 결합하여 쓰이면 유용하다.</p>\n<p>각 단어가 있으면 그걸 Linear 연산 을 통해 차원값을 만들어낸다!!!</p>\n<p>예를 들어</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 287px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.777777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABJ0AAASdAHeZh94AAAAjUlEQVQI142NSQ6FMAxDORDQBRWlhZYOdJAQCMH9z+Kv5AR/8RTLdpLuOA4QrTXknJlaK3s0Syl/5aSJzhiDYRjwPA/meca6rriuC33fc5lKQgh83wcpJZZlwfu+vHOeJ0IIGMcR931j2zZ0WmsuUkBaKcWaPCrs+45pmhBj5GP0NKXEuXOOodx7D2stfhmHaHQ1aP1BAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/976040ccf895245ecd1bba60556db4fd/480fd/1.png\"\n        srcset=\"/static/976040ccf895245ecd1bba60556db4fd/e9ff0/1.png 180w,\n/static/976040ccf895245ecd1bba60556db4fd/480fd/1.png 287w\"\n        sizes=\"(max-width: 287px) 100vw, 287px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>8차원의 원핫 인코딩이 있다고 해보자</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 306px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 82.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAABJ0AAASdAHeZh94AAACA0lEQVQ4y1XUV26CQQwEYA6EkIDQS0jonSSkQB5y/1Ns9Fkyggdrl7U9nrHNXxkOh+Xl5aXMZrPy/PxcBoNBORwOxftisSjr9bo8PT2VRqNxs2az+fD7/r0iablclvP5XObzeWm32+V4PEaB9/f3uLdarVKv1yOBuSuSwO5ZtAKs2+0GGJDv7++y3++DtZP1er3wMQR+f3/L29tbKLlcLhFLUTCcTqeRMJlMwrHZbG4Br6+v0QaV+/3+rSViVqtVKNtutxHLguFoNIogTgAkfn19ldPpFGwxqtVqcSoO6PPzs3x8fMRdj8fjcYETDCEDxNCjRKYFEshUWYxE74p7d0dC3gNDlVVykuNOCrbX6zWk8gHZ7XYxQAoowTQlB8OUrG9OYBhIZNiaoAQqgN/7nJjymf5DD4GZmh5h8ff3F6yq1eptC7K3TmvFxJIegCqboP64k4YFcCDMFiiqOMth5A67U3hjCFCQKgIE/vz8BAuMyeEDrsf6hiW5ftvVXK9g6AeTZBiq5TS9YdjpdEIF2bl/LO+3HmazOZieYCjIvwGAoVDiTol/iD7zMz3UqgDM7b9fizSFJPNrSxZOMCr4tUDuw1CyR/qR0gEJdJKdXyUg4pJx5gRgTjd7k3+7/NJgA5BJysX2zo8dDD5nJavnx0HFXAdv7vy+SPeDsyrJUH+BUfoPcWxfmb0L+yUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/0f1db7efd568945b6793d92387ce9a0e/98b92/2.png\"\n        srcset=\"/static/0f1db7efd568945b6793d92387ce9a0e/e9ff0/2.png 180w,\n/static/0f1db7efd568945b6793d92387ce9a0e/98b92/2.png 306w\"\n        sizes=\"(max-width: 306px) 100vw, 306px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이런 가중치 를 가진 레이어가 있다고 치면\n저 위의 1 0 0 0 0 0 0 0 에 각 하나의  [ _ _ ] 가 들어가게 되고,</p>\n<p>그 결과값으로 [_ _ ] 의 형태 1 개가 나오겠지 (원핫인코딩의 행이 1이니까)</p>\n<p>그럼 원핫 인코딩이</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 311px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 90.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABJ0AAASdAHeZh94AAABtUlEQVQ4y33UR3LDMBBEUd1IOYvK+f73geuhqmnSsr2gEDTz0ROAwXa7Lc/ns+x2uzIej8v1ei3L5bKO0+m03G63Mp/Py/F4LJvNphwOh7Jarcp+v68+5vf7vTRNU/0H6/W6bgADXC6XFgiUNRAbIGAwc/851LwCh8NhdbIxmUwqPKfOZrPyeDwq+Hw+VyClRDiAD1sRtgr9nE6nVqH5YrGoAMCso4wjIHsqKYygChyNRjW8KKQopwIacwCAA4ApTT5fr9e3QkAnMKYQHEBeALPuFiVKEzJb+z0gg+QwhZI7ioVFGZuAwaJQFL2QhQNKYXJJGYX2gTk7iKNRRMkh217IgJQAAAKkysLhRBnAbyHz7SmkAkDIyV2UdhvbXpQC+j4auwvsAhgk5BQlwJ8K+fYUgrzf7wqUYAq0ApAwoxyk2+Bpcr4tMI3NEJBRQqQQxBrEAQmTyuylA3qNHYUSHIVAxlQ1d/tfhYBRKDRzjsbu1eNonbyB+nQA34+iUEJhGjuPQq5iwksOFefPqwfCMcnPk5Q+THGiOA2efLJJ0WpR5CIPKoeEkWeLE4h196YA5LVJW30B2hRjj0mVs9kAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/fc04f86827c4c59964a4496cc71898b4/ffa0f/3.png\"\n        srcset=\"/static/fc04f86827c4c59964a4496cc71898b4/e9ff0/3.png 180w,\n/static/fc04f86827c4c59964a4496cc71898b4/ffa0f/3.png 311w\"\n        sizes=\"(max-width: 311px) 100vw, 311px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이렇게 10 개 있으면<br>\n10개에 대한 [_ _ ] 값이 나올 것이다. 그럼 그것이<br>\n바로 유사도를 나타내는 벡터값이 될 수 있다.</p>\n<hr>\n<p>다시 말해서 임베딩 레이어란</p>\n<ol>\n<li>단어들을 원핫 인코딩 한다.</li>\n<li>선형변환(레이어 씌우기) 를 한다.</li>\n<li>각 단어들을 {index : 선형변환값 } 으로 저장</li>\n</ol>\n<p>을 해주는 레이어 인 것!!!!!!</p>\n<p>보여주는 코드는 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n\nsome_words <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">57</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장입니다.</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Embedding을 진행할 문장:\"</span><span class=\"token punctuation\">,</span> some_words<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\nembedding_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 총 64개의 단어를 포함한 Embedding 레이어를 선언할 것이고,</span>\n<span class=\"token comment\"># 각 단어는 100차원으로 분산 표현 할 것입니다.</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Embedding된 문장:\"</span><span class=\"token punctuation\">,</span> embedding_layer<span class=\"token punctuation\">(</span>some_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Embedding Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> embedding_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Embedding을 진행할 문장: (1, 3)\nEmbedding된 문장: (1, 3, 100)\nEmbedding Layer의 Weight 형태: (64, 100)</code></pre></div>\n<h4 id=\"근데-임베딩-레이어는-미분을-할수-없는-애라-어떤-연산-결과를\" style=\"position:relative;\"><a href=\"#%EA%B7%BC%EB%8D%B0-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EB%8A%94-%EB%AF%B8%EB%B6%84%EC%9D%84-%ED%95%A0%EC%88%98-%EC%97%86%EB%8A%94-%EC%95%A0%EB%9D%BC-%EC%96%B4%EB%96%A4-%EC%97%B0%EC%82%B0-%EA%B2%B0%EA%B3%BC%EB%A5%BC\" aria-label=\"근데 임베딩 레이어는 미분을 할수 없는 애라 어떤 연산 결과를 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>근데 임베딩 레이어는 미분을 할수 없는 애라 어떤 연산 결과를</h4>\n<h4 id=\"임베딩-레이어에-적으면-안된다네\" style=\"position:relative;\"><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%97%90-%EC%A0%81%EC%9C%BC%EB%A9%B4-%EC%95%88%EB%90%9C%EB%8B%A4%EB%84%A4\" aria-label=\"임베딩 레이어에 적으면 안된다네 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>임베딩 레이어에 적으면 안된다네</h4>\n<h2 id=\"그런-임베딩-레이어와-함께-쓰는-문장-특화-레이어\" style=\"position:relative;\"><a href=\"#%EA%B7%B8%EB%9F%B0-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%99%80-%ED%95%A8%EA%BB%98-%EC%93%B0%EB%8A%94-%EB%AC%B8%EC%9E%A5-%ED%8A%B9%ED%99%94-%EB%A0%88%EC%9D%B4%EC%96%B4\" aria-label=\"그런 임베딩 레이어와 함께 쓰는 문장 특화 레이어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>그런 임베딩 레이어와 함께 쓰는 문장 특화 레이어</h2>\n<h1 id=\"recurrent-layer\" style=\"position:relative;\"><a href=\"#recurrent-layer\" aria-label=\"recurrent layer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Recurrent layer</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABJ0AAASdAHeZh94AAABjklEQVQ4y5VUS0+EMBjk//8Fr3rw4ElPmmjUPZio8RFX1/jCFVZ26YPS0sJCGUPVFRWMO8kktE3nm37fBK+ua7TZYEYF9i8CnI5eMZ/PHU1RoSxLFEXh1g1+3m3ooQX7ITh+pVjfHWHv7AVCCKRCgKkSEUnBOQMhFFVVLUTb8N6V7TeH2uS48WcYR+zXhU987v/psC3atf8fekon8CeXIDxcCGij8BgOEc4esSy8p+AaO8crGD0PYOvKcTK7w8HFGob+nuuhlBKUUvi+D5pMwDiBUhkk55BxjDRN3XlDb0rHGJxv4Cm8XFRJZIyTqy3c+kduqg211u6iKTJkWrp1VZbIpYTRGnmeI1Oq1cP6e7NtXqCu7PJPXjQUXwOx1iJjFLlSyw+lK9giivCwtYmXwWFv3noddmUrCQIM11bh72y7PvW57Ax287w2m2OjFOL7OyRh4FrRhy7XHmUUjHNQxsA5R5IIUM4glHJSrsiHo/Z3r8Ouyu5noDUIIYimU6QyhTEGMSEuk018Kms7Bd8AaahDWzAmluoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/17aa24ec4bd035038c60ced1da695ffa/37523/4.png\"\n        srcset=\"/static/17aa24ec4bd035038c60ced1da695ffa/e9ff0/4.png 180w,\n/static/17aa24ec4bd035038c60ced1da695ffa/f21e7/4.png 360w,\n/static/17aa24ec4bd035038c60ced1da695ffa/37523/4.png 720w,\n/static/17aa24ec4bd035038c60ced1da695ffa/aa08e/4.png 967w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>딥러닝에서 시퀀스 데이터는 순차적인 특성을 꼭 지닌다.</li>\n</ul>\n<p>이런 순차 데이터를 처리하는 레이어가 recurrent layer</p>\n<p>RNN 은 단 하나의 Weight 를 순차적으로 업데이트 한다.</p>\n<p>다음은 RNN 의 예시이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sentence <span class=\"token operator\">=</span> <span class=\"token string\">\"What time is it ?\"</span>\ndic <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"is\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"it\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"What\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"time\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"?\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"RNN에 입력할 문장:\"</span><span class=\"token punctuation\">,</span> sentence<span class=\"token punctuation\">)</span>\n\nsentence_tensor <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span>dic<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> sentence<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Embedding을 위해 단어 매핑:\"</span><span class=\"token punctuation\">,</span> sentence_tensor<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"입력 문장 데이터 형태:\"</span><span class=\"token punctuation\">,</span> sentence_tensor<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nembedding_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>dic<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\nemb_out <span class=\"token operator\">=</span> embedding_layer<span class=\"token punctuation\">(</span>sentence_tensor<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nEmbedding 결과:\"</span><span class=\"token punctuation\">,</span> emb_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Embedding Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> embedding_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nrnn_seq_layer <span class=\"token operator\">=</span> \\\ntf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>SimpleRNN<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> return_sequences<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nrnn_seq_out <span class=\"token operator\">=</span> rnn_seq_layer<span class=\"token punctuation\">(</span>emb_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nRNN 결과 (모든 Step Output):\"</span><span class=\"token punctuation\">,</span> rnn_seq_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Simple RNN Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> rnn_seq_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nrnn_fin_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>SimpleRNN<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nrnn_fin_out <span class=\"token operator\">=</span> rnn_fin_layer<span class=\"token punctuation\">(</span>emb_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nRNN 결과 (최종 Step Output):\"</span><span class=\"token punctuation\">,</span> rnn_fin_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Simple RNN Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> rnn_fin_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">RNN에 입력할 문장: What time is it ?\nEmbedding을 위해 단어 매핑: [[2 3 0 1 4]]\n입력 문장 데이터 형태: (1, 5)\n\nEmbedding 결과: (1, 5, 100)\nEmbedding Layer의 Weight 형태: (5, 100)\n\nRNN 결과 (모든 Step Output): (1, 5, 64)\nRNN Layer의 Weight 형태: (100, 64)\n\nRNN 결과 (최종 Step Output): (1, 64)\nRNN Layer의 Weight 형태: (100, 64)</code></pre></div>\n<p>어떤 문장이 긍정인지 부정인지 나누기 위해서라면 문장을 모두 읽은 후,<br>\n최종 Step의 Output만 확인해도 판단이 가능하다.</p>\n<p>하지만 문장을 생성하는 경우라면<br>\n이전 단어를 입력으로 받아 생성된<br>\n모든 다음 단어, 즉 모든 Step에 대한 Output이 필요하다.</p>\n<p>모든 step 의 output 은 <code class=\"language-text\"> return_sequences=True</code> 로 조절 가능하다</p>\n<p>위의 결과를 보면 결국 마지막에 남는 Weight 은 (100, 64)로 똑같은 값을 가진다.</p>\n<h4 id=\"위의-코드는-아래의-lstm-사용-코드와-동일하다\" style=\"position:relative;\"><a href=\"#%EC%9C%84%EC%9D%98-%EC%BD%94%EB%93%9C%EB%8A%94-%EC%95%84%EB%9E%98%EC%9D%98-lstm-%EC%82%AC%EC%9A%A9-%EC%BD%94%EB%93%9C%EC%99%80-%EB%8F%99%EC%9D%BC%ED%95%98%EB%8B%A4\" aria-label=\"위의 코드는 아래의 lstm 사용 코드와 동일하다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>위의 코드는 아래의 LSTM 사용 코드와 동일하다</h4>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">lstm_seq_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> return_sequences<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nlstm_seq_out <span class=\"token operator\">=</span> lstm_seq_layer<span class=\"token punctuation\">(</span>emb_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nLSTM 결과 (모든 Step Output):\"</span><span class=\"token punctuation\">,</span> lstm_seq_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"LSTM Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> lstm_seq_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nlstm_fin_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nlstm_fin_out <span class=\"token operator\">=</span> lstm_fin_layer<span class=\"token punctuation\">(</span>emb_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nLSTM 결과 (최종 Step Output):\"</span><span class=\"token punctuation\">,</span> lstm_fin_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"LSTM Layer의 Weight 형태:\"</span><span class=\"token punctuation\">,</span> lstm_fin_layer<span class=\"token punctuation\">.</span>weights<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n\nLSTM 결과 (모든 Step Output): (1, 5, 64)\nLSTM Layer의 Weight 형태: (100, 256)\nWARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n\nLSTM 결과 (최종 Step Output): (1, 64)\nLSTM Layer의 Weight 형태: (100, 256)</code></pre></div>\n<p>잠깐잠깐 LSTM 이 뭔데 갑자기 나와..?</p>\n<h1 id=\"recuurent-layer---lstm\" style=\"position:relative;\"><a href=\"#recuurent-layer---lstm\" aria-label=\"recuurent layer   lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Recuurent layer - LSTM</h1>\n<p>; Long short Term memory</p>\n<p>얘도 RNN 레이어의 일종이다.</p>\n<hr>\n<p>딥러닝은 back propagation 으로 가중치의 미분을 구한 다음 업데이트한다.</p>\n<p>가중치를 업데이트 하는 RNN 의 특성상, input 이 길수록 초기 단어의 미분값이<br>\n매우 작아지거나 커지는 현상이 발생한다.</p>\n<p>이 현상을 기울기 소실 (vanishing) 혹은 포화 (exploding) 이라고 한다.</p>\n<p>LSTM 은 일반 RNN보다 4배 큰 가중치 값을 가진다.<br>\n위를 보면 RNN =(100,64) , LSTM = (100,256) 인거를 보면 된다.</p>\n<p>하지만 단순히 weight 가 4배 ‘많은’ 게 아니라 4베 ‘다양한’ 것이다.</p>\n<p>각 weight 는 <code class=\"language-text\">Gate</code> 라는 구조에 포함되어 기억할 정보, 전달할 정보를 결정한다.</p>\n<p>LSTM 에는 <code class=\"language-text\">Cell state</code> 를 통해서 긴 문장의 앞부분도 손실 없이 저장해준다.<br>\n앞서 언급한 Gate 가 Cell state 에 정보를 추가/삭제 한다.</p>\n<hr>\n<h3 id=\"자세한-설명\" style=\"position:relative;\"><a href=\"#%EC%9E%90%EC%84%B8%ED%95%9C-%EC%84%A4%EB%AA%85\" aria-label=\"자세한 설명 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>자세한 설명</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABJ0AAASdAHeZh94AAABuklEQVQoz41T2U7bUBDN/38EP1DxQFWplLZIkARsbGeBFESTOC2CgEli+/ou3n3QTApqSlp6peOZsUfHZ5bbEkJgdHmFdqeL/mCI9skpbMdlv9fvwz5zMTi/wNFxGyeWBcfz+Pvt3Rx06qbB76dVliXCKEIQBFgsl0ikxCoMkSQJlFKIhYBUiuM4jhmUr7RmgqZpNtDCf5yqLre+b36p2yCkR1mUEMUjFsUMQepjkf9gPGYzBNkUD2aKNDeo6/qVoq0K5/dz2FdfcGi/g3f7Fe3BHjqj97D8fXSvP8CefoLldaC1ebOa1rPs8aKPz71dXEc2dj/u4PK+i5/pEJPIgS89ZLXivDRNGcYYSCmRZRmDYhrwC+FN8g2Du0M4/gGOh3uYRDZ86WKaePBVD6ZKOC/PcybUWvPQKCZCiglMmJcpbuQI38UpfO1ilroYCwu+cjBRDsbiDKF54B6+WfKzo8sYslpC1yFUtWLLfr2CaeKNyf5zKFVVcS/WJRgoaaDZaobR6TpWeqvC5s/FpiTqBTWVLEEbDakk+2Sp2bTYWZ4zAYkoimJjD1+V/NelrqqXqdItIn/9I7WV8AkqA5dZu1PQRAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/7407f12fa125bfc200e45f2922c3e77a/37523/5.png\"\n        srcset=\"/static/7407f12fa125bfc200e45f2922c3e77a/e9ff0/5.png 180w,\n/static/7407f12fa125bfc200e45f2922c3e77a/f21e7/5.png 360w,\n/static/7407f12fa125bfc200e45f2922c3e77a/37523/5.png 720w,\n/static/7407f12fa125bfc200e45f2922c3e77a/5b481/5.png 846w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"아래의-그림은-하나의-활성함수를-지닌-기본-rnn-이다\" style=\"position:relative;\"><a href=\"#%EC%95%84%EB%9E%98%EC%9D%98-%EA%B7%B8%EB%A6%BC%EC%9D%80-%ED%95%98%EB%82%98%EC%9D%98-%ED%99%9C%EC%84%B1%ED%95%A8%EC%88%98%EB%A5%BC-%EC%A7%80%EB%8B%8C-%EA%B8%B0%EB%B3%B8-rnn-%EC%9D%B4%EB%8B%A4\" aria-label=\"아래의 그림은 하나의 활성함수를 지닌 기본 rnn 이다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>아래의 그림은 하나의 활성함수를 지닌 기본 RNN 이다.</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABJ0AAASdAHeZh94AAABnUlEQVQoz2WS226bUBBF+f9fqdQ+9SFWL0qkpsFJsImb4GADxsEQMObO4WpWVZw2abul0Uh7NGse9ki81XBqy6mBawSv3ou/W/uslA3/qiprbiYLhuOANAwDTVfTdBVVIzjSYZomRV6MpF9+01YM9ETxgaetM0JEU1C1groViLpgra9GX8qrGCNVsMs7nPqeTaFiZArbYI3mXqNH11i5ihbIqLsL5rrM9vCImd+yFYtxz8xnmMmcQxQiJWKPWd6gJ1M+z95jV3MsoWD5SzRfZu6ec7X8wMyeoEWXaM4tRrDA6VWu1mfIxgSnW2BmCvvQPwFXuYyeysjm2Qg3yimh2PHD+8631Ue+qu9YZzJ2p+CVa7zCwBBTZs4X7rxz7GaGVSh0fYtUtyV2eM9zabGvbbzCHBeS4oDlPhBUG3xhj/NdviKtQ/aJw1OyxBcWz6WJk+p4mUXf90hbL+RSNU6BHl+Ta9qOT9MHuo7/0r5QHnGD9K/P+C0pFxVBnL0BNcRJRBTHaKZNnCSkaULy0rM0Rbe2uL5PVVWnO8Pwp34CjpFXKhO+FB8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/efb88d54d6e244dde8264612aedfae4e/37523/6.png\"\n        srcset=\"/static/efb88d54d6e244dde8264612aedfae4e/e9ff0/6.png 180w,\n/static/efb88d54d6e244dde8264612aedfae4e/f21e7/6.png 360w,\n/static/efb88d54d6e244dde8264612aedfae4e/37523/6.png 720w,\n/static/efb88d54d6e244dde8264612aedfae4e/d2a60/6.png 807w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"이와-달리-lstm-은-한-레이어의-4-가지-가중치-존재\" style=\"position:relative;\"><a href=\"#%EC%9D%B4%EC%99%80-%EB%8B%AC%EB%A6%AC-lstm-%EC%9D%80-%ED%95%9C-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%9D%98-4-%EA%B0%80%EC%A7%80-%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A1%B4%EC%9E%AC\" aria-label=\"이와 달리 lstm 은 한 레이어의 4 가지 가중치 존재 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>이와 달리 LSTM 은 한 레이어의 4 가지 가중치 존재</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABJ0AAASdAHeZh94AAACSUlEQVQ4y2WT2XLaQBBF+f8/yFOqksfk0YmzFcTEjgOOgYDNKjFCG6sEQhubOKkZFjtOV3VNT8/tO92aqxwvbOmHaNWBivf7vXJpabyid2+QZdk/+N1uR/dOsErWap+TBTK53WxVYjb26NR1FWfZE+E63dCsdA/5XcZO+nanzhvlFqv0SDiOBhhBAzfp4ERtRFDH8Fo4U4E+q2L4DwivTn/+h4ZRZuhZDJYPOHEHN+1iR21V7wezA+Fg+YixrnAnvvI4vUYkv9G9GrXOLb/0S26aHyhrX8iX3lP8fUF/3ESPy7S8n5T0zxhplV5QZuw5sD8SiuSOm+5H7s1v9JM7+n6dplGmUHnN5dUrflTe8L32juv6J0yvixaVqNkFrjsXGGmF7qKEF4yPIy/7iEVdjSzbN8NHLL9HW6tR04s07bJyMa/TtmqYroYZNBRW1lhhEzNoMvMnhw5vGzpBmB6ebH90oKE7DL3wpQjomiNaYnSAZ/8dkxPulGS15sS43qwJlwFNzUBYDlG4JAgW+L5PHEf0DIt2f8A+27Fnf5aWlJNcc/DU3El7z7V2ks7L/ElOp/UU57IjUGoxy3ZnwGGfnePtdnsmfu4nojAMnzpcBi5W/y1D+4rThXPbZhUESMrxeEy1WmU2m6nRi8Wi2ssL5/M5hUKBfD5/+IZypNHQxRx0ME2dOE5I0xRDCEbDoQJNJhNFKm2xWGDbNq7rqs43mw1CCOXnX08S2LbLdOqd29c0DcdxVBeWZalYWpIkRFGkMJJQWhzH51f+C2xa1dRk9vPYAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/2d8c740ce607bdb9f8c58d091eca361f/37523/7.png\"\n        srcset=\"/static/2d8c740ce607bdb9f8c58d091eca361f/e9ff0/7.png 180w,\n/static/2d8c740ce607bdb9f8c58d091eca361f/f21e7/7.png 360w,\n/static/2d8c740ce607bdb9f8c58d091eca361f/37523/7.png 720w,\n/static/2d8c740ce607bdb9f8c58d091eca361f/42d54/7.png 858w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>LSTM 이 가진 가장 큰 특징은 상단에 가로로 그어진 Cell ctate 이다.</p>\n<p>얘는 컨베이어 벨트처럼 작은 선형변환을 아주 조금씩 하면서 정보가 나아간다.</p>\n<p>LSTM 은 이 능력을 gate 라고 불리는 구조로 조금씩 변형시킨다.</p>\n<p>Gate = 시그모이드 와 pointwise 곱셈으로 이루어진 정보전달 방법</p>\n<p>시그모이드의 output 은 0과 1로만 이루어져 있어 보낼 정보와 막을 정보를 고른다</p>\n<p>LSTM 은 3개의 gate 값을 가지고 있다. 이 3개로 CELL STATE 에 보낼 값을 제어한다.</p>\n<p>3개의 GATE 는 다음과 같다.</p>\n<ol>\n<li>forgat gate layer</li>\n</ol>\n<p>cell state 에서 지울 값 선정</p>\n<ol start=\"2\">\n<li>input gate layer</li>\n</ol>\n<p>새로운 cell state 를 기존 cell state 에 반영할 정도를 선정</p>\n<ul>\n<li>(여기서 원래 본연의 가중치를 통해 이전 1,2번에서 정한 일 해줌)</li>\n</ul>\n<ol start=\"3\">\n<li>output gate layer</li>\n</ol>\n<p>cell state 로 필터된 output 배출</p>\n<hr>\n<p>1번 , 2번, - 번, 4번 이렇게 총 4번의 레이어 활동으로 LSTM 은 작동한다.</p>\n<hr>\n<p>이 외에도 뭐</p>\n<p>엿보기 LSTM,, GRU,,,</p>\n<p>BIRNN 등 뭐 이것저것 많다\n아래 코드는 양방향(Bidirectional) RNN 코드임</p>\n<p>양방향이라서 가중치가 두배임 앞 뒤에서 가야되니까</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n\nsentence <span class=\"token operator\">=</span> <span class=\"token string\">\"What time is it ?\"</span>\ndic <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"is\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"it\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"What\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"time\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"?\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span>\n<span class=\"token punctuation\">}</span>\n\nsentence_tensor <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span>dic<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> sentence<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nembedding_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>dic<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\nemb_out <span class=\"token operator\">=</span> embedding_layer<span class=\"token punctuation\">(</span>sentence_tensor<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"입력 문장 데이터 형태:\"</span><span class=\"token punctuation\">,</span> emb_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nbi_rnn <span class=\"token operator\">=</span> \\\ntf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Bidirectional<span class=\"token punctuation\">(</span>\n    tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>SimpleRNN<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> use_bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> return_sequences<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\nbi_out <span class=\"token operator\">=</span> bi_rnn<span class=\"token punctuation\">(</span>emb_out<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Bidirectional RNN 결과 (최종 Step Output):\"</span><span class=\"token punctuation\">,</span> bi_out<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">입력 문장 데이터 형태: (1, 5, 100)\nBidirectional RNN 결과 (최종 Step Output): (1, 5, 128)</code></pre></div>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EB%8A%94-%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B0%80-%EC%95%8C%EC%95%84%EB%A8%B9%EB%8A%94-%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84%EC%9D%B4%EB%8B%A4\">임베딩 레이어는 컴퓨터가 알아먹는 단어사전이다.</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#%EA%B7%BC%EB%8D%B0-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EB%8A%94-%EB%AF%B8%EB%B6%84%EC%9D%84-%ED%95%A0%EC%88%98-%EC%97%86%EB%8A%94-%EC%95%A0%EB%9D%BC-%EC%96%B4%EB%96%A4-%EC%97%B0%EC%82%B0-%EA%B2%B0%EA%B3%BC%EB%A5%BC\">근데 임베딩 레이어는 미분을 할수 없는 애라 어떤 연산 결과를</a></li>\n<li><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%97%90-%EC%A0%81%EC%9C%BC%EB%A9%B4-%EC%95%88%EB%90%9C%EB%8B%A4%EB%84%A4\">임베딩 레이어에 적으면 안된다네</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EA%B7%B8%EB%9F%B0-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%99%80-%ED%95%A8%EA%BB%98-%EC%93%B0%EB%8A%94-%EB%AC%B8%EC%9E%A5-%ED%8A%B9%ED%99%94-%EB%A0%88%EC%9D%B4%EC%96%B4\">그런 임베딩 레이어와 함께 쓰는 문장 특화 레이어</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#%EC%9C%84%EC%9D%98-%EC%BD%94%EB%93%9C%EB%8A%94-%EC%95%84%EB%9E%98%EC%9D%98-lstm-%EC%82%AC%EC%9A%A9-%EC%BD%94%EB%93%9C%EC%99%80-%EB%8F%99%EC%9D%BC%ED%95%98%EB%8B%A4\">위의 코드는 아래의 LSTM 사용 코드와 동일하다</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%9E%90%EC%84%B8%ED%95%9C-%EC%84%A4%EB%AA%85\">자세한 설명</a></p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%95%84%EB%9E%98%EC%9D%98-%EA%B7%B8%EB%A6%BC%EC%9D%80-%ED%95%98%EB%82%98%EC%9D%98-%ED%99%9C%EC%84%B1%ED%95%A8%EC%88%98%EB%A5%BC-%EC%A7%80%EB%8B%8C-%EA%B8%B0%EB%B3%B8-rnn-%EC%9D%B4%EB%8B%A4\">아래의 그림은 하나의 활성함수를 지닌 기본 RNN 이다.</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%9D%B4%EC%99%80-%EB%8B%AC%EB%A6%AC-lstm-%EC%9D%80-%ED%95%9C-%EB%A0%88%EC%9D%B4%EC%96%B4%EC%9D%98-4-%EA%B0%80%EC%A7%80-%EA%B0%80%EC%A4%91%EC%B9%98-%EC%A1%B4%EC%9E%AC\">이와 달리 LSTM 은 한 레이어의 4 가지 가중치 존재</a></p>\n</li>\n</ul>\n</div>","excerpt":"임베딩이란..? 단어를 표현하기 위해서는 적어도 1차원에서는 안된다 (의미가 담기기엔 작다) 그래서 벡터의 특정 차원을 직접 만들어 의미를 직접 mapping 해야 하고, 이를 희소 표현 (Sparse Representation) 이라고 한다. 반면에 그냥 차원은 일정하게 256차원 이렇게 정해놓고 유사한 맥락에서 자주 나오는 단어들은 의미가 비슷하다고 판단하는 방식을\n분포 가설 (distribution hypothesis) 이라고 한다. 그리고 이 가설을 통해 분산표현 (distribution Representation) 이라고 한다. 맥락이라 함은 단어 좌우에 함께 위치하는 단어를 의미한다. 분산표현 은 희소표현 과 달리 단어 간 유사도를 구할 수 있다. embedding 레이어라는 것은 이 단어의 분산표현을 구현하기 위한 레이어!!!!!!!!!!!!!!!!!!!!!! 우리가 단어를 n 개 쓸거야~ k차원으로 구현해조~ 하면 컴퓨터가 n x k 형태의 분산표현 사전을 만든다. …","frontmatter":{"date":"April 21, 2022","title":"임베딩이란","categories":"STUDY","author":"하성민","emoji":"😁"},"fields":{"slug":"/NLP_4/"}},"next":{"id":"594039ec-3322-5302-b54c-d19ecd03dc20","html":"<h1 id=\"span-stylebackground-color-fff5b1딥러닝모델-vgg-16--️span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff5b1%EB%94%A5%EB%9F%AC%EB%8B%9D%EB%AA%A8%EB%8D%B8-vgg-16--%EF%B8%8Fspan\" aria-label=\"span stylebackground color fff5b1딥러닝모델 vgg 16  ️span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff5b1'>딥러닝모델 VGG-16  🚶🏽‍♂️</span></h1>\n<p>오늘 구현한 모델은 VGG-16 이다.</p>\n<h2 id=\"목차\" style=\"position:relative;\"><a href=\"#%EB%AA%A9%EC%B0%A8\" aria-label=\"목차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>목차</h2>\n<ol>\n<li>데이터 불러오기</li>\n<li>VGG-16 생성</li>\n<li>본인의 과업에 맞게 VGG-16 개선</li>\n</ol>\n<p>이 모델은 미국에서 진행한 이미지 인식 대회 ILSVRC 에서<br>\n2014년 준우승을 한 모델이다! (물론 지금은 더 좋은게 많이 있다)</p>\n<p>하지만 계속 발전되는 Deep Learning 모델을 이해하기 위해선<br>\n기본적인 구조를 갖춘 VGG-16 의 모델 이해가 필요하다.</p>\n<p>본 게시글에서는 VGG-16 를 Tensorflow 라이브러리에서 불러와<br>\n사용하는 방법을 제시한다.</p>\n<p>모델 사용에는 tensorflow 실습 데이터인\n<code class=\"language-text\">cats vs dogs</code> 를 사용했다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">import</span> tensorflow_datasets <span class=\"token keyword\">as</span> tfds\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token operator\">%</span>matplotlib inline\n<span class=\"token operator\">%</span>config InlineBackend<span class=\"token punctuation\">.</span>figure_format <span class=\"token operator\">=</span> <span class=\"token string\">'retina'</span>\n\n\n\n<span class=\"token punctuation\">(</span>raw_train<span class=\"token punctuation\">,</span> raw_validation<span class=\"token punctuation\">,</span> raw_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> metadata <span class=\"token operator\">=</span> tfds<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'cats_vs_dogs'</span><span class=\"token punctuation\">,</span>\n    split<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train[:80%]'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'train[80%:90%]'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'train[90%:]'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    with_info<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    as_supervised<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\nIMG_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">160</span> <span class=\"token comment\"># 리사이징할 이미지의 크기</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">format_example</span><span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    image <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># image=float(image)같은 타입캐스팅의  텐서플로우 버전입니다.</span>\n    image <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>image<span class=\"token operator\">/</span><span class=\"token number\">127.5</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span> <span class=\"token comment\"># 픽셀값의 scale 수정</span>\n    image <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>image<span class=\"token punctuation\">.</span>resize<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>IMG_SIZE<span class=\"token punctuation\">,</span> IMG_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> image<span class=\"token punctuation\">,</span> label\n\ntrain <span class=\"token operator\">=</span> raw_train<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\nvalidation <span class=\"token operator\">=</span> raw_validation<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\ntest <span class=\"token operator\">=</span> raw_test<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /aiffel/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n\n\n\nDl Completed...: 0 url [00:00, ? url/s]\n\n\n\nDl Size...: 0 MiB [00:00, ? MiB/s]\n\n\n\nGenerating splits...:   0%|          | 0/1 [00:00&lt;?, ? splits/s]\n\n\n\nGenerating train examples...:   0%|          | 0/23262 [00:00&lt;?, ? examples/s]\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\nShuffling cats_vs_dogs-train.tfrecord...:   0%|          | 0/23262 [00:00&lt;?, ? examples/s]\n\n\n\u001b[1mDataset cats_vs_dogs downloaded and prepared to /aiffel/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\nget_label_name <span class=\"token operator\">=</span> metadata<span class=\"token punctuation\">.</span>features<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>int2str\n\n<span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">,</span> idx<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    image <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>image <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">2</span>\n    plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'label </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>label<span class=\"token punctuation\">}</span></span><span class=\"token string\">: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>get_label_name<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">'off'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAADNUlEQVQozwEqA9X8AFVQQz6LiGA8iYpjLaqATg9FOB5BLzYROS4uETd1hUwNboNbOLayrzyCiIw9NT09D11FEC1aSiFAXUsbO2dNDRdmZWghR0RBQVdRTzx/fX0uAGdgVf6smnL/uKV8xI1tS2pkSCr/OzQe9l8/IexxgURcioJW6ryGYveWe2j/NjUyZyIZA8toVy/8eWY//29dN4SNiIWhfHhz/29rZ/aVk5PkAF5VS/88Jx7/d2VMyW5vN25kQCP/ayoh/l09IPJgcTtgiHde8p1/b/+Nblf/hndsaxgdIdJPRjj/f3Zp/3FnV4kuLSimGx8Z/zM2Lf1yZFXrAHZnVf9hVTb/ZFtC1WZoNHRiPyX/fzQd/1NFHv9ueklllaKC/3iFaP+PkIL/xsK7cVJELN10Xz3/l4l3/2deUpFmYV2vendz/5GJhP+WiID4AFtURmpUWkFrQkc8UlBPJy48MBxsXDwgZ2lYL2NsdU8oj6N8Ymh/P2d2gWhryMa4LH1SD1WDYitpdmA4blpOMjjPxcJEw7m1brSsqmeqoqFhAP/60AD///cA28a0AElFKAD/4HcA//+xAKeMTgBFTj4A///lAP//kgDb3rYApqefANafTQD//9UA48OPAHRrVQD/+dkA////AP///wDEuKcAALOPdsLovqfCyJR/lkQ4OkxZUEbGbGVavDo2N7EhJTRBZGZtsqOMdr6Xh3m/aHKGSjo0MJqvrq7BW1lWxz4+QWNHRTh4Xlg8yVBJLLxCPSerAJ9/a/+PeW3/uJCB04RsTnSef1D/n4ts/4tzUP8vLTBlbniF/cazoP+oloT/ZHCAcW1nZNyUiYb/Z2Jf/0xLS5BbVTOvT0s0/1tUOP9IQir3AJqQg/94dW//t6uey6OXgW+5nm3/sZNl/52JbPVDRlJganiO9KGbmP+IgXz/X2lxbGVcVtN3bWj/cGtl/19cWYo4NCGofn53/2xtZ/9IQi/tAK+gjuWqoJLmy72nsHl+gGChjnHprZNq3oSGgtReZm1UeX+P032Dit+AhYjlgoWHXVxVT7d1cGnke3Rs7HFqY3hBQjaRmpqV7cvIw91GQTHOUnqDylKzWK4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/1271a3703b5b224b0557b0d74541dd9c/37523/output_2_0.png\"\n        srcset=\"/static/1271a3703b5b224b0557b0d74541dd9c/e9ff0/output_2_0.png 180w,\n/static/1271a3703b5b224b0557b0d74541dd9c/f21e7/output_2_0.png 360w,\n/static/1271a3703b5b224b0557b0d74541dd9c/37523/output_2_0.png 720w,\n/static/1271a3703b5b224b0557b0d74541dd9c/302a4/output_2_0.png 1080w,\n/static/1271a3703b5b224b0557b0d74541dd9c/ee3fb/output_2_0.png 1144w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"vgg-16-불러오기\" style=\"position:relative;\"><a href=\"#vgg-16-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\" aria-label=\"vgg 16 불러오기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VGG 16 불러오기</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">feature_batch <span class=\"token operator\">=</span> base_model<span class=\"token punctuation\">(</span>image_batch<span class=\"token punctuation\">)</span>\nfeature_batch<span class=\"token punctuation\">.</span>shape</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">TensorShape([32, 5, 5, 512])</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">base_model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Model: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 160, 160, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 160, 160, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 160, 160, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 80, 80, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 80, 80, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 80, 80, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 40, 40, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 40, 40, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 40, 40, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 40, 40, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 20, 20, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 20, 20, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 10, 10, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________</code></pre></div>\n<p>18층으로 구성되어 있다.</p>\n<p>해당 사이트에서 전문적인 내용을 다룬다.</p>\n<p><a href=\"https://neurohive.io/en/popular-networks/vgg16/\">자세한 내용을 보시려면</a></p>\n<p>이 모델은 해당 그림을 구현하고 있다.</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 565px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 62.22222222222222%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABJ0AAASdAHeZh94AAABuElEQVQoz52Sz2sTQRzF9w8RwZungp68efMP8FgPgheh/4Jn/QcEoSIowYOtpb0IHryp/4ERTZO02R8m7e5kszvZndnZbPKRGdtQYirUBw+WZebNe9/v88qypNKKUSyYyClRGJBNJlRVhcVisbgSPXtpvljwZPs9h/1j0vGYc1xVbCm48+kLjzcf4R92KIoCmWUopf7PYW+YsvX0GR933jqBcRTS63bJZb4UXcX55XXwPnz9zN2Hm/w88qE2jIKAcRTR7XQIBgNSIUjTFCkldV0j85wsy1yS+Xzu/jVN42i/vdZBi437D+gdBzTFFDEcIuOY0WhEJQThYIDv+0zC0D1w1O9TCkFezTjNC6IkJc6nyCx1D3mt/V1ubtzmx/c2GOMEsyQmEQJTFGil/kQ/c2HJWdx1ob3ddwdcv3aDdvsbKMWvIKDS2tm/OKfKGFclS6U1xpi1S/Oev3jFrTv38IPIubAzWleb/OSUJI6JogiRJGit1wu+fLPH9us9pjLDmPrSqsiyJAxDR7uky2rldfsBclogROK2tnpg6XbeoLVyo/hXP72L8VYPWlgB68ZGnM1mf/VwVfA3ueuPgaIpqCoAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"VGG\" title=\"VGG\" src=\"/static/70208de06e060390432303c3020768e8/07eba/VGG.png\" srcset=\"/static/70208de06e060390432303c3020768e8/e9ff0/VGG.png 180w,\n/static/70208de06e060390432303c3020768e8/f21e7/VGG.png 360w,\n/static/70208de06e060390432303c3020768e8/07eba/VGG.png 565w\" sizes=\"(max-width: 565px) 100vw, 565px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p>그림의 세 번째 파란상자를 보면 fully nected 라고 되어있는데,<br>\nfully connected layer 의 오타이다. 순방향 신경망을 의미한다.</p>\n<p>18층으로 구성되어있으나 VGG-16 인 이유는<br>\nconvolution layer 와 fully connected layer 만 포함했기 때문이다.<br>\n이 두 개의 총 합은 16개이다.</p>\n<p>현재 파이썬에 불러온 모델 상에는\n마지막 maxfooling 까지만 구현되어있다.</p>\n<p>때문에 마지막 네 개의 층을 직접 도입해보기로 하자!</p>\n<h3 id=\"vgg-16-개선\" style=\"position:relative;\"><a href=\"#vgg-16-%EA%B0%9C%EC%84%A0\" aria-label=\"vgg 16 개선 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VGG-16 개선</h3>\n<hr>\n<p>마지막 layer 까지 제공해주지 않는 이유는<br>\n우리가 입력하는 input 데이터에 따라 output 데이터도<br>\n달라지기 때문이다.</p>\n<p>마지막 layer 인 Dense layer (Fully connected layer) 에 넣어주기 위해서는 input data 를 Flatten 시킨 후 입력하여야 한다.</p>\n<p>우리 shape 는 32,5,5,512 라서 이거 한줄로 만들어줄건데</p>\n<p>아래는 Flatten의 예시이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\nimage <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                  <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nflattened_image <span class=\"token operator\">=</span> image<span class=\"token punctuation\">.</span>flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Original image:\\n\"</span><span class=\"token punctuation\">,</span> image<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Original image shape:\"</span><span class=\"token punctuation\">,</span> image<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Flattened image:\\n\"</span><span class=\"token punctuation\">,</span> flattened_image<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Flattened image shape:\"</span><span class=\"token punctuation\">,</span> flattened_image<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Original image:\n [[1 2]\n [3 4]]\nOriginal image shape: (2, 2)\n\nFlattened image:\n [1 2 3 4]\nFlattened image shape: (4,)</code></pre></div>\n<p>이처럼 차원이 존재하는 배열데이터를 한 줄로<br>\n이어준다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>\n<p>아 근데 이거 말고 더 좋은게 있데</p>\n<p>그게 바로 Global Average Pooling</p>\n<p>3차원의 tensor 가 있을때 (예를 들어, 가로, 세로, 채널)<br>\n겹겹이 쌓여있는 2차원 배열의 평균을 구한 후 하나로 축소하는 방법</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">global_average_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>GlobalAveragePooling2D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># global Average 사용</span>\n\nfeature_batch_average <span class=\"token operator\">=</span> global_average_layer<span class=\"token punctuation\">(</span>feature_batch<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">#만든 glo aver 를 이어 붙이기</span>\n\ndense_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span>\nprediction_layer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># feature_batch_averag가 dense_layer를 거친 결과가 다시 prediction_layer를 거치게 되면</span>\nprediction_batch <span class=\"token operator\">=</span> prediction_layer<span class=\"token punctuation\">(</span>dense_layer<span class=\"token punctuation\">(</span>feature_batch_average<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  \n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>prediction_batch<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 사용</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">(32, 2)</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">base_model<span class=\"token punctuation\">.</span>trainable <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n\nmodel <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n  base_model<span class=\"token punctuation\">,</span>\n  global_average_layer<span class=\"token punctuation\">,</span>\n  dense_layer<span class=\"token punctuation\">,</span>\n  prediction_layer\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<p>이게 최종 모델이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 5, 5, 512)         14714688  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               262656    \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 1026      \n=================================================================\nTotal params: 14,978,370\nTrainable params: 263,682\nNon-trainable params: 14,714,688\n_________________________________________________________________</code></pre></div>\n<p>VGG16 모델 밑으로 Flatten(Global_average), Dense 레이어 2개가<br>\n들어갔다.</p>\n<p>모델의 사용은 다음 게시글에 이어서 쓰도록 한다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EB%AA%A9%EC%B0%A8\">목차</a></p>\n<ul>\n<li><a href=\"#vgg-16-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\">VGG 16 불러오기</a></li>\n<li><a href=\"#vgg-16-%EA%B0%9C%EC%84%A0\">VGG-16 개선</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","frontmatter":{"date":"April 21, 2022","title":"VGG16","categories":"STUDY","author":"하성민","emoji":"😁"},"fields":{"slug":"/DML_AI2/"}},"prev":{"id":"178ea2b3-2dfd-599d-86ad-b2669c612f60","html":"<h1 id=\"span-stylebackground-color-fff4f5인공지능-기초-️-사진-분류-프로그램-맛보기span\" style=\"position:relative;\"><a href=\"#span-stylebackground-color-fff4f5%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B8%B0%EC%B4%88-%EF%B8%8F-%EC%82%AC%EC%A7%84-%EB%B6%84%EB%A5%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-%EB%A7%9B%EB%B3%B4%EA%B8%B0span\" aria-label=\"span stylebackground color fff4f5인공지능 기초 ️ 사진 분류 프로그램 맛보기span permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><span style='background-color: #fff4f5'>인공지능 기초 🚶‍♂️: 사진 분류 프로그램 맛보기</span></h1>\n<h2 id=\"강아지--고양이-분류-프로그램-제작-\" style=\"position:relative;\"><a href=\"#%EA%B0%95%EC%95%84%EC%A7%80--%EA%B3%A0%EC%96%91%EC%9D%B4-%EB%B6%84%EB%A5%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-%EC%A0%9C%EC%9E%91-\" aria-label=\"강아지  고양이 분류 프로그램 제작  permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>강아지 &#x26; 고양이 분류 프로그램 제작 !</h2>\n<h3 id=\"content\" style=\"position:relative;\"><a href=\"#content\" aria-label=\"content permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Content</h3>\n<ol>\n<li>데이터 불러오기 (Tensor Flow)</li>\n<li>데이터 전처리</li>\n<li>모델 생성 (자체 생성 모델)</li>\n<li>학습 및 평가</li>\n</ol>\n<p><strong>시작하기에 앞서, 이번 게시글은<br>\n전체적인 인공지능 적용에 대한 맥락을 설명하기 위한 글입니다.<br>\n아주 간단한 Deep Learning Layer 몇 종류를 이용했습니다.</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>__version__ <span class=\"token punctuation\">,</span> <span class=\"token string\">'이미지 분류 모델을 만들 라이브러리 tensor flow 입니다'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">2.6.0 이미지 분류 모델을 만들 라이브러리 tensor flow 입니다</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow_datasets <span class=\"token keyword\">as</span> tfds\n\ntfds<span class=\"token punctuation\">.</span>__version__</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">'4.4.0'</code></pre></div>\n<p>tensor flow 에서는 다양한 데이터셋을 이미 제공하고 있다.<br>\n강아지고양이, 음성, 이미지, 텍스트 데이터셋 보유하고 있으니 세부 내용 확인해보고 싶음 해보기</p>\n<p><a href=\"https://www.tensorflow.org/datasets/catalog/overview\">tensor flow link</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">(</span>raw_train<span class=\"token punctuation\">,</span> raw_validation<span class=\"token punctuation\">,</span> raw_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> metadata <span class=\"token operator\">=</span> tfds<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'cats_vs_dogs'</span><span class=\"token punctuation\">,</span>\n    split<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train[:80%]'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'train[80%:90%]'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'train[90%:]'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    with_info<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    as_supervised<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /aiffel/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n\n\n\nDl Completed...: 0 url [00:00, ? url/s]\n\n\n\nDl Size...: 0 MiB [00:00, ? MiB/s]\n\n\n\nGenerating splits...:   0%|          | 0/1 [00:00&lt;?, ? splits/s]\n\n\n\nGenerating train examples...:   0%|          | 0/23262 [00:00&lt;?, ? examples/s]\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\nShuffling cats_vs_dogs-train.tfrecord...:   0%|          | 0/23262 [00:00&lt;?, ? examples/s]\n\n\n\u001b[1mDataset cats_vs_dogs downloaded and prepared to /aiffel/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m</code></pre></div>\n<hr>\n<p>“WARNING:absl:1738 images were corrupted and were skipped”라는 경고가 나타날 수 있습니다. 우선 무시하시면 됩니다.<br>\n1738 장의 사진은 쓸 수 없다는 뜻입니다.<br>\n이런 것들은 tf 사이트 데이터셋 설명에서 확인 가능합니다</p>\n<p><img src=\"/a0b5d21d105e7623091003cd8f24e715/1.png\" alt=\"PNG\"></p>\n<p>1738이 currupted 되어있다 되어있져?<br>\n그리고 밑에 Split을 보면 23262 장의 사진이 있음을 확인 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>raw_train<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>raw_validation<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>raw_test<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n&lt;PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n&lt;PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)></code></pre></div>\n<p>잘 변수로 지정되어있음을 알 수 있다.</p>\n<p>모든 데이터셋은 (image, label)의 형태를 가집니다.<br>\n((None, None, 3), ())가 이를 나타내죠.</p>\n<p>여기에서 앞에 있는 (None, None, 3)은 image의 shape를,<br>\n뒤의 ()는 정답 카테고리인 label의 shape를 의미합니다.</p>\n<p>이미지는 (height, width, channel)로 3차원 데이터이기 때문에<br>\n(None, None, 3)과 같이 나타났습니다.</p>\n<p>이때 height와 width가 None으로 나타난 이유는<br>\n모든 사진들의 크기가 제각각이기 때문입니다.<br>\n하나의 값으로 나타낼 수 없으니 None 으로 표기됩니다.</p>\n<h2 id=\"1-데이터-전처리\" style=\"position:relative;\"><a href=\"#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\" aria-label=\"1 데이터 전처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 데이터 전처리</h2>\n<p>자 데이터를 불러왔으니 깔끔하게 처리해야지</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token operator\">%</span>matplotlib inline\n<span class=\"token operator\">%</span>config InlineBackend<span class=\"token punctuation\">.</span>figure_format <span class=\"token operator\">=</span> <span class=\"token string\">'retina'</span></code></pre></div>\n<p>데이터를 어떻게 처리할지 고민하기 위해서 우리가 가져온 데이터를<br>\n스리슬쩍 들여다 보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#전체 칸바스 크기</span>\n\nget_label_name <span class=\"token operator\">=</span> metadata<span class=\"token punctuation\">.</span>features<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>int2str \n\n<span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>raw_train<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>  <span class=\"token comment\"># 10개의 데이터를 따로 가져 옵니다.</span>\n    plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">,</span> idx<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#10개의 사진을 꺼내보겠음 판 꺼냄</span>\n    plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'label </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>label<span class=\"token punctuation\">}</span></span><span class=\"token string\">: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>get_label_name<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">'off'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.222222222222214%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAADNUlEQVQozwEqA9X8AIqJkgD//98A////AMO5kgB3WDdNSEEedTo8GXVNPhk/ZaBgEHmIfyydqbYmQ1NdExwaDgDtw1sA+tJ9AINtQwDq4d4A////APPr5AD+//8AAFRRVXZ3b1uhs6N3pberglF4UzK5Uz8l/084If9nSSKWfZBZlauFYPGwinHwXFdTjxsNAFlZSSOsdWE0qW1YK2GQjItZeXNvqm9rZ6WMi41yAH99cNVdUET/aFQ9/5eHYphRSiCoaDIj/2kxIP9URSCKfn9YrJh7Z/+WdWD/kXdknRgcHalJQC//gXRf/3RnVLJHRUGgOjs3/1FQSf9+cmfIAJGOd6pnWETjVlA16V1YP3hmWy66djUh/2w8Hv9BQxWWmqiFsoKQcP98gWz/vr22p2VQLoNrVTHpkH5k6W1lWY1xa2eBf3t365GKhOeWiIGhAFNcVgs1RDgPP15WEGeUaARIQiCARywXuXBNJrpdVSVojaKGNXuSWVxlc0hbqbCpOUAAAAeWdT0TemUwEkY3CQz///8B////Av///wH///8BAP///wAAAAACAAAAAf/h3wAHDjcdAA4wNgAJJC8NF0YZXmtnAKaqfQCZkWkApKqsAC8ZAwDm3M0Aq6SWAD88NgAREhYsV1NCWi8sGlcqKBk3AGJdTqPPo4nb166a4bl+bHNZUEKsiXZc/3BlVf86NDKUP0hXQntxaYKWf2d5goWQQhwXFEGPjo58amhneywsMz5PTDSRWFI0/1tTMP9EPya3AGhgVcqWemj/hnRq/7mUh5CegU2sqoxe/6KLaP+FZz2MQlBmtaejoP+2nYX/fIGIplZSUaSOhID/cmxo/0xKS6xeVzN/VFE//1ZSPP9KQyypAIh9bs+akob/l5KH/761qpSrmnezvJ9t/6mQav92bmOTW2h/p4OJlP+IhYL/dXyCm15XUZtwZ2H/d3Fq/2xmYaIrKBiEdXRs/3Jya/9CPSqvAIVxX22snYuRs6eWlcO2oUyFgnqOspRn156Se9ZtdHh8e36JEoSJkCuIk5gqp6mqGFZPSRlxb2csfXZtLq2clBBBQDKBmZmT+Lm2svZJRDOf3gpvCUkCLewAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/d16b019357261570c527d6ef9965d80a/37523/output_13_0.png\"\n        srcset=\"/static/d16b019357261570c527d6ef9965d80a/e9ff0/output_13_0.png 180w,\n/static/d16b019357261570c527d6ef9965d80a/f21e7/output_13_0.png 360w,\n/static/d16b019357261570c527d6ef9965d80a/37523/output_13_0.png 720w,\n/static/d16b019357261570c527d6ef9965d80a/302a4/output_13_0.png 1080w,\n/static/d16b019357261570c527d6ef9965d80a/ee3fb/output_13_0.png 1144w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>자 봐라, 이미지 크기가 다 제각각이네?</p>\n<p>올바른 학습을 위해서는 이미지 사이즈부터 제대로 맞춰줘야 한다.</p>\n<p>format_example() 함수로 이미지를 같은 포멧으로 맞춥니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">IMG_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">160</span> <span class=\"token comment\"># 리사이징할 이미지의 크기</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">format_example</span><span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    image <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># image=float(image)같은 타입캐스팅의  텐서플로우 버전입니다.</span>\n    image <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>image<span class=\"token operator\">/</span><span class=\"token number\">127.5</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span> <span class=\"token comment\"># 픽셀값의 scale 수정</span>\n    image <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>image<span class=\"token punctuation\">.</span>resize<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>IMG_SIZE<span class=\"token punctuation\">,</span> IMG_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> image<span class=\"token punctuation\">,</span> label</code></pre></div>\n<p>픽셀값의 scale 을 수정햇다는 것은 픽셀값을 정규화햇다는 말과 근사하다.<br>\n0~ 255인 픽셀값을 127.5로 나누면 0~ 2가된다. 그걸 1로 뺐으니<br>\n-1~1 사이의 값으로 변했다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train <span class=\"token operator\">=</span> raw_train<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\nvalidation <span class=\"token operator\">=</span> raw_validation<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\ntest <span class=\"token operator\">=</span> raw_test<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>format_example<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>validation<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;MapDataset shapes: ((160, 160, 3), ()), types: (tf.float32, tf.int64)>\n&lt;MapDataset shapes: ((160, 160, 3), ()), types: (tf.float32, tf.int64)>\n&lt;MapDataset shapes: ((160, 160, 3), ()), types: (tf.float32, tf.int64)></code></pre></div>\n<p>map 메서드로 모든 raw_** 의 정보를 format_example 의 함수로 변환시켜주었다.<br>\n변환된 후의 내용물은 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\nget_label_name <span class=\"token operator\">=</span> metadata<span class=\"token punctuation\">.</span>features<span class=\"token punctuation\">[</span><span class=\"token string\">'label'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>int2str\n\n<span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">,</span> idx<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    image <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>image <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">2</span>\n    plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'label </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>label<span class=\"token punctuation\">}</span></span><span class=\"token string\">: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>get_label_name<span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">'off'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAADNUlEQVQozwEqA9X8AFVQQz6LiGA8iYpjLaqATg9FOB5BLzYROS4uETd1hUwNboNbOLayrzyCiIw9NT09D11FEC1aSiFAXUsbO2dNDRdmZWghR0RBQVdRTzx/fX0uAGdgVf6smnL/uKV8xI1tS2pkSCr/OzQe9l8/IexxgURcioJW6ryGYveWe2j/NjUyZyIZA8toVy/8eWY//29dN4SNiIWhfHhz/29rZ/aVk5PkAF5VS/88Jx7/d2VMyW5vN25kQCP/ayoh/l09IPJgcTtgiHde8p1/b/+Nblf/hndsaxgdIdJPRjj/f3Zp/3FnV4kuLSimGx8Z/zM2Lf1yZFXrAHZnVf9hVTb/ZFtC1WZoNHRiPyX/fzQd/1NFHv9ueklllaKC/3iFaP+PkIL/xsK7cVJELN10Xz3/l4l3/2deUpFmYV2vendz/5GJhP+WiID4AFtURmpUWkFrQkc8UlBPJy48MBxsXDwgZ2lYL2NsdU8oj6N8Ymh/P2d2gWhryMa4LH1SD1WDYitpdmA4blpOMjjPxcJEw7m1brSsqmeqoqFhAP/60AD///cA28a0AElFKAD/4HcA//+xAKeMTgBFTj4A///lAP//kgDb3rYApqefANafTQD//9UA48OPAHRrVQD/+dkA////AP///wDEuKcAALOPdsLovqfCyJR/lkQ4OkxZUEbGbGVavDo2N7EhJTRBZGZtsqOMdr6Xh3m/aHKGSjo0MJqvrq7BW1lWxz4+QWNHRTh4Xlg8yVBJLLxCPSerAJ9/a/+PeW3/uJCB04RsTnSef1D/n4ts/4tzUP8vLTBlbniF/cazoP+oloT/ZHCAcW1nZNyUiYb/Z2Jf/0xLS5BbVTOvT0s0/1tUOP9IQir3AJqQg/94dW//t6uey6OXgW+5nm3/sZNl/52JbPVDRlJganiO9KGbmP+IgXz/X2lxbGVcVtN3bWj/cGtl/19cWYo4NCGofn53/2xtZ/9IQi/tAK+gjuWqoJLmy72nsHl+gGChjnHprZNq3oSGgtReZm1UeX+P032Dit+AhYjlgoWHXVxVT7d1cGnke3Rs7HFqY3hBQjaRmpqV7cvIw91GQTHOUnqDylKzWK4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/1271a3703b5b224b0557b0d74541dd9c/37523/output_19_0.png\"\n        srcset=\"/static/1271a3703b5b224b0557b0d74541dd9c/e9ff0/output_19_0.png 180w,\n/static/1271a3703b5b224b0557b0d74541dd9c/f21e7/output_19_0.png 360w,\n/static/1271a3703b5b224b0557b0d74541dd9c/37523/output_19_0.png 720w,\n/static/1271a3703b5b224b0557b0d74541dd9c/302a4/output_19_0.png 1080w,\n/static/1271a3703b5b224b0557b0d74541dd9c/ee3fb/output_19_0.png 1144w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>데이터 전처리 끝~</p>\n<hr>\n<h2 id=\"2-모델-생성-및-학습\" style=\"position:relative;\"><a href=\"#2-%EB%AA%A8%EB%8D%B8-%EC%83%9D%EC%84%B1-%EB%B0%8F-%ED%95%99%EC%8A%B5\" aria-label=\"2 모델 생성 및 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2 모델 생성 및 학습</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> Flatten<span class=\"token punctuation\">,</span> MaxPooling2D\n</code></pre></div>\n<p>models 에는 모델 자체를 구축하기 위한 함수가 있고  그 안의 Sequential 함수 안에 여러가지 layer 들이 들어갈 수 있다.<br>\nlayers 에는 모델의 구성 요소인 여러가지 종류의 layer(층) 함수들을 가지고 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">160</span><span class=\"token punctuation\">,</span> <span class=\"token number\">160</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    MaxPooling2D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    MaxPooling2D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    MaxPooling2D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>딥러닝에서는 <strong>레이어</strong> 라는 개념을 자세하게 공부한다.<br>\n여기서는</p>\n<ul>\n<li>Conv2D</li>\n<li>MaxPooling2D</li>\n<li>Flatten</li>\n<li>Dense</li>\n</ul>\n<p>라는 네 레이어를 사용했다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 160, 160, 16)      448       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 80, 80, 16)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 80, 80, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 40, 40, 32)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 40, 40, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 20, 20, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 25600)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               13107712  \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 1026      \n=================================================================\nTotal params: 13,132,322\nTrainable params: 13,132,322\nNon-trainable params: 0\n_________________________________________________________________</code></pre></div>\n<p>모델을 만들었으니 학습시켜보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">learning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.0001</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>optimizers<span class=\"token punctuation\">.</span>RMSprop<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n              loss<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>losses<span class=\"token punctuation\">.</span>sparse_categorical_crossentropy<span class=\"token punctuation\">,</span>\n              metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<p>학습시키기 위해서는 Optimer , Loss, Metrics 가 필요하다.</p>\n<p>opt : 학습을 어떤 방식으로 시킬 것인지<br>\nloss : 모델이 학습해나가야 할 방향 (이 경우는 확률분포)<br>\nmetrics : [accuracy, precision, recall]</p>\n<p>아직은 실행하기 전이다. 지금은 사전 작업만 거친 상태</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">BATCH_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">32</span>\nSHUFFLE_BUFFER_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\n\ntrain_batches <span class=\"token operator\">=</span> train<span class=\"token punctuation\">.</span>shuffle<span class=\"token punctuation\">(</span>SHUFFLE_BUFFER_SIZE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">)</span>\nvalidation_batches <span class=\"token operator\">=</span> validation<span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">)</span>\ntest_batches <span class=\"token operator\">=</span> test<span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>BATCH_SIZE<span class=\"token punctuation\">)</span></code></pre></div>\n<p>이렇게 train 데이터를 통으로 넣지 않고 32개씩 끊어 넣는 이유는\n랜덤한 32개의 사진들을 묶어 여러개의 Decision Tree 를 만들어 ansamble 하기 위함</p>\n<p>train_batches 의 데이터를 확인해보자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> image_batch<span class=\"token punctuation\">,</span> label_batch <span class=\"token keyword\">in</span> train_batches<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\nimage_batch<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> label_batch<span class=\"token punctuation\">.</span>shape</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">(TensorShape([32, 160, 160, 3]), TensorShape([32]))</code></pre></div>\n<p>모델 학습 전에 초기 모델의 성능을 테스트해볼까? validation data 로 모델을 평가해보자<br>\n20번의 예측을 해보고 loss 와 accuracy를 구해보자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">validation_steps <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\nloss0<span class=\"token punctuation\">,</span> accuracy0 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>evaluate<span class=\"token punctuation\">(</span>validation_batches<span class=\"token punctuation\">,</span> steps<span class=\"token operator\">=</span>validation_steps<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"initial loss: {:.2f}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>loss0<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"initial accuracy: {:.2f}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>accuracy0<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">10/20 [==============>...............] - ETA: 0s - loss: 0.6924 - accuracy: 0.5156\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\n\n\n20/20 [==============================] - 3s 32ms/step - loss: 0.6919 - accuracy: 0.5172\ninitial loss: 0.69\ninitial accuracy: 0.52\n\n\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9</code></pre></div>\n<p>보면 아무것도 하기 전에 모델 evaluate 는 걍 뭐 아무것도 모른다.\n이제 이걸 학습시켜보자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">EPOCHS <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nhistory <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_batches<span class=\"token punctuation\">,</span>\n                    epochs<span class=\"token operator\">=</span>EPOCHS<span class=\"token punctuation\">,</span>\n                    validation_data<span class=\"token operator\">=</span>validation_batches<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Epoch 1/10\n170/582 [=======>......................] - ETA: 18s - loss: 0.5444 - accuracy: 0.7250\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n202/582 [=========>....................] - ETA: 17s - loss: 0.5408 - accuracy: 0.7274\n\nWarning: unknown JFIF revision number 0.00\n\n\n212/582 [=========>....................] - ETA: 16s - loss: 0.5427 - accuracy: 0.7258\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n284/582 [=============>................] - ETA: 13s - loss: 0.5408 - accuracy: 0.7272\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n492/582 [========================>.....] - ETA: 3s - loss: 0.5280 - accuracy: 0.7363\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n504/582 [========================>.....] - ETA: 3s - loss: 0.5277 - accuracy: 0.7360\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.5271 - accuracy: 0.7372\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.5269 - accuracy: 0.7374\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n550/582 [===========================>..] - ETA: 1s - loss: 0.5271 - accuracy: 0.7372\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.7389\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 29s 48ms/step - loss: 0.5259 - accuracy: 0.7389 - val_loss: 0.5102 - val_accuracy: 0.7502\nEpoch 2/10\n169/582 [=======>......................] - ETA: 18s - loss: 0.4561 - accuracy: 0.7898\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n201/582 [=========>....................] - ETA: 16s - loss: 0.4553 - accuracy: 0.7886\n\nWarning: unknown JFIF revision number 0.00\n\n\n213/582 [=========>....................] - ETA: 16s - loss: 0.4548 - accuracy: 0.7893\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n283/582 [=============>................] - ETA: 13s - loss: 0.4483 - accuracy: 0.7911\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n492/582 [========================>.....] - ETA: 3s - loss: 0.4446 - accuracy: 0.7943\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n506/582 [=========================>....] - ETA: 3s - loss: 0.4450 - accuracy: 0.7947\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.4445 - accuracy: 0.7951\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.4437 - accuracy: 0.7953\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n548/582 [===========================>..] - ETA: 1s - loss: 0.4433 - accuracy: 0.7952\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n580/582 [============================>.] - ETA: 0s - loss: 0.4425 - accuracy: 0.7956\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.4422 - accuracy: 0.7958 - val_loss: 0.5503 - val_accuracy: 0.7386\nEpoch 3/10\n169/582 [=======>......................] - ETA: 18s - loss: 0.3937 - accuracy: 0.8229\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n201/582 [=========>....................] - ETA: 16s - loss: 0.3927 - accuracy: 0.8232\n\nWarning: unknown JFIF revision number 0.00\n\n\n213/582 [=========>....................] - ETA: 16s - loss: 0.3927 - accuracy: 0.8234\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n283/582 [=============>................] - ETA: 13s - loss: 0.3870 - accuracy: 0.8266\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n492/582 [========================>.....] - ETA: 3s - loss: 0.3836 - accuracy: 0.8281\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n506/582 [=========================>....] - ETA: 3s - loss: 0.3843 - accuracy: 0.8278\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.3836 - accuracy: 0.8281\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n543/582 [==========================>...] - ETA: 1s - loss: 0.3820 - accuracy: 0.8288\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n549/582 [===========================>..] - ETA: 1s - loss: 0.3814 - accuracy: 0.8292\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n581/582 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.8289\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.3799 - accuracy: 0.8290 - val_loss: 0.4954 - val_accuracy: 0.7674\nEpoch 4/10\n169/582 [=======>......................] - ETA: 18s - loss: 0.3449 - accuracy: 0.8460\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n201/582 [=========>....................] - ETA: 16s - loss: 0.3404 - accuracy: 0.8469\n\nWarning: unknown JFIF revision number 0.00\n\n\n213/582 [=========>....................] - ETA: 16s - loss: 0.3395 - accuracy: 0.8480\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n283/582 [=============>................] - ETA: 12s - loss: 0.3357 - accuracy: 0.8520\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n492/582 [========================>.....] - ETA: 3s - loss: 0.3270 - accuracy: 0.8581\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n506/582 [=========================>....] - ETA: 3s - loss: 0.3273 - accuracy: 0.8581\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.3260 - accuracy: 0.8586\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.3256 - accuracy: 0.8590\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n550/582 [===========================>..] - ETA: 1s - loss: 0.3252 - accuracy: 0.8593\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n580/582 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8602\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.3244 - accuracy: 0.8602 - val_loss: 0.4858 - val_accuracy: 0.7825\nEpoch 5/10\n170/582 [=======>......................] - ETA: 18s - loss: 0.2913 - accuracy: 0.8765\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n202/582 [=========>....................] - ETA: 16s - loss: 0.2866 - accuracy: 0.8790\n\nWarning: unknown JFIF revision number 0.00\n\n\n212/582 [=========>....................] - ETA: 16s - loss: 0.2868 - accuracy: 0.8791\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n284/582 [=============>................] - ETA: 12s - loss: 0.2804 - accuracy: 0.8825\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n493/582 [========================>.....] - ETA: 3s - loss: 0.2756 - accuracy: 0.8869\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n505/582 [=========================>....] - ETA: 3s - loss: 0.2740 - accuracy: 0.8876\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n515/582 [=========================>....] - ETA: 2s - loss: 0.2733 - accuracy: 0.8877\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n543/582 [==========================>...] - ETA: 1s - loss: 0.2732 - accuracy: 0.8886\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n549/582 [===========================>..] - ETA: 1s - loss: 0.2727 - accuracy: 0.8889\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n581/582 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.8886\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 46ms/step - loss: 0.2721 - accuracy: 0.8885 - val_loss: 0.4933 - val_accuracy: 0.7919\nEpoch 6/10\n169/582 [=======>......................] - ETA: 18s - loss: 0.2373 - accuracy: 0.9051\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n201/582 [=========>....................] - ETA: 16s - loss: 0.2335 - accuracy: 0.9083\n\nWarning: unknown JFIF revision number 0.00\n\n\n211/582 [=========>....................] - ETA: 16s - loss: 0.2331 - accuracy: 0.9083\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n283/582 [=============>................] - ETA: 13s - loss: 0.2268 - accuracy: 0.9110\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n493/582 [========================>.....] - ETA: 3s - loss: 0.2216 - accuracy: 0.9123\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n505/582 [=========================>....] - ETA: 3s - loss: 0.2209 - accuracy: 0.9127\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.2196 - accuracy: 0.9133\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.2178 - accuracy: 0.9147\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n548/582 [===========================>..] - ETA: 1s - loss: 0.2171 - accuracy: 0.9150\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n580/582 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9152\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 29s 47ms/step - loss: 0.2162 - accuracy: 0.9153 - val_loss: 0.6168 - val_accuracy: 0.7601\nEpoch 7/10\n169/582 [=======>......................] - ETA: 18s - loss: 0.1858 - accuracy: 0.9310\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n201/582 [=========>....................] - ETA: 16s - loss: 0.1836 - accuracy: 0.9331\n\nWarning: unknown JFIF revision number 0.00\n\n\n213/582 [=========>....................] - ETA: 16s - loss: 0.1820 - accuracy: 0.9341\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n283/582 [=============>................] - ETA: 13s - loss: 0.1783 - accuracy: 0.9361\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n493/582 [========================>.....] - ETA: 3s - loss: 0.1724 - accuracy: 0.9365\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n505/582 [=========================>....] - ETA: 3s - loss: 0.1717 - accuracy: 0.9369\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n515/582 [=========================>....] - ETA: 2s - loss: 0.1709 - accuracy: 0.9374\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n543/582 [==========================>...] - ETA: 1s - loss: 0.1699 - accuracy: 0.9378\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n549/582 [===========================>..] - ETA: 1s - loss: 0.1698 - accuracy: 0.9378\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n581/582 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9382\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.1677 - accuracy: 0.9382 - val_loss: 0.6220 - val_accuracy: 0.7627\nEpoch 8/10\n170/582 [=======>......................] - ETA: 18s - loss: 0.1322 - accuracy: 0.9537\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n202/582 [=========>....................] - ETA: 16s - loss: 0.1317 - accuracy: 0.9547\n\nWarning: unknown JFIF revision number 0.00\n\n\n212/582 [=========>....................] - ETA: 16s - loss: 0.1293 - accuracy: 0.9558\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n284/582 [=============>................] - ETA: 13s - loss: 0.1267 - accuracy: 0.9558\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n493/582 [========================>.....] - ETA: 3s - loss: 0.1241 - accuracy: 0.9564\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n505/582 [=========================>....] - ETA: 3s - loss: 0.1237 - accuracy: 0.9566\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n515/582 [=========================>....] - ETA: 2s - loss: 0.1235 - accuracy: 0.9567\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.1223 - accuracy: 0.9574\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n550/582 [===========================>..] - ETA: 1s - loss: 0.1219 - accuracy: 0.9577\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n580/582 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9580\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.1209 - accuracy: 0.9581 - val_loss: 0.6818 - val_accuracy: 0.7623\nEpoch 9/10\n170/582 [=======>......................] - ETA: 18s - loss: 0.0973 - accuracy: 0.9688\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n202/582 [=========>....................] - ETA: 16s - loss: 0.0932 - accuracy: 0.9711\n\nWarning: unknown JFIF revision number 0.00\n\n\n212/582 [=========>....................] - ETA: 16s - loss: 0.0942 - accuracy: 0.9704\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n284/582 [=============>................] - ETA: 13s - loss: 0.0915 - accuracy: 0.9712\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n492/582 [========================>.....] - ETA: 3s - loss: 0.0893 - accuracy: 0.9715\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n506/582 [=========================>....] - ETA: 3s - loss: 0.0888 - accuracy: 0.9719\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n516/582 [=========================>....] - ETA: 2s - loss: 0.0883 - accuracy: 0.9721\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n542/582 [==========================>...] - ETA: 1s - loss: 0.0880 - accuracy: 0.9722\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n550/582 [===========================>..] - ETA: 1s - loss: 0.0874 - accuracy: 0.9725\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n580/582 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9725\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 28s 47ms/step - loss: 0.0874 - accuracy: 0.9725 - val_loss: 0.6500 - val_accuracy: 0.7863\nEpoch 10/10\n170/582 [=======>......................] - ETA: 18s - loss: 0.0692 - accuracy: 0.9783\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\n\n\n202/582 [=========>....................] - ETA: 17s - loss: 0.0681 - accuracy: 0.9785\n\nWarning: unknown JFIF revision number 0.00\n\n\n212/582 [=========>....................] - ETA: 16s - loss: 0.0674 - accuracy: 0.9788\n\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\n\n\n284/582 [=============>................] - ETA: 13s - loss: 0.0648 - accuracy: 0.9798\n\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n\n\n493/582 [========================>.....] - ETA: 3s - loss: 0.0626 - accuracy: 0.9816\n\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n\n\n505/582 [=========================>....] - ETA: 3s - loss: 0.0624 - accuracy: 0.9817\n\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\n\n\n515/582 [=========================>....] - ETA: 2s - loss: 0.0621 - accuracy: 0.9820\n\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\n\n\n543/582 [==========================>...] - ETA: 1s - loss: 0.0610 - accuracy: 0.9826\n\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n\n\n549/582 [===========================>..] - ETA: 1s - loss: 0.0615 - accuracy: 0.9825\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n581/582 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9827\n\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\n582/582 [==============================] - 29s 47ms/step - loss: 0.0613 - accuracy: 0.9827 - val_loss: 0.6904 - val_accuracy: 0.7889</code></pre></div>\n<p>다음은 학습시킨 모델에 대한 그래프 보고이다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> image_batch<span class=\"token punctuation\">,</span> label_batch <span class=\"token keyword\">in</span> test_batches<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    images <span class=\"token operator\">=</span> image_batch\n    labels <span class=\"token operator\">=</span> label_batch\n    predictions <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>image_batch<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">pass</span>\n\npredictions</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">array([[9.9999702e-01, 2.9417115e-06],\n       [8.3842850e-01, 1.6157144e-01],\n       [7.4218982e-01, 2.5781012e-01],\n       [9.9970847e-01, 2.9155653e-04],\n       [9.9640131e-01, 3.5986665e-03],\n       [7.8427315e-02, 9.2157269e-01],\n       [1.8649189e-02, 9.8135078e-01],\n       [9.1933328e-01, 8.0666728e-02],\n       [9.4125561e-02, 9.0587437e-01],\n       [7.8091651e-02, 9.2190832e-01],\n       [1.1057696e-01, 8.8942307e-01],\n       [9.2630666e-01, 7.3693395e-02],\n       [9.9996006e-01, 3.9985713e-05],\n       [4.6824862e-05, 9.9995315e-01],\n       [9.9207205e-01, 7.9279514e-03],\n       [9.9890709e-01, 1.0929310e-03],\n       [3.1051392e-02, 9.6894866e-01],\n       [5.8752208e-09, 1.0000000e+00],\n       [8.3172768e-01, 1.6827232e-01],\n       [9.9952376e-01, 4.7630019e-04],\n       [7.9531687e-01, 2.0468311e-01],\n       [9.9072027e-01, 9.2797335e-03],\n       [9.9999344e-01, 6.5057743e-06],\n       [9.1565454e-01, 8.4345400e-02],\n       [9.9570221e-01, 4.2977203e-03],\n       [1.0600092e-02, 9.8939985e-01],\n       [9.9997663e-01, 2.3377719e-05],\n       [1.0107538e-01, 8.9892459e-01],\n       [9.9934202e-01, 6.5792818e-04],\n       [9.9927968e-01, 7.2028313e-04],\n       [9.9999619e-01, 3.7975171e-06],\n       [2.8957015e-01, 7.1042985e-01]], dtype=float32)</code></pre></div>\n<p>이것이 바로 우리의 정확도이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\npredictions <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>predictions<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\npredictions</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">array([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 1, 0, 0, 0, 1])</code></pre></div>\n<p>이제 32장의 image 와 32개의 label , 32개의 prediction 을 얻었다\n최종 확인을 해보자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">,</span> prediction<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>images<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> predictions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">,</span> idx<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    image <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>image <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">2</span>\n    plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span>\n    correct <span class=\"token operator\">=</span> label <span class=\"token operator\">==</span> prediction\n    title <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f'real: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>label<span class=\"token punctuation\">}</span></span><span class=\"token string\"> / pred :</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>prediction<span class=\"token punctuation\">}</span></span><span class=\"token string\">\\n </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>correct<span class=\"token punctuation\">}</span></span><span class=\"token string\">!'</span></span>\n    <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> correct<span class=\"token punctuation\">:</span>\n        plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span>title<span class=\"token punctuation\">,</span> fontdict<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">'color'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'red'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span>title<span class=\"token punctuation\">,</span> fontdict<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">'color'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'blue'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n    plt<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">'off'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAAD10lEQVQozwHMAzP8AHt+llZbXIBmh4aHNreNhmuldGpKSx8VSGoxKmxJOzQ3YmGAZYF9j1FwbX5CbWyGbipGQzheU1NhdG1wWFVfaTtmZ3xxZF1iOL+HhVq/dmlmAG9va/SAfnn/a2Zeq4Z5Zf+di3nUhnZo0IZ5bf9oY1yqiYJ4/6ihl+Wdnp6/mZCM/2hRSapweWH/aXp29lxjW69zcmL/cGVbqpx4VPiZcUv/AG1iVpyKhH+vLyQXbGZON7dkVECHv7mqhcnCu7iRk5FsdXFzsH94cpKijH56ZFNav1AgHWxzZk6pWWZonV1oa29oZmPGbmVSbG1PJp9GOyO2AM6tpk/WsK9co5OFMWgtH2FqMxlDfXybQUJDb2BTVFIymZmxXMbH3kkTDBQ8JChGZBwpNTOHgjxXtZE5T0E3RjUyLE9na219M////1GOjJ5dALKhm/Kqkor/q6OaqE0+Lv9eVUjSpJmXzm9oav96enmoh4eA/7e3s+OFhX+9nJmS/3NvV6iUiRH/o3kY80k/N60fIBz/UlRQqKCioPUyNzn/AOfUz6TCo5y21L2ucY15Zr+ek4eOm4aGi19UXsGLhItxY2FiuEhEQZqgoKSA4uLqyOXoxnGBcgCxZ1QBpXByXnVVVU3PNC8ucSQkJqcvMDq/AEgrKUhaMzNVZW1eLLi2yViFhYk9el5TO3FSWVm0sKktkKTHVHaInEMoFhg2NCY+XFFDQC5fWHhQlpGqSQ4QHzEAAAxfHB8uL4WGoEtaXHVVADIrI+41LSX/WF5MpZKSff9vcV3OZ1E2ykExGv+Cc16leHhq/5eflN9ORDW6Vk9B/2JcT6WQhn//hnZp7319d6lraWj/Ky0spF9jW/FtcWf/AD80Kq1MQTfAT1M9eGlqS8lZVz2Wc1tFk11MOsunjXF4gHpswpWUh6J6dWaHcnFp03h5d3jb0sm6kG1ercrKw3va4ePaSEtReFZYUK9oamHJAEA/WURdXolRZGZfKqN+aFW3qJI6VFhpOMS63FXLzdQreoGsUKqz0D8yGBM0MxwsWEtDRixXQ1tNOiA2Rmtmbi6XkLRar7LOLK5/b0ilblZRADEzMuVpbG//f3hqn76yif+dk2/HMC8pw1ROSv+trK6fbGxr/4iMjtdIOy2zTjsm/zkrGp9ubWr4TlBW535pXaS7q5H/wK+fn5iMe+m5r57/AH5sW8iBd2nfXlU+i5R9VulzWjeue3h3q2VfYeuVlqCLoKiw4Jqgp7xodnqdXmhm9CQjIYujn5XYiISDyWckMY+IWkz8lGlai4iEb8uklXrpdIjI2btfkaMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/ec1ce97527379951f010a8c71cac03f3/37523/output_42_0.png\"\n        srcset=\"/static/ec1ce97527379951f010a8c71cac03f3/e9ff0/output_42_0.png 180w,\n/static/ec1ce97527379951f010a8c71cac03f3/f21e7/output_42_0.png 360w,\n/static/ec1ce97527379951f010a8c71cac03f3/37523/output_42_0.png 720w,\n/static/ec1ce97527379951f010a8c71cac03f3/302a4/output_42_0.png 1080w,\n/static/ec1ce97527379951f010a8c71cac03f3/07a9c/output_42_0.png 1440w,\n/static/ec1ce97527379951f010a8c71cac03f3/fbae3/output_42_0.png 2260w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">count <span class=\"token operator\">=</span> <span class=\"token number\">0</span>   <span class=\"token comment\"># 정답을 맞춘 개수</span>\n<span class=\"token keyword\">for</span> image<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">,</span> prediction <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>images<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> predictions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    correct <span class=\"token operator\">=</span> label <span class=\"token operator\">==</span> prediction\n    <span class=\"token keyword\">if</span> correct<span class=\"token punctuation\">:</span>\n        count <span class=\"token operator\">=</span> count <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>count <span class=\"token operator\">/</span> <span class=\"token number\">32</span> <span class=\"token operator\">*</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">71.875</code></pre></div>\n<p>중간에 들어가있는 딥러닝 모델이 단순한 구조이기 때문에<br>\n좋은 결과가 나오고 있지는 않다.</p>\n<p>다만 중간의 모델구조를 변경한다면 더 좋은 결과를<br>\n끌어낼 수 있을 것이다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EA%B0%95%EC%95%84%EC%A7%80--%EA%B3%A0%EC%96%91%EC%9D%B4-%EB%B6%84%EB%A5%98-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-%EC%A0%9C%EC%9E%91-\">강아지 &#x26; 고양이 분류 프로그램 제작 !</a></p>\n<ul>\n<li><a href=\"#content\">Content</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC\">1. 데이터 전처리</a></p>\n</li>\n<li>\n<p><a href=\"#2-%EB%AA%A8%EB%8D%B8-%EC%83%9D%EC%84%B1-%EB%B0%8F-%ED%95%99%EC%8A%B5\">2 모델 생성 및 학습</a></p>\n</li>\n</ul>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"></code></pre></div>","frontmatter":{"date":"April 21, 2022","title":"인공지능 기초 사진 분류 프로그램 맛보기","categories":"STUDY","author":"하성민","emoji":"😁"},"fields":{"slug":"/DML_AI1/"}},"site":{"siteMetadata":{"siteUrl":"https://xman227.github.io","comments":{"utterances":{"repo":"xman227/blog_comments"}}}}},"pageContext":{"slug":"/NLP_4/","nextSlug":"/DML_AI2/","prevSlug":"/DML_AI1/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}