---
emoji: ğŸ§™â€â™‚ï¸
title: íŒŒì´ì¬ìœ¼ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì œì‘í•˜ê¸°
date: '2022-02-19 23:00:00'
author: í•˜ì„±ë¯¼
tags: blog gatsby theme ê°œì¸ ë¹„ í…Œë§ˆ
categories: STUDY
---



## 1. ğŸŒ  Depp Learning ëª¨ë¸ ìƒì„± ë°©ë²• ì„¸ê°€ì§€


Tensor flow V2 ë²„ì „ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‘ì„± ë°©ë²•ì—ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆë‹¤.

- Sequential
- Functional : sequential ì˜ ì¼ë°˜í™”ëœ ê°œë…
- Model Subclassing : í´ë˜ìŠ¤ë¡œ êµ¬í˜„ëœ ê¸°ì¡´ ëª¨ë¸ì„ ìƒì†ë°›ì•„ ìê¸° ëª¨ë¸ ë§Œë“¤ê¸°

ìˆœì°¨ì ìœ¼ë¡œ ì–´ë–¤ ì°¨ì´ê°€ ìˆê³ ,  
ì–´ë–¤ ì‹ìœ¼ë¡œ ì œì‘í•˜ëŠ”ì§€ ì•Œì•„ê°€ë³´ì.

---



### 1. Sequential Model  


```python
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
model.add(__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__)
model.add(__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__)
model.add(__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__)

model.fit(x, y, epochs=20, batch_size=16)
```

epochs : ëª¨ë¸ë¡œ ë°ì´í„°ë¥¼ í•™ìŠµí•  íšŸìˆ˜  
batch_size : ë°ì´í„°ë¥¼ ì†Œë¶„í•´ ë„£ì„ ì–‘

model = keras.Sequential() ì„ í™œìš©í•˜ë©´  
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìŒ“ì•„ ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.

ì…ë ¥ë¶€í„° ì¶œë ¥ê¹Œì§€ ìˆœì°¨ì (ì‹œí€€ì…œ) ìœ¼ë¡œ add í•˜ë©´ ëœë‹¤.

but, ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ ì—¬ëŸ¬ê°œì¸ ê²½ìš°ì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.  
(ë°˜ë“œì‹œ ì…ë ¥ 1ê°œ ì¶œë ¥ 1ê°€ì§€ ì—¬ì•¼ í•¨)

### 2. Functional API


```python
import tensorflow as tf
from tensorflow import keras

inputs = keras.Input(shape=(__ì›í•˜ëŠ” ì…ë ¥ê°’ ëª¨ì–‘__))
x = keras.layers.__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__(ê´€ë ¨ íŒŒë¼ë¯¸í„°)(input)
x = keras.layers.__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__(ê´€ë ¨ íŒŒë¼ë¯¸í„°)(x)
outputs = keras.layers.__ë„£ê³ ì‹¶ì€ ë ˆì´ì–´__(ê´€ë ¨ íŒŒë¼ë¯¸í„°)(x)

model = keras.Model(inputs=inputs, outputs=outputs)
model.fit(x,y, epochs=10, batch_size=32)
```

model ì— ketas.Model ì´ ë“¤ì–´ê°„ë‹¤.  
ì´ê²ƒì€ ìš°ë¦¬ê°€ danse ë‚˜ Flatten ê°™ì€ ì§œì—¬ì ¸ ìˆëŠ” ì‹ ê²½ë§ì„ ì“°ëŠ” ê²Œ ì•„ë‹ˆë¼  
ì§ì ‘ input ê³¼ output ì„ êµ¬ì„±í•œë‹¤.

ë•Œë¬¸ì— ì…ë ¥ê³¼ ì¶œë ¥ê°’ì´ ììœ ë¡­ë‹¤

---
ë”¥ ëŸ¬ë‹ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë ˆì´ì–´ì˜ DAG Directed Acyclic graph ì´ë‹¤.  
ë ˆì´ì–´ì˜ ê·¸ë˜í”„ë¥¼ bulid í•œë‹¤ëŠ” ëœ»ì´ë‹¤.


### 3. Subclassing


```python
import tensorflow as tf
from tensorflow import keras

class CustomModel(keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__()
        self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__()
        self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__()
    
    def call(self, x):
        x = self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__(x)
        x = self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__(x)
        x = self.__ì •ì˜í•˜ê³ ì í•˜ëŠ” ë ˆì´ì–´__(x)
        
        return x
    
model = CustomModel()
model.fit(x,y, epochs=10, batch_size=32)
```

ì œì¼ ììœ ë¡œìš´ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•œ subclassing  
ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ì ‘ êµ¬í˜„í•´ë³´ì


```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
```


```python
# ë°ì´í„° êµ¬ì„±ë¶€ë¶„
mnist = keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train=x_train[...,np.newaxis]
x_test=x_test[...,np.newaxis]

print(len(x_train), len(x_test))
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11493376/11490434 [==============================] - 0s 0us/step
    11501568/11490434 [==============================] - 0s 0us/step
    60000 10000


## 2. ì‹¤ì œ êµ¬í˜„
### 1. Sequential model


```python
# Sequential Modelì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""
Spec:
1. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. 64ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
3. Flatten ë ˆì´ì–´
4. 128ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
5. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
"""


model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32,3,activation='relu'),
    tf.keras.layers.Conv2D(32,3,activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10,activation='softmax')
                           ])


"""
model.add.Conv2D(32, 3, activation="relu")
model.add.Conv2D(64, 3, activation="relu")
model.add.Flatten()
model.add.Dense(128, activation='relu')
model.add.Dense(10, activation='softmax')
"""
```




    '\nmodel.add.Conv2D(32, 3, activation="relu")\nmodel.add.Conv2D(64, 3, activation="relu")\nmodel.add.Flatten()\nmodel.add.Dense(128, activation=\'relu\')\nmodel.add.Dense(10, activation=\'softmax\')\n'




```python
# ëª¨ë¸ í•™ìŠµ ì„¤ì •

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)
```

    Epoch 1/5
    1875/1875 [==============================] - 31s 3ms/step - loss: 0.1118 - accuracy: 0.9654
    Epoch 2/5
    1875/1875 [==============================] - 5s 3ms/step - loss: 0.0368 - accuracy: 0.9882
    Epoch 3/5
    1875/1875 [==============================] - 5s 3ms/step - loss: 0.0211 - accuracy: 0.9933
    Epoch 4/5
    1875/1875 [==============================] - 5s 3ms/step - loss: 0.0131 - accuracy: 0.9956
    Epoch 5/5
    1875/1875 [==============================] - 5s 3ms/step - loss: 0.0100 - accuracy: 0.9968
    313/313 - 1s - loss: 0.0516 - accuracy: 0.9878





    [0.051644984632730484, 0.9878000020980835]



### 2. Functional API


```python
mnist = keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train=x_train[...,np.newaxis]
x_test=x_test[...,np.newaxis]

print(len(x_train), len(x_test))
```

    60000 10000



```python
"""
Spec:
0. (28X28X1) ì°¨ì›ìœ¼ë¡œ ì •ì˜ëœ Input
1. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. 64ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
3. Flatten ë ˆì´ì–´
4. 128ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
5. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
"""

inputs = keras.Input(shape=(28,28,1))
x = keras.layers.Conv2D(32,3,activation='relu')(inputs)
x = keras.layers.Conv2D(64,3,activation='relu')(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(128, activation='relu')(x)
outputs = keras.layers.Dense(10, activation='softmax')(x)

model = keras.Model(inputs=inputs, outputs=outputs)


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)
```

    Epoch 1/5
    1875/1875 [==============================] - 7s 3ms/step - loss: 0.1039 - accuracy: 0.9679
    Epoch 2/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0339 - accuracy: 0.9895
    Epoch 3/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0204 - accuracy: 0.9931
    Epoch 4/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0130 - accuracy: 0.9959
    Epoch 5/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0093 - accuracy: 0.9972
    313/313 - 1s - loss: 0.0522 - accuracy: 0.9868





    [0.05223735421895981, 0.9868000149726868]



### 3. Subclassing API

keras.models ë¥¼ ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒ

1. __init__ ë©”ì„œë“œì— ë ˆì´ì–´ ì„ ì–¸
2. call() ë©”ì„œë“œì— forward propagation ë°©ì‹ ì²´ê³„ êµ¬í˜„


```python
"""
Spec:
0. keras.Model ì„ ìƒì†ë°›ì•˜ìœ¼ë©°, __init__()ì™€ call() ë©”ì„œë“œë¥¼ ê°€ì§„ ëª¨ë¸ í´ë˜ìŠ¤
1. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. 64ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
3. Flatten ë ˆì´ì–´
4. 128ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
5. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
6. callì˜ ì…ë ¥ê°’ì´ ëª¨ë¸ì˜ Input, callì˜ ë¦¬í„´ê°’ì´ ëª¨ë¸ì˜ Output
"""


class kerasModel(keras.Model):
    def __init__(self):
        super().__init__()
        
        self.Conv2D32 = keras.layers.Conv2D(32, 3, activation='relu')
        self.Conv2D64 = keras.layers.Conv2D(64, 3, activation='relu')
        self.Flatten = keras.layers.Flatten()
        self.Dense128 = keras.layers.Dense(128, activation='relu')
        self.Dense = keras.layers.Dense(10, activation='softmax')
    
    def call(self, x):
        x = self.Conv2D32(x)
        x = self.Conv2D64(x)
        x = self.Flatten(x)
        x = self.Dense128(x)
        output = self.Dense(x)
        
        return output
    
model = kerasModel()


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)

```

    Epoch 1/5
    1875/1875 [==============================] - 7s 3ms/step - loss: 0.1011 - accuracy: 0.9689
    Epoch 2/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0339 - accuracy: 0.9893
    Epoch 3/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0184 - accuracy: 0.9938
    Epoch 4/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0136 - accuracy: 0.9954
    Epoch 5/5
    1875/1875 [==============================] - 6s 3ms/step - loss: 0.0078 - accuracy: 0.9973
    313/313 - 1s - loss: 0.0581 - accuracy: 0.9853





    [0.05812956392765045, 0.9853000044822693]



ì´ê±´ ê²°ë¡ ì ìœ¼ë¡œëŠ” model ì„ ê°–ë‹¤ ì“°ëŠ”ê±°ë¼ì„œ  
input ì„ ë”°ë¡œ ì„¤ì •ì•ˆí•˜ê³   
ê·¸ë¦¬ê³  call ë©”ì„œë“œë„ fit í• ë•Œ ìë™ìœ¼ë¡œ ì¨ì§€ëŠ” ë“¯ í•˜ë‹¤.


---
## 3. CIFAR -100 ë°ì´í„° ì˜ˆì œ ì‚¬ìš©í•˜ê¸°

### 1. Sequential


```python
# ë°ì´í„° êµ¬ì„±ë¶€ë¶„
cifar100 = keras.datasets.cifar100

(x_train, y_train), (x_test, y_test) = cifar100.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print(len(x_train), len(x_test))
```

    50000 10000



```python
# Sequential Modelì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""
Spec:
1. 16ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
3. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
4. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
5. 256ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
6. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16,3,activation='relu'),
    tf.keras.layers.MaxPool2D((2,2)),
    tf.keras.layers.Conv2D(32,3,activation='relu'),
    tf.keras.layers.MaxPool2D((2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256,activation='relu'),
    tf.keras.layers.Dense(100,activation='softmax')
])

```


```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)
```

    Epoch 1/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 2/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 3/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 4/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 5/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    313/313 - 1s - loss: nan - accuracy: 0.0100





    [nan, 0.009999999776482582]



### 2. Functional


```python
"""
Spec:
0. (32X32X3) ì°¨ì›ìœ¼ë¡œ ì •ì˜ëœ Input
1. 16ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
3. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
4. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
5. 256ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
6. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
"""

inputs = tf.keras.Input(shape=(32,32,3))
x = tf.keras.layers.Conv2D(16,3,activation='relu')(inputs)
x = tf.keras.layers.MaxPool2D((2,2))(x)
x = tf.keras.layers.Conv2D(32,3,activation='relu')(x)
x = tf.keras.layers.MaxPool2D((2,2))(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(256,activation='relu')(x)
outputs = tf.keras.layers.Dense(100,activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

model.fit(x_train, y_train, epochs = 5)

model.evaluate(x_test, y_test, verbose =2)
```

    Epoch 1/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 2/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 3/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 4/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 5/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    313/313 - 1s - loss: nan - accuracy: 0.0100





    [nan, 0.009999999776482582]



### 3. Subclass API


```python
"""
Spec:
0. keras.Model ì„ ìƒì†ë°›ì•˜ìœ¼ë©°, __init__()ì™€ call() ë©”ì„œë“œë¥¼ ê°€ì§„ ëª¨ë¸ í´ë˜ìŠ¤
1. 16ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
2. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
3. 32ê°œì˜ ì±„ë„ì„ ê°€ì§€ê³ , ì»¤ë„ì˜ í¬ê¸°ê°€ 3, activation functionì´ reluì¸ Conv2D ë ˆì´ì–´
4. pool_sizeê°€ 2ì¸ MaxPool ë ˆì´ì–´
5. 256ê°œì˜ ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ reluì¸ Fully-Connected Layer(Dense)
6. ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ëŠ” ì•„ì›ƒí’‹ ë…¸ë“œë¥¼ ê°€ì§€ê³ , activation functionì´ softmaxì¸ Fully-Connected Layer(Dense)
7. callì˜ ì…ë ¥ê°’ì´ ëª¨ë¸ì˜ Input, callì˜ ë¦¬í„´ê°’ì´ ëª¨ë¸ì˜ Output
"""


class kerasModel(keras.Model):
    def __init__(self):
        super().__init__()
        
        self.Conv2D16 = keras.layers.Conv2D(16, 3, activation='relu')
        self.Conv2D32 = keras.layers.Conv2D(32, 3, activation='relu')
        self.Maxpool = keras.layers.MaxPool2D((2,2))
        self.Flatten = keras.layers.Flatten()
        self.Dense256 = keras.layers.Dense(256, activation='relu')
        self.Dense = keras.layers.Dense(10, activation='softmax')
    
    def call(self, x):
        x = self.Conv2D16(x)
        x = self.Maxpool(x)
        x = self.Conv2D32(x)
        x = self.Maxpool(x)
        x = self.Flatten(x)
        x = self.Dense256(x)
        output = self.Dense(x)
        
        return output
    
model = kerasModel()


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)

```

    Epoch 1/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 2/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 3/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 4/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    Epoch 5/5
    1563/1563 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0100
    313/313 - 1s - loss: nan - accuracy: 0.0100





    [nan, 0.009999999776482582]



---

ìš°ë¦¬ëŠ” model.compile ì— ìˆì–´ì„œ ê³„ì† ë™ì¼í•œ êµ¬ì„±ì„ ìœ ì§€í–ˆë‹¤.

ì ì‹œ ì¸ê³µì§€ëŠ¥ í•™ìŠµ ê³¼ì •ì„ ë³µê¸°í•´ë³´ì.
1. Forward Propagation
2. Loss ê°’ ê³„ì‚°
3. ì¤‘ê°„ ë ˆì´ì–´ ê°’ ë° loss ë¥¼ í™œìš©í•œ chain rule ë°©ì‹ì˜ Back propagation
4. parameter update
5. repeat

ì´ëŸ° ê³¼ì •ì´ tf v2 ì—ì„œëŠ” fit ì— ë‹¤ ë‹´ê²¨ìˆë‹¤.

tf.gredient tape ëŠ” propagation ë™ì•ˆ ì§„í–‰ë˜ëŠ” ëª¨ë“  ì—°ì‚°ì˜ ì¤‘ê°„ ë ˆì´ì–´ ê°’ì„  
tape ì— ê¸°ë¡í•œë‹¤.

ì´ë¥¼ ì´ìš©í•´ gredient ë¥¼ ê³„ì‚°í•˜ê³  tape ë¥¼ íê¸°í•œë‹¤.

ìš°ë¦¬ëŠ” ì´ tape ê°’ì„ ì´ìš©í•´ ê³ ê¸‰ê¸°ë²•ì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤.


```python
import tensorflow as tf
from tensorflow import keras

# ë°ì´í„° êµ¬ì„±ë¶€ë¶„
cifar100 = keras.datasets.cifar100

(x_train, y_train), (x_test, y_test) = cifar100.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print(len(x_train), len(x_test))

# ëª¨ë¸ êµ¬ì„±ë¶€ë¶„
class CustomModel(keras.Model):
    def __init__(self):
        super().__init__()
        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')
        self.maxpool1 = keras.layers.MaxPool2D((2,2))
        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')
        self.maxpool2 = keras.layers.MaxPool2D((2,2))
        self.flatten = keras.layers.Flatten()
        self.fc1 = keras.layers.Dense(256, activation='relu')
        self.fc2 = keras.layers.Dense(100, activation='softmax')

    def call(self, x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.fc2(x)

        return x

model = CustomModel()
```

    50000 10000


ì§€ê¸ˆê¹Œì§€ëŠ”


```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

ì—ì„œ ì•Œì•„ì„œ loss ë¥¼ í†µí•´ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì—ˆë‹¤.

ì´ë²ˆì—ëŠ” tape.gradient() ë¥¼ í†µí•´ì„œ  
ë§¤ ìŠ¤í… ë§ˆë‹¤ ë°œìƒí•˜ëŠ” gredient ë¥¼ export,  
optimizer.apply_grediens() ë¥¼ í†µí•´ ë°œìƒí•œ gredient ê°€   
model.trainable_variables ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´í„° í•˜ë„ë¡ í•œë‹¤.  


```python
loss_func = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# tf.GradientTape()ë¥¼ í™œìš©í•œ train_step
def train_step(features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features)
        loss = loss_func(labels, predictions)
        gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss
```

ìœ„ì˜ train_step ë©”ì„œë“œê°€ ë°”ë¡œ  
fit ì„ í†µí•´ 1ë²ˆ epoch í•˜ëŠ” ë™ì•ˆì˜ ì•ˆì—ì„œ ì´ë£¨ì–´ì§€ëŠ” steps ë“¤ì˜ ì§„í–‰ë°©ì‹ì„ ë‚˜íƒ€ë‚¸ê²ƒ.  



```python
import time
def train_model(batch_size=32):
    start = time.time()
    for epoch in range(5):
        x_batch = []
        y_batch = []
        for step, (x, y) in enumerate(zip(x_train, y_train)):
            x_batch.append(x)
            y_batch.append(y)
            if step % batch_size == batch_size-1:
                loss = train_step(np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.float32))
                x_batch = []
                y_batch = []
        print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))
    print("It took {} seconds".format(time.time() - start))

train_model()
```

    Epoch 0: last batch loss = 3.2522
    Epoch 1: last batch loss = 2.6932
    Epoch 2: last batch loss = 2.4441
    Epoch 3: last batch loss = 2.3071
    Epoch 4: last batch loss = 2.2136
    It took 85.202951669693 seconds


ìœ„ ë‘ í•¨ìˆ˜ì˜ ì—°ê³„ì‚¬ìš©ì´
fit ë©”ì„œë“œë¥¼ í’€ì–´ ì“´ ê²ƒì´ë‹¤.

ì‰½ê²Œ ë§í•˜ë©´

ë°ì´í„°ë“¤ì„ batch_size ë§Œí¼ ëŠì–´ì„œ  
ê·¸ ëŠì€ ë§Œí¼ì˜ x_train(feature) ë¥¼ ë„£ì–´ ëª¨ë¸ ì‹ ê²½ë§ì— ë„£ê³  ëŒë ¤
ì˜ˆì¸¡í•œ y_train ê°’ì„ ë°˜í™˜í•˜ê³   

ì‹¤ì œ y_trian(label) ê°’ê³¼ ì˜ˆì¸¡í•œ y_trian ê°’ì„ ì†ì‹¤í•¨ìˆ˜(loss) ë¥¼ ëŒë ¤  
ê·¸ ì¶œë ¥ ê°’ìœ¼ë¡œ Back propagation ì„ í†µí•´ ëª¨ë¸ ë‚´ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ update

ì ê·¸ëŸ¼ ì†ì‹¤ê°’ì€ í•œ 1epochs ì— ë°°ì¹˜ì‚¬ì´ì¦ˆ ëŒë¦°ë§Œí¼ ë‚˜ì˜¤ê² ì§€?  
(ìœ„ì˜ ê²½ìš° 32ë°°ì¹˜ì‚¬ì´ì¦ˆ ì´ë¯€ë¡œ ë°ì´í„°ê°€ 100ê°œë©´ í•œ epochs ë‹¹ loss ê°€ 3ê°œ ë‚˜ì˜¤ì§€)
í•˜ì§€ë§Œ ì¶œë ¥ë˜ëŠ” loss ëŠ” ë§¨ ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ loss ì´ë‹¤.

ì´ ê³¼ì •ì„ epoch íšŸìˆ˜ ë§Œí¼ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤.  

ì´ê²Œ fit ë©”ì„œë“œê°€ í•˜ëŠ” ì¼.






---
ì´ê±¸ êµ³ì´ ë„ì§‘ì–´ë‚´ì„œ ë³´ëŠ” ì´ìœ ëŠ” 

ìš°ë¦¬ê°€ ê°•í™”í•™ìŠµ ë˜ëŠ” GAN ì„ ì‹œí–‰í•  ë•Œ 
ì´ ë‚´ë¶€ train_step ì˜ ì¬êµ¬ì„±ì„ í•´ì•¼ í•˜ë¯€ë¡œ!!!!!

---


```python
prediction = model.predict(x_test, batch_size=x_test.shape[0], verbose=1)
temp = sum(np.squeeze(y_test) == np.argmax(prediction, axis=1))
temp/len(y_test)  # Accuracy
```

    1/1 [==============================] - 1s 822ms/step





    0.346



ì¼ë‹¨ epoch í•´ë´£ìœ¼ë‹ˆ ê²°ê³¼ê°’ë„ ì´ë ‡ê²Œ ë„ì¶œì´ ëœë‹¤.

```toc
```