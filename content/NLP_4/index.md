---
emoji: ğŸ˜
title: ì„ë² ë”©ì´ë€
date: '2022-04-21 23:00:00'
author: í•˜ì„±ë¯¼
tags: blog gatsby theme ê°œì¸ ë¹„ í…Œë§ˆ
categories: NLP
---

# <span style='background-color: #fff5b1'>ì„ë² ë”©ì´ë€..?</span>


ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ì ì–´ë„ 1ì°¨ì›ì—ì„œëŠ” ì•ˆëœë‹¤ (ì˜ë¯¸ê°€ ë‹´ê¸°ê¸°ì—” ì‘ë‹¤)

ê·¸ë˜ì„œ ë²¡í„°ì˜ **íŠ¹ì • ì°¨ì›ì„ ì§ì ‘** ë§Œë“¤ì–´ ì˜ë¯¸ë¥¼ ì§ì ‘ mapping í•´ì•¼ í•˜ê³ ,  
ì´ë¥¼ í¬ì†Œ í‘œí˜„ (Sparse Representation) ì´ë¼ê³  í•œë‹¤.

---

ë°˜ë©´ì— ê·¸ëƒ¥ ì°¨ì›ì€ ì¼ì •í•˜ê²Œ 256ì°¨ì› ì´ë ‡ê²Œ ì •í•´ë†“ê³ 

ìœ ì‚¬í•œ ë§¥ë½ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ” ë°©ì‹ì„
ë¶„í¬ ê°€ì„¤ (distribution hypothesis) ì´ë¼ê³  í•œë‹¤. ê·¸ë¦¬ê³  ì´ ê°€ì„¤ì„ í†µí•´ ë¶„ì‚°í‘œí˜„ (distribution Representation) ì´ë¼ê³  í•œë‹¤.

ë§¥ë½ì´ë¼ í•¨ì€ ë‹¨ì–´ ì¢Œìš°ì— í•¨ê»˜ ìœ„ì¹˜í•˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ë¯¸í•œë‹¤.

---

ë¶„ì‚°í‘œí˜„ ì€ í¬ì†Œí‘œí˜„ ê³¼ ë‹¬ë¦¬ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

embedding ë ˆì´ì–´ë¼ëŠ” ê²ƒì€

ì´ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ë ˆì´ì–´!!!!!!!!!!!!!!!!!!!!!!

ìš°ë¦¬ê°€ ë‹¨ì–´ë¥¼ n ê°œ ì“¸ê±°ì•¼~ kì°¨ì›ìœ¼ë¡œ êµ¬í˜„í•´ì¡°~ í•˜ë©´

ì»´í“¨í„°ê°€ n x k í˜•íƒœì˜ ë¶„ì‚°í‘œí˜„ ì‚¬ì „ì„ ë§Œë“ ë‹¤.

ì´ê²Œ weihght ì´ ë˜ëŠ” ê±°ê³  íŒŒë¼ë¯¸í„°ê°€ ëœë‹¤.

---

ì´ ì„ë² ë”©ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´

word2vec , FastText, Glove, ELMo ë“±ì´ ìˆëŠ” ê±°ì„ ë°©ë²•ë“¤ì´

---

## ì„ë² ë”© ë ˆì´ì–´ëŠ” ì»´í“¨í„°ê°€ ì•Œì•„ë¨¹ëŠ” ë‹¨ì–´ì‚¬ì „ì´ë‹¤.


weight ì€ 
1. ë‹¨ì–´ì˜ ê°œìˆ˜
2. ì„ë² ë”© ì‚¬ì´ì¦ˆ  
ë¡œ ì •ì˜ëœë‹¤.

ì„ë² ë”© ë ˆì´ì–´ëŠ” input ë°ì´í„°ë¥¼ ë¶„ì‚° ë°ì´í„°ë¡œ ì—°ê²°í•´ì£¼ë‹ˆ LUT ë£©ì—… í…Œì´ë¸”  
ì´ë¼ê³ ë„ í•œë‹¤.

ê·¸ê²ƒì€ ì›-í•« ì¸ì½”ë”© ì´ë¼ê³ ë„ í•˜ëŠ”ë°

----

ì›í•« ì¸ì½”ë”© ìì²´ëŠ” sparse í‘œí˜„ì´ì§€ë§Œ

embedding ì´ë‘ í•¨ê»˜ ê²°í•©í•˜ì—¬ ì“°ì´ë©´ ìœ ìš©í•˜ë‹¤.

ê° ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ Linear ì—°ì‚° ì„ í†µí•´ ì°¨ì›ê°’ì„ ë§Œë“¤ì–´ë‚¸ë‹¤!!!

ì˜ˆë¥¼ ë“¤ì–´

![png](1.png)

8ì°¨ì›ì˜ ì›í•« ì¸ì½”ë”©ì´ ìˆë‹¤ê³  í•´ë³´ì

![png](2.png)

ì´ëŸ° ê°€ì¤‘ì¹˜ ë¥¼ ê°€ì§„ ë ˆì´ì–´ê°€ ìˆë‹¤ê³  ì¹˜ë©´ 
ì € ìœ„ì˜ 1 0 0 0 0 0 0 0 ì— ê° í•˜ë‚˜ì˜  [ _ _ ] ê°€ ë“¤ì–´ê°€ê²Œ ë˜ê³ ,

ê·¸ ê²°ê³¼ê°’ìœ¼ë¡œ [_ _ ] ì˜ í˜•íƒœ 1 ê°œê°€ ë‚˜ì˜¤ê² ì§€ (ì›í•«ì¸ì½”ë”©ì˜ í–‰ì´ 1ì´ë‹ˆê¹Œ)

ê·¸ëŸ¼ ì›í•« ì¸ì½”ë”©ì´  

![png](3.png)

ì´ë ‡ê²Œ 10 ê°œ ìˆìœ¼ë©´  
10ê°œì— ëŒ€í•œ [_ _ ] ê°’ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ê·¸ëŸ¼ ê·¸ê²ƒì´  
ë°”ë¡œ ìœ ì‚¬ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ê°’ì´ ë  ìˆ˜ ìˆë‹¤.


---
ë‹¤ì‹œ ë§í•´ì„œ ì„ë² ë”© ë ˆì´ì–´ë€

1. ë‹¨ì–´ë“¤ì„ ì›í•« ì¸ì½”ë”© í•œë‹¤.
2. ì„ í˜•ë³€í™˜(ë ˆì´ì–´ ì”Œìš°ê¸°) ë¥¼ í•œë‹¤.
3. ê° ë‹¨ì–´ë“¤ì„ {index : ì„ í˜•ë³€í™˜ê°’ } ìœ¼ë¡œ ì €ì¥

ì„ í•´ì£¼ëŠ” ë ˆì´ì–´ ì¸ ê²ƒ!!!!!!

ë³´ì—¬ì£¼ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


```python
import tensorflow as tf

some_words = tf.constant([[3, 57, 35]])
# 3ë²ˆ ë‹¨ì–´ / 57ë²ˆ ë‹¨ì–´ / 35ë²ˆ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ í•œ ë¬¸ì¥ì…ë‹ˆë‹¤.

print("Embeddingì„ ì§„í–‰í•  ë¬¸ì¥:", some_words.shape)
embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)
# ì´ 64ê°œì˜ ë‹¨ì–´ë¥¼ í¬í•¨í•œ Embedding ë ˆì´ì–´ë¥¼ ì„ ì–¸í•  ê²ƒì´ê³ ,
# ê° ë‹¨ì–´ëŠ” 100ì°¨ì›ìœ¼ë¡œ ë¶„ì‚° í‘œí˜„ í•  ê²ƒì…ë‹ˆë‹¤.

print("Embeddingëœ ë¬¸ì¥:", embedding_layer(some_words).shape)
print("Embedding Layerì˜ Weight í˜•íƒœ:", embedding_layer.weights[0].shape)
```

    Embeddingì„ ì§„í–‰í•  ë¬¸ì¥: (1, 3)
    Embeddingëœ ë¬¸ì¥: (1, 3, 100)
    Embedding Layerì˜ Weight í˜•íƒœ: (64, 100)


#### ê·¼ë° ì„ë² ë”© ë ˆì´ì–´ëŠ” ë¯¸ë¶„ì„ í• ìˆ˜ ì—†ëŠ” ì• ë¼ ì–´ë–¤ ì—°ì‚° ê²°ê³¼ë¥¼ 
#### ì„ë² ë”© ë ˆì´ì–´ì— ì ìœ¼ë©´ ì•ˆëœë‹¤ë„¤

## ê·¸ëŸ° ì„ë² ë”© ë ˆì´ì–´ì™€ í•¨ê»˜ ì“°ëŠ” ë¬¸ì¥ íŠ¹í™” ë ˆì´ì–´

# Recurrent layer

![png](4.png)


- ë”¥ëŸ¬ë‹ì—ì„œ ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” ìˆœì°¨ì ì¸ íŠ¹ì„±ì„ ê¼­ ì§€ë‹Œë‹¤.

ì´ëŸ° ìˆœì°¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë ˆì´ì–´ê°€ recurrent layer

RNN ì€ ë‹¨ í•˜ë‚˜ì˜ Weight ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤.

ë‹¤ìŒì€ RNN ì˜ ì˜ˆì‹œì´ë‹¤.


```python
sentence = "What time is it ?"
dic = {
    "is": 0,
    "it": 1,
    "What": 2,
    "time": 3,
    "?": 4
}

print("RNNì— ì…ë ¥í•  ë¬¸ì¥:", sentence)

sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])

print("Embeddingì„ ìœ„í•´ ë‹¨ì–´ ë§¤í•‘:", sentence_tensor.numpy())
print("ì…ë ¥ ë¬¸ì¥ ë°ì´í„° í˜•íƒœ:", sentence_tensor.shape)

embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)
emb_out = embedding_layer(sentence_tensor)

print("\nEmbedding ê²°ê³¼:", emb_out.shape)
print("Embedding Layerì˜ Weight í˜•íƒœ:", embedding_layer.weights[0].shape)

rnn_seq_layer = \
tf.keras.layers.SimpleRNN(units=64, return_sequences=True, use_bias=False)
rnn_seq_out = rnn_seq_layer(emb_out)

print("\nRNN ê²°ê³¼ (ëª¨ë“  Step Output):", rnn_seq_out.shape)
print("Simple RNN Layerì˜ Weight í˜•íƒœ:", rnn_seq_layer.weights[0].shape)

rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64, use_bias=False)
rnn_fin_out = rnn_fin_layer(emb_out)

print("\nRNN ê²°ê³¼ (ìµœì¢… Step Output):", rnn_fin_out.shape)
print("Simple RNN Layerì˜ Weight í˜•íƒœ:", rnn_fin_layer.weights[0].shape)
```

    RNNì— ì…ë ¥í•  ë¬¸ì¥: What time is it ?
    Embeddingì„ ìœ„í•´ ë‹¨ì–´ ë§¤í•‘: [[2 3 0 1 4]]
    ì…ë ¥ ë¬¸ì¥ ë°ì´í„° í˜•íƒœ: (1, 5)
    
    Embedding ê²°ê³¼: (1, 5, 100)
    Embedding Layerì˜ Weight í˜•íƒœ: (5, 100)
    
    RNN ê²°ê³¼ (ëª¨ë“  Step Output): (1, 5, 64)
    RNN Layerì˜ Weight í˜•íƒœ: (100, 64)
    
    RNN ê²°ê³¼ (ìµœì¢… Step Output): (1, 64)
    RNN Layerì˜ Weight í˜•íƒœ: (100, 64)


ì–´ë–¤ ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ ë‚˜ëˆ„ê¸° ìœ„í•´ì„œë¼ë©´ ë¬¸ì¥ì„ ëª¨ë‘ ì½ì€ í›„,  
ìµœì¢… Stepì˜ Outputë§Œ í™•ì¸í•´ë„ íŒë‹¨ì´ ê°€ëŠ¥í•˜ë‹¤.  

í•˜ì§€ë§Œ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê²½ìš°ë¼ë©´  
ì´ì „ ë‹¨ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒì„±ëœ  
ëª¨ë“  ë‹¤ìŒ ë‹¨ì–´, ì¦‰ ëª¨ë“  Stepì— ëŒ€í•œ Outputì´ í•„ìš”í•˜ë‹¤.

ëª¨ë“  step ì˜ output ì€ ` return_sequences=True` ë¡œ ì¡°ì ˆ ê°€ëŠ¥í•˜ë‹¤

ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ ê²°êµ­ ë§ˆì§€ë§‰ì— ë‚¨ëŠ” Weight ì€ (100, 64)ë¡œ ë˜‘ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤.



#### ìœ„ì˜ ì½”ë“œëŠ” ì•„ë˜ì˜ LSTM ì‚¬ìš© ì½”ë“œì™€ ë™ì¼í•˜ë‹¤


```python
lstm_seq_layer = tf.keras.layers.LSTM(units=64, return_sequences=True, use_bias=False)
lstm_seq_out = lstm_seq_layer(emb_out)

print("\nLSTM ê²°ê³¼ (ëª¨ë“  Step Output):", lstm_seq_out.shape)
print("LSTM Layerì˜ Weight í˜•íƒœ:", lstm_seq_layer.weights[0].shape)

lstm_fin_layer = tf.keras.layers.LSTM(units=64, use_bias=False)
lstm_fin_out = lstm_fin_layer(emb_out)

print("\nLSTM ê²°ê³¼ (ìµœì¢… Step Output):", lstm_fin_out.shape)
print("LSTM Layerì˜ Weight í˜•íƒœ:", lstm_fin_layer.weights[0].shape)
```

    WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
    
    LSTM ê²°ê³¼ (ëª¨ë“  Step Output): (1, 5, 64)
    LSTM Layerì˜ Weight í˜•íƒœ: (100, 256)
    WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
    
    LSTM ê²°ê³¼ (ìµœì¢… Step Output): (1, 64)
    LSTM Layerì˜ Weight í˜•íƒœ: (100, 256)


ì ê¹ì ê¹ LSTM ì´ ë­”ë° ê°‘ìê¸° ë‚˜ì™€..?

# Recuurent layer - LSTM

; Long short Term memory

ì–˜ë„ RNN ë ˆì´ì–´ì˜ ì¼ì¢…ì´ë‹¤.

---

ë”¥ëŸ¬ë‹ì€ back propagation ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ì„ êµ¬í•œ ë‹¤ìŒ ì—…ë°ì´íŠ¸í•œë‹¤.

ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” RNN ì˜ íŠ¹ì„±ìƒ, input ì´ ê¸¸ìˆ˜ë¡ ì´ˆê¸° ë‹¨ì–´ì˜ ë¯¸ë¶„ê°’ì´  
ë§¤ìš° ì‘ì•„ì§€ê±°ë‚˜ ì»¤ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤.

ì´ í˜„ìƒì„ ê¸°ìš¸ê¸° ì†Œì‹¤ (vanishing) í˜¹ì€ í¬í™” (exploding) ì´ë¼ê³  í•œë‹¤.

LSTM ì€ ì¼ë°˜ RNNë³´ë‹¤ 4ë°° í° ê°€ì¤‘ì¹˜ ê°’ì„ ê°€ì§„ë‹¤.  
ìœ„ë¥¼ ë³´ë©´ RNN =(100,64) , LSTM = (100,256) ì¸ê±°ë¥¼ ë³´ë©´ ëœë‹¤.

í•˜ì§€ë§Œ ë‹¨ìˆœíˆ weight ê°€ 4ë°° 'ë§ì€' ê²Œ ì•„ë‹ˆë¼ 4ë²  'ë‹¤ì–‘í•œ' ê²ƒì´ë‹¤.

ê° weight ëŠ” `Gate` ë¼ëŠ” êµ¬ì¡°ì— í¬í•¨ë˜ì–´ ê¸°ì–µí•  ì •ë³´, ì „ë‹¬í•  ì •ë³´ë¥¼ ê²°ì •í•œë‹¤.

LSTM ì—ëŠ” `Cell state` ë¥¼ í†µí•´ì„œ ê¸´ ë¬¸ì¥ì˜ ì•ë¶€ë¶„ë„ ì†ì‹¤ ì—†ì´ ì €ì¥í•´ì¤€ë‹¤.  
ì•ì„œ ì–¸ê¸‰í•œ Gate ê°€ Cell state ì— ì •ë³´ë¥¼ ì¶”ê°€/ì‚­ì œ í•œë‹¤.

---
### ìì„¸í•œ ì„¤ëª…

![png](5.png)


## ì•„ë˜ì˜ ê·¸ë¦¼ì€ í•˜ë‚˜ì˜ í™œì„±í•¨ìˆ˜ë¥¼ ì§€ë‹Œ ê¸°ë³¸ RNN ì´ë‹¤.

![png](6.png)


## ì´ì™€ ë‹¬ë¦¬ LSTM ì€ í•œ ë ˆì´ì–´ì˜ 4 ê°€ì§€ ê°€ì¤‘ì¹˜ ì¡´ì¬

![png](7.png)


LSTM ì´ ê°€ì§„ ê°€ì¥ í° íŠ¹ì§•ì€ ìƒë‹¨ì— ê°€ë¡œë¡œ ê·¸ì–´ì§„ Cell ctate ì´ë‹¤.

ì–˜ëŠ” ì»¨ë² ì´ì–´ ë²¨íŠ¸ì²˜ëŸ¼ ì‘ì€ ì„ í˜•ë³€í™˜ì„ ì•„ì£¼ ì¡°ê¸ˆì”© í•˜ë©´ì„œ ì •ë³´ê°€ ë‚˜ì•„ê°„ë‹¤.

LSTM ì€ ì´ ëŠ¥ë ¥ì„ gate ë¼ê³  ë¶ˆë¦¬ëŠ” êµ¬ì¡°ë¡œ ì¡°ê¸ˆì”© ë³€í˜•ì‹œí‚¨ë‹¤.

Gate = ì‹œê·¸ëª¨ì´ë“œ ì™€ pointwise ê³±ì…ˆìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì •ë³´ì „ë‹¬ ë°©ë²•

ì‹œê·¸ëª¨ì´ë“œì˜ output ì€ 0ê³¼ 1ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆì–´ ë³´ë‚¼ ì •ë³´ì™€ ë§‰ì„ ì •ë³´ë¥¼ ê³ ë¥¸ë‹¤

LSTM ì€ 3ê°œì˜ gate ê°’ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ 3ê°œë¡œ CELL STATE ì— ë³´ë‚¼ ê°’ì„ ì œì–´í•œë‹¤.

3ê°œì˜ GATE ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. forgat gate layer

cell state ì—ì„œ ì§€ìš¸ ê°’ ì„ ì •

2. input gate layer

ìƒˆë¡œìš´ cell state ë¥¼ ê¸°ì¡´ cell state ì— ë°˜ì˜í•  ì •ë„ë¥¼ ì„ ì •


- (ì—¬ê¸°ì„œ ì›ë˜ ë³¸ì—°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ ì´ì „ 1,2ë²ˆì—ì„œ ì •í•œ ì¼ í•´ì¤Œ)


3. output gate layer

cell state ë¡œ í•„í„°ëœ output ë°°ì¶œ

---
1ë²ˆ , 2ë²ˆ, - ë²ˆ, 4ë²ˆ ì´ë ‡ê²Œ ì´ 4ë²ˆì˜ ë ˆì´ì–´ í™œë™ìœ¼ë¡œ LSTM ì€ ì‘ë™í•œë‹¤.

---
ì´ ì™¸ì—ë„ ë­

ì—¿ë³´ê¸° LSTM,, GRU,,, 

BIRNN ë“± ë­ ì´ê²ƒì €ê²ƒ ë§ë‹¤
ì•„ë˜ ì½”ë“œëŠ” ì–‘ë°©í–¥(Bidirectional) RNN ì½”ë“œì„

ì–‘ë°©í–¥ì´ë¼ì„œ ê°€ì¤‘ì¹˜ê°€ ë‘ë°°ì„ ì• ë’¤ì—ì„œ ê°€ì•¼ë˜ë‹ˆê¹Œ


```python
import tensorflow as tf

sentence = "What time is it ?"
dic = {
    "is": 0,
    "it": 1,
    "What": 2,
    "time": 3,
    "?": 4
}

sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])

embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)
emb_out = embedding_layer(sentence_tensor)

print("ì…ë ¥ ë¬¸ì¥ ë°ì´í„° í˜•íƒœ:", emb_out.shape)

bi_rnn = \
tf.keras.layers.Bidirectional(
    tf.keras.layers.SimpleRNN(units=64, use_bias=False, return_sequences=True)
)
bi_out = bi_rnn(emb_out)

print("Bidirectional RNN ê²°ê³¼ (ìµœì¢… Step Output):", bi_out.shape)
```

    ì…ë ¥ ë¬¸ì¥ ë°ì´í„° í˜•íƒœ: (1, 5, 100)
    Bidirectional RNN ê²°ê³¼ (ìµœì¢… Step Output): (1, 5, 128)




```toc

```
