---
emoji: ğŸ˜
title: í™œì„±í™” í•¨ìˆ˜ë€?
date: '2022-05-20 23:15:00'
author: í•˜ì„±ë¯¼
tags: blog gatsby theme ê°œì¸ ë¹„ í…Œë§ˆ
categories: DeepML
---

# <span style='background-color: #fff5b1'>í™œì„±í™”í•¨ìˆ˜, ë°ì´í„°ë¥¼ ì£½ì—¬ì‚´ë ¤? </span>

ì˜ì–´ë¡œëŠ” activation í•¨ìˆ˜ë¼ê³  í•œë‹¤.

í™œì„±í™” ë˜ì—ˆë‹¤?

=> ì–´ë–¤ ì¡°ê±´ì„ ë§Œì¡±ì‹œì¼°ë‹¤. ë¼ëŠ” ëœ».

ì‹ ê²½ë§ ì†ì˜ í¼ì…‰íŠ¸ë¡  perceptron í˜¹ì€ node ë„  
'íŠ¹ì •ì¡°ê±´' ì´ ê°–ì¶°ì§€ë©´ 'í™œì„±í™”' ëœë‹¤.

íŠ¹ì • ì¡°ê±´ ì´ë€ ì–´ë–¤ ì„ê³„ì¹˜ë¥¼ ë„˜ì—ˆëƒ ë„˜ì§€ ì•Šì•˜ëŠëƒë¡œ êµ¬ë¶„ëœë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ê°€ ìˆë‹¤.

## í™œì„±í™” í•¨ìˆ˜ ì‹œê·¸ëª¨ì´ë“œ ì˜ ì˜ˆ

<img src='./image1.png' width=300>
ê·¸ë¦¼ì„ ë³´ë©´, x ê°’ì€ ë¬´í•œí•˜ê³ , yì˜ ê°’ì€ 0ì—ì„œ 1 ì‚¬ì´ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.

ë§Œì•½ ì¶œë ¥ê°’ì´ 0.5 ì´ìƒì´ë©´ í™œì„±í™” ëœê²ƒìœ¼ë¡œ ë³´ê³ ,  
ê·¸ ë¯¸ë§Œì´ë©´ ë¹„í™œì„±í™” ë¼ê³  ìƒê° í–ˆì„ë•Œ,  

ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ì´ 0 ì´í•˜ì¼ë•ŒëŠ” ë¹„í™œì„±í™”  
0 ì´ìƒì¼ ë•Œë§Œ í™œì„±í™”ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.

ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ë¬´ì¡°ê±´ ì¶œë ¥ì´ 0 ì—ì„œ 1 ì‚¬ì´ë‹ˆê¹Œ

ì´ì§„ ë¶„ë¥˜ì— ìš©ì´í•˜ë‹¤.

0 í˜¹ì€ 1ì— ìˆ˜ë ´í•˜ê¸° ë•Œë¬¸ì´ë‹¤

#### ë”¥ëŸ¬ë‹ì—ì„œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì“°ëŠ” í° ì´ìœ ëŠ”

ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ **í‘œí˜„ë ¥** ì„ í–¥ìƒì‹œì¼œì£¼ê¸° ìœ„í•´ì„œì´ë‹¤.

ë‹¤ë¥¸ ë§ë¡œ

representation capacity   
expressivity  

ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤ê³  í•œë‹¤.

ë§Œì•½ì— y = wx + b ë¼ëŠ” ëª¨ë¸ì´ ìˆë‹¤.

<img src='./image2.png' width=300>
ì´ ëª¨ë¸ë¡œëŠ” x^2 ë‚˜ sin(x) ì˜ ë¶„í¬ë¥¼ ì ˆëŒ€ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤.  
ë‹¤ì‹œ ë§í•´ì„œ, ì„ í˜• í•¨ìˆ˜ (y = wx +b ) ì— ì–´ë–¤ ìˆ˜ë¥¼ ê³±í•˜ê±°ë‚˜ ë”í•œë‹¤ê³  í•´ì„œ    
ë¹„ì„ í˜• í•¨ìˆ˜ë¡œ ë§Œë“¤ ìˆ˜ëŠ” ì—†ë‹¤.

ê·¸ëŸ°ë° ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ì™€ ì„ í˜•í•¨ìˆ˜ë¥¼ ì—°ì‚°í•´ì¤€ë‹¤ë©´   
ì´ëŸ° ë¹„ì„ í˜• ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

### í™œì„±í™” í•¨ìˆ˜ì˜ ì¢…ë¥˜


0. ì´ì§„ ê³„ë‹¨ í•¨ìˆ˜ ( Binary step Function)

1. ì„ í˜• í™œì„±í™” í•¨ìˆ˜

ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì“°ë©´ ê°€ì¤‘ì¹˜ê°€ ë§ì„ í•„ìš”ê°€ ì—†ë‹¤.
ì–´ì°¨í”¼ ì„ í˜• í•¨ìˆ˜ë¼ë¦¬ëŠ” ê³±í•´ë„ ì„ í˜•í•¨ìˆ˜ê¸° ë•Œë¬¸ì—
ë‹¤ í•©ì„±ë˜ì–´ ê·¸ëƒ¥ ì»¤ë‹¤ë€ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ì¹˜í™˜ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ëŸ¼ ì´ê±´ ë¹„ì„ í˜• ë¶„í¬ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ê°€ ì—†ë‹¤.  
ì´ëŸ¬ë‹ˆ í‘œí˜„ë ¥ì´ ë–¨ì–´ì§„ë‹¤.

2. ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜


ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì¼ì„ ë•Œ

representation capacity ê°€ ì˜¤ë¥¸ë‹¤.

---


## binary step function

ì–˜ëŠ” ì¶œë ¥ì´ 0 í˜¹ì€ 1 ë°–ì— ì—†ë‹¤.

ê·¸ë˜ì„œ ì–˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì“°ì¸ë‹¤.

ì„ê³„ì  ì´ ìˆìœ¼ë©´ ê±”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0ê³¼ 1ì„ ì¶œë ¥í•œë‹¤.




```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from itertools import product
import tensorflow as tf

tf.random.set_seed(7879)

```


```python
def binary_step(x, threshold=0):  # 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0, ê°™ê±°ë‚˜ í¬ë©´ 1
    return 0 if x<threshold else 1
```


```python
binary_step(5)
```




    1




```python
binary_step(-6)
```




    0




```python
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

def plot_and_visulize(image_url, function, derivative=False):
    X = [-10 + x/100 for x in range(2000)]
    y = [function(y) for y in X]
    
    plt.figure(figsize=(12,12))
    
    # í•¨ìˆ˜ ê·¸ë˜í”„
    plt.subplot(3,2,1)
    plt.title('function')
    plt.plot(X,y)
    
    # í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê·¸ë˜í”„
    plt.subplot(3,2,2)
    plt.title('derivative')
    if derivative:
        dev_y = [derivative(y) for y in X]
        plt.plot(X,dev_y)
    
    # ë¬´ì‘ìœ„ ìƒ˜í”Œë“¤ ë¶„í¬
    samples = np.random.rand(1000)
    samples -= np.mean(samples)
    plt.subplot(3,2,3)
    plt.title('samples')
    plt.hist(samples,100)
    
    # í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µê³¼í•œ ìƒ˜í”Œë“¤ ë¶„í¬
    act_values = [function(y) for y in samples]
    plt.subplot(3,2,4)
    plt.title('activation values')
    plt.hist(act_values,100)
    
    # ì›ë³¸ ì´ë¯¸ì§€
    image = np.array(Image.open(image_url), dtype=np.float64)[:,:,0]/255. # êµ¬ë¶„ì„ ìœ„í•´ gray-scaleí•´ì„œ í™•ì¸
    image -= np.median(image) #ì¤‘ì•™ê°’ì„ ë¹¼ì¤¬ìœ¼ë‹ˆ 125 ì´í•˜ëŠ” 0, 125 ì´ìƒì€ 1ë¡œ ë°”ë€œ
    plt.subplot(3,2,5)
    plt.title('origin image')
    plt.imshow(image, cmap='gray')
    
    # í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µê³¼í•œ ì´ë¯¸ì§€
    activation_image = np.zeros(image.shape)
    h, w = image.shape
    for i in range(w):
        for j in range(h):
            activation_image[j][i] += function(image[j][i])
    plt.subplot(3,2,6)
    plt.title('activation results')
    plt.imshow(activation_image, cmap='gray')
    
    return plt
```

ë‚´ ì‚¬ì§„ì˜ pixel ë°ì´í„°ë¥¼ ê°€ì§€ê³   
ì§ì € ë§Œë“  í™œì„±í™” í•¨ìˆ˜ Binary step Function ì— ë„£ì–´ë³´ì•˜ë‹¤.




```python
import os
img_path = os.getenv('HOME')+'/aiffel/Study/26/activation/me.jpg'
ax = plot_and_visulize(img_path, binary_step)
ax.show()
```


    
![png](output_7_0.png)
    


ì´ë ‡ë“¯ ì´ì§„ í•¨ìˆ˜ëŠ” ê°’ì´ ìˆê³  ì—†ê³ ë¡œ ì €ë ‡ê²Œ 0 í˜¹ì€ 1ë¡œ ë”± êµ¬ë¶„í•´ì¤€ë‹¤!

Binary step Func ì€ ë‹¨ì¸µ  perceptron êµ¬ì¡° ì‹ ê²½ë§ì—ì„œ ë§ì´ ì‚¬ìš©ë¨

<img src='./image3.png' width=300>




```python
# í¼ì…‰íŠ¸ë¡ 
class Perceptron(object):
    def __init__(self, input_size, activation_ftn, threshold=0, learning_rate=0.01):
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn(1)
        self.activation_ftn = np.vectorize(activation_ftn) #ì„ í˜• ë³€í™˜ í•´ì£¼ê² ë‹¤ëŠ” ëœ»
        self.learning_rate = learning_rate
        self.threshold = threshold

    def train(self, training_inputs, labels, epochs=100, verbose=1):
        '''
        verbose : 1-ë§¤ ì—í¬í¬ ê²°ê³¼ ì¶œë ¥, 
                  0-ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ì¶œë ¥ 
        '''
        for epoch in range(epochs):
            for inputs, label in zip(training_inputs, labels):
                prediction = self.__call__(inputs)
                self.weights += self.learning_rate * (label - prediction) * inputs
                self.bias += self.learning_rate * (label - prediction)
            if verbose == 1:
                pred = self.__call__(training_inputs)
                accuracy = np.sum(pred==labels)/len(pred)
                print(f'{epoch}th epoch, accuracy : {accuracy}')
        if verbose == 0:
            pred = self.__call__(training_inputs)
            accuracy = np.sum(pred==labels)/len(pred)
            print(f'{epoch}th epoch, accuracy : {accuracy}')
    
    def get_weights(self):
        return self.weights, self.bias
                
    def __call__(self, inputs):
        summation = np.dot(inputs, self.weights) + self.bias
        return self.activation_ftn(summation, self.threshold)
```

ê·¼ë° ìš”ëŸ° ì‹ ê²½ë§ì€ and ë‚˜ or ì— ëŒ€í•´ì„  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.

ë‹¤ì‹œë§í•´ í•˜ë‚˜ì˜ ì„ ì„ ë”± ê·¸ì–´ì„œ êµ¬ë¶„í•˜ëŠ” ë¬¸ì œëŠ” ì„¤ëª… ê°€ëŠ¥í•˜ë‹¤.


```python
def scatter_plot(plt, X, y, threshold = 0, three_d=False):
    ax = plt
    if not three_d:
        area1 = np.ma.masked_where(y <= threshold, y)
        area2 = np.ma.masked_where(y > threshold, y+1)
        ax.scatter(X[:,0], X[:,1], s = area1*10, label='True')
        ax.scatter(X[:,0], X[:,1], s = area2*10, label='False')
        ax.legend()
    else:
        area1 = np.ma.masked_where(y <= threshold, y)
        area2 = np.ma.masked_where(y > threshold, y+1)
        ax.scatter(X[:,0], X[:,1], y-threshold, s = area1, label='True')
        ax.scatter(X[:,0], X[:,1], y-threshold, s = area2, label='False')
        ax.scatter(X[:,0], X[:,1], 0, s = 0.05, label='zero', c='gray')
        ax.legend()
    return ax
```


```python
# AND gate, OR gate
X = np.array([[0,0], [1,0], [0,1], [1,1]])

plt.figure(figsize=(10,5))
# OR gate
or_y = np.array([x1 | x2 for x1,x2 in X])
ax1 = plt.subplot(1,2,1)
ax1.set_title('OR gate ' + str(or_y))
ax1 = scatter_plot(ax1, X, or_y)

# AND gate
and_y = np.array([x1 & x2 for x1,x2 in X])
ax2 = plt.subplot(1,2,2)
ax2.set_title('AND gate ' + str(and_y))
ax2 = scatter_plot(ax2, X, and_y)

plt.show()
```


    
![png](output_13_0.png)
    



```python
# OR gate
or_p = Perceptron(input_size=2, activation_ftn=binary_step)
or_p.train(X, or_y, epochs=1000, verbose=0)
print(or_p.get_weights()) # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ê°’ì€ í›ˆë ¨ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# AND gate
and_p = Perceptron(input_size=2, activation_ftn=binary_step)
and_p.train(X, and_y, epochs=1000, verbose=0)
print(and_p.get_weights()) # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ê°’ì€ í›ˆë ¨ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

    999th epoch, accuracy : 1.0
    (array([0.802858  , 0.99642675]), array([-0.78371765]))
    999th epoch, accuracy : 1.0
    (array([0.65508949, 0.02640151]), array([-0.67231513]))


ì´ ì •í™•ë„ë¥¼ ì–´ë–»ê²Œ ë‚˜íƒ€ëƒˆëŠ”ì§€ë¥¼ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

x,y ì¶•ì„ 100ë“±ë¶„í•œ ë‹¤ìŒ True ì™€ False ì˜ ê²½ê³„ì„ ì„ í•˜ë‚˜ ê³¨ë¼ ê·¸ì—ˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.


```python
from itertools import product

# ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ê¸°
test_X = np.array([[x/100,y/100] for (x,y) in product(range(101),range(101))])
pred_or_y = or_p(test_X)
pred_and_y = and_p(test_X)

plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,2,1)
ax1.set_title('predict OR gate')
ax1 = scatter_plot(ax1, test_X, pred_or_y)

ax2 = plt.subplot(2,2,2, projection='3d')
ax2.set_title('predict OR gate 3D')
ax2 = scatter_plot(ax2, test_X, pred_or_y, three_d=True)

ax3 = plt.subplot(2,2,3)
ax3.set_title('predict AND gate')
ax3 = scatter_plot(ax3, test_X, pred_and_y)

ax4 = plt.subplot(2,2,4, projection='3d')
ax4.set_title('predict AND gate 3D')
ax4 = scatter_plot(ax4, test_X, pred_and_y, three_d=True)

plt.show()
```


    
![png](output_16_0.png)
    


í•˜ì§€ë§Œ ì´ëŒ€ë¡œëŠ” XOR ë¬¸ì œë¥¼ í’€ìˆ˜ê°€ ì—†ë‹¤, ì •í™•ë„ê°€ 25%ë°–ì— ì•ˆë‚˜ì˜¨ë‹¤


```python
# XOR gate
threshold = 0
X = np.array([[0,0], [1,0], [0,1], [1,1]])

plt.figure(figsize=(5,5))
xor_y = np.array([x1 ^ x2 for x1,x2 in X])
plt.title('XOR gate '+ str(xor_y))
plt = scatter_plot(plt, X, xor_y)
plt.show()
```


    
![png](output_18_0.png)
    



```python
# XOR gateê°€ í’€ë¦´ê¹Œ?
xor_p = Perceptron(input_size=2, activation_ftn=binary_step, threshold=threshold)
xor_p.train(X, xor_y, epochs=1000, verbose=0)
print(xor_p.get_weights())

# ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ê¸°
test_X = np.array([[x/100,y/100] for (x,y) in product(range(101),range(101))])
pred_xor_y = xor_p(test_X)

plt.figure(figsize=(10,5))
ax1 = plt.subplot(1,2,1)
ax1.set_title('predict XOR gate?')
ax1 = scatter_plot(ax1, test_X, pred_xor_y)

ax2 = plt.subplot(1,2,2, projection='3d')
ax2.set_title('predict XOR gate 3D?')
ax2 = scatter_plot(ax2, test_X, pred_xor_y, three_d=True)

plt.show()
```

    999th epoch, accuracy : 0.25
    (array([-0.01313922, -0.01549442]), array([0.00652869]))



    
![png](output_19_1.png)
    


ì´ë¥¼ í•´ê²°í•œ ë°©ë²•ì´ ë°”ë¡œ ë‹¤ì¸µ perceptron ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.

Multi-Layer Perceptron

MLP

ì¡°ì‹¬í•´ì•¼í• ê±°!!! ì—¬ê¸°ì„œ layer ëŠ” perceptron ì„ ì–¼ë§ˆë‚˜ ì“°ëƒì— ëŒ€í•œ layer ì´ì§€  
ìš°ë¦¬ê°€ í‰ì†Œì— ë§í•˜ëŠ” layer ê°€ ì•„ë‹ˆë‹¤!!

multi layer ëŠ” ë…¸ë“œê°€ 2ê°œ ì´ìƒì¸ layer í•˜ë‚˜ë¥¼ ì˜ë¯¸í•œë‹¤! layer ê°€ ì—¬ëŸ¬ê°œì¸ê²Œ ì•„ë‹ˆë‹¤!

<img src='./image4.png' width=300>

ì´ì²˜ëŸ¼ ì¸µì„ ì—¬ëŸ¬ê°œ ìŒ“ì•„ë²„ë¦¬ë©´ ë¹„ì„ í˜• ì¸ XOR ë„ ì—ì¸¡ì€ í•  ìˆ˜ ìˆì§€ë§Œ

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë¬¸ì œì 
1. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ëª»í•œë‹¤

ì‚¬ì‹¤ ì´ì§„ í•¨ìˆ˜ Binary step function ì€ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ back propagation  
ì—­ì „íŒŒë¥¼ í•˜ë ¤ë©´ ë¯¸ë¶„ì´ í•„ìš”í•œë° ì–˜ë„¤ëŠ” ë‹¤ ìƒìˆ˜ë¼ ë¯¸ë¶„ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì´ë‹¤.

2. ë‹¤ì¤‘ ì¶œë ¥ì„ í•  ìˆ˜ ì—†ë‹¤.
ì¶œë ¥ì´ 0 1 ë‘˜ì¤‘ í•˜ë‚˜ë°–ì— ì•ˆë‚˜ì˜¤ë‹ˆê¹Œ
ë‹¤ì¤‘ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ ê³¨ë¼ì•¼ í•  ë•ŒëŠ” ì“¸ ìˆ˜ ì—†ë‹¤.

ê²°ë¡  : ì–´ì°¨í”¼ ì˜›ë‚ ê±°ë‹ˆê¹Œ ê± ë„˜ì–´ê°€ì

#### ì„ í˜•í™œì„±í™” í•¨ìˆ˜ëŠ” ì•„ê¹Œ ë§í–‡ë“¯ì´ ì–¼ë§ˆë‚˜ ë„£ë“  í•˜ë‚˜ì˜ í™œì„±í™” í•¨ìˆ˜ë‘ ë˜‘ê°™ê³  ë³„ë¡œë‹ˆê¹Œ ë„˜ì–´ê°„ë‹¤

# ì´ì œ ì§„ì§œ ì¤‘ìš”í•œ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ Non-Linear

1. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ê°€ëŠ¥
2. ë‹¤ì¤‘ ì¶œë ¥ ê°€ëŠ¥
3. ë¹„ì„ í˜• íŠ¹ì§• ì˜ˆì¸¡ ê°€ëŠ¥

ì™€ìš°;; ì¸ê³µì§€ëŠ¥ ë§ŒëŠ¥ì—´ì‡  Non-linear

### ì¢…ë¥˜

1. ì‹œê·¸ëª¨ì´ë“œ(logistic)


```python
import os
img_path = os.getenv('HOME')+'/aiffel/Study/26/activation/me.jpg'

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
def sigmoid(x):
    return 1/(1+np.exp(-x).astype(np.float64))

def dev_sigmoid(x):
    return sigmoid(x)*(1-sigmoid(x))

# ì‹œê°í™”
ax = plot_and_visulize(img_path, sigmoid, dev_sigmoid)
ax.show()
```


    
![png](output_24_0.png)
    



```python
# ìˆ˜ì¹˜ ë¯¸ë¶„
def num_derivative(x, function):
    h = 1e-15 # ì´ ê°’ì„ ë°”ê¾¸ì–´ ê°€ë©° ê·¸ë˜í”„ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”
    numerator = function(x+h)-function(x)
    return numerator/h

# ë‘ ê·¸ë˜í”„ì˜ ì°¨ì´
diff_X = [-5+x/100 for x in range(1001)]
dev_y = np.array([dev_sigmoid(x) for x in diff_X])
num_dev_y = np.array([num_derivative(x, sigmoid) for x in diff_X])

diff_y = dev_y - num_dev_y
plt.plot(diff_X, num_dev_y, label='numerical')
plt.plot(diff_X, dev_y, label='analytic')
plt.plot(diff_X, diff_y, label='differnce')
plt.legend()

plt.show()
```


    
![png](output_25_0.png)
    



```python
# OR gate
or_sigmoid_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
or_sigmoid_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])
or_sigmoid_model.fit(X, or_y, epochs=1000, verbose=0)

# AND gate
and_sigmoid_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
and_sigmoid_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])
and_sigmoid_model.fit(X, and_y, epochs=1000, verbose=0)

# XOR gate
xor_sigmoid_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
xor_sigmoid_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])
xor_sigmoid_model.fit(X, xor_y, epochs=1000, verbose=0)

# ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ê¸°
test_X = np.array([[x/100,y/100] for (x,y) in product(range(101),range(101))])
pred_or_y = or_sigmoid_model(test_X)
pred_and_y = and_sigmoid_model(test_X)
pred_xor_y = xor_sigmoid_model(test_X)

plt.figure(figsize=(10,15))
ax1 = plt.subplot(3,2,1)
ax1.set_title('predict OR gate')
ax1 = scatter_plot(ax1, test_X, pred_or_y, threshold=0.5)

ax2 = plt.subplot(3,2,2, projection='3d')
ax2.set_title('predict OR gate 3D')
ax2 = scatter_plot(ax2, test_X, pred_or_y, threshold=0.5, three_d=True)

ax3 = plt.subplot(3,2,3)
ax3.set_title('predict AND gate')
ax3 = scatter_plot(ax3, test_X, pred_and_y, threshold=0.5)

ax4 = plt.subplot(3,2,4, projection='3d')
ax4.set_title('predict AND gate 3D')
ax4 = scatter_plot(ax4, test_X, pred_and_y, threshold=0.5, three_d=True)

ax5 = plt.subplot(3,2,5)
ax5.set_title('predict XOR gate')
ax5 = scatter_plot(ax5, test_X, pred_xor_y, threshold=0.5)

ax6 = plt.subplot(3,2,6, projection='3d')
ax6.set_title('predict XOR gate 3D')
ax6 = scatter_plot(ax6, test_X, pred_xor_y, threshold=0.5, three_d=True)

plt.show()
```


    
![png](output_26_0.png)
    


ê·¼ë° ì‚¬ì‹¤ ì–˜ë„ XOR ë¬¸ì œë¥¼ ëª»í‘¼ë‹¤. ë‹¤ë¥¸ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì¨ì¤˜ì•¼ ëœë‹¤.

 2ì°¨ ë‹¤í•­ì‹(quadratic polynomial)ì„ ì¶”ê°€í•œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ XOR gateë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
 
 ì´ ë°–ì—ë„ layerë¥¼ ì¶”ê°€í•´ ì¤€ë‹¤ë©´ XOR gateë¥¼ ë¬´ë¦¬ ì—†ì´ êµ¬í˜„í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
 
 


```python
# ë ˆì´ì–´ë¥¼ ì¶”ê°€í–ˆì„ ë•Œ
# XOR gate
xor_sigmoid_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),
    tf.keras.layers.Dense(2, activation='sigmoid'), # 2 nodesë¡œ ë³€ê²½
    tf.keras.layers.Dense(1)
])
xor_sigmoid_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])
xor_sigmoid_model.fit(X, xor_y, epochs=1000, verbose=0)

plt.figure(figsize=(10,5))
pred_xor_y = xor_sigmoid_model(test_X)

ax1 = plt.subplot(1,2,1)
ax1.set_title('predict XOR gate')
ax1 = scatter_plot(ax1, test_X, pred_xor_y, threshold=0.5)

ax2 = plt.subplot(1,2,2, projection='3d')
ax2.set_title('predict XOR gate 3D')
ax2 = scatter_plot(ax2, test_X, pred_xor_y, threshold=0.5, three_d=True)

plt.show()
```

    /opt/conda/lib/python3.9/site-packages/matplotlib/collections.py:1003: RuntimeWarning: invalid value encountered in sqrt
      scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor



    
![png](output_28_1.png)
    


### ì‹œê·¸ëª¨ì´ë“œëŠ”

0ì—ì„œ 1 ì‚¬ì´ ê°’ì„ ë‚˜íƒ€ë‚´ì£¼ê¸° ë•Œë¬¸ì— ì´ì§„ ë¶„ë¥˜ì—ì„œ ë§ì´ ì“°ì¸ë‹¤.

ê·¼ë° softmax ëŠ” êµ³ì´ 0,1 ì´ ì•„ë‹ˆë”ë¼ë„ ê°€ìœ„ë°”ìœ„ë³´, ì‚¬ì§„ ë¶„ë³„ ë“± ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë¡œ ì´ë¯¸ì§€ì˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ì‡ë‹¤. ëª¨ë“  classì˜ í™•ë¥ ì„ ë”í•˜ë©´ 1ì´ ëœë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. ë‹¤ë§Œ ì´ê±´ hidden layer ì— ë“¤ì–´ê°€ëŠ” í™œì„±í™” í•¨ìˆ˜ê°€ ì•„ë‹ˆë‹¤.

2. í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜ ; tanh

ì–˜ëŠ” ìŒê³¡ì„  í•¨ìˆ˜ì´ë‹¤.

<img src='./image5.png' width=300>

ì €ê¸° íŒŒë€ìƒ‰ ì ë°•ì´ ì„ ì´ í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜

ê·¸ë¦¼ì²˜ëŸ¼ ì–˜ëŠ” xì˜ ê°’ì´ ë¬´ì—‡ì´ë“  -1 ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ë§Œ ë‚˜ì˜¨ë‹¤.  
ì–˜ëŠ” 0 ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ê³  ìˆë‹¤.  



```python
import os
img_path = os.getenv('HOME')+'/aiffel/Study/26/activation/me.jpg'

# í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜
def tanh(x):
    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

def dev_tanh(x):
    return 1-tanh(x)**2

# ì‹œê°í™”
ax = plot_and_visulize(img_path, tanh, dev_tanh)
ax.show()
```


    
![png](output_30_0.png)
    


#### ë‹¨ì 
-1 ê³¼ 1ì—ì„œ í¬í™”ëœë‹¤.

3. ReLU í•¨ìˆ˜

ì–˜ê°€ ìš”ì¦˜ í•«í•˜ë‹¤.


```python
import os
img_path = os.getenv('HOME')+'/aiffel/Study/26/activation/me.jpg'

# relu í•¨ìˆ˜
def relu(x):
    return max(0,x)

# ì‹œê°í™”
ax = plot_and_visulize(img_path, relu)
ax.show()
```


    
![png](output_34_0.png)
    


ì–˜ëŠ” yê°’ì´ 0 ì´ìƒìœ¼ë¡œ ì­‰ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. ì–˜ëŠ” tanh ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ  
í›ˆë ¨ì´ ë˜ëŠ”ë°,  
<img src='./image6.png' width=300>

ì‹¤ì„ ì´ ReLU ì¼ì„ ë•Œ ì—ëŸ¬ ë¹„ìœ¨ì´ê³ ,  
ì ì„ ì´ tanh ì¼ì„ ë•Œ ì—ëŸ¬ ë¹„ìœ¨ì´ë‹¤

ReLUì˜ ì—ëŸ¬ ê°ì†Œë¹„ìœ¨ì´ í›¨ì”¬ ë¹ ë¥´ë‹¤.

#### ë¯¸ë¶„í• ë•ŒëŠ” 0ì˜ ê²½ìš°ì—ë§Œ 0,ë˜ëŠ” 1ì„ ë¬´ì‘ìœ„ë¡œ ë°°ì¶œí•œë‹¤.

ê·¼ë° ì—¬ê¸°ì„œ ì‹ ê¸°í•œê±°
ì–˜ëŠ” ê³¡ì„ ê°™ì€ ë¹„ì„ í˜•ì´ ì•„ë‹Œë° ì–´ë–»ê²Œ ë¹„ì„ í˜•ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì¡ì•„ë‚´ì§€?




```python
q_X = np.array([-10+x/100 for x in range(2001)])
q_y = np.array([(x)**2 + np.random.randn(1)*10 for x in q_X])
plt.scatter(q_X, q_y, s=0.5)
```




    <matplotlib.collections.PathCollection at 0x7fd529d83610>




    
![png](output_36_1.png)
    


ë­ ì´ë ‡ê²Œ ì¡ì•„ë‚¼ ìˆ˜ ìˆë´ë‹¤; ì–´ë µë„¤ì°¸


```python
approx_relu_model_p = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(6, activation='relu'), # 6 nodes ë³‘ë ¬ ì—°ê²°
    tf.keras.layers.Dense(1)
])
approx_relu_model_p.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), metrics=['accuracy'])
approx_relu_model_p.fit(q_X, q_y, batch_size=32, epochs=100, verbose=0)

approx_relu_model_s = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(2, activation='relu'),# 2 nodes ì§ë ¬ë¡œ 3ë²ˆ ì—°ê²°
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(1)
])
approx_relu_model_s.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), metrics=['accuracy'])
approx_relu_model_s.fit(q_X, q_y, batch_size=32, epochs=100, verbose=0)

approx_relu_model_p.summary()
approx_relu_model_s.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_5 (Dense)              (None, 6)                 12        
    _________________________________________________________________
    dense_6 (Dense)              (None, 1)                 7         
    =================================================================
    Total params: 19
    Trainable params: 19
    Non-trainable params: 0
    _________________________________________________________________
    Model: "sequential_5"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_7 (Dense)              (None, 2)                 4         
    _________________________________________________________________
    dense_8 (Dense)              (None, 2)                 6         
    _________________________________________________________________
    dense_9 (Dense)              (None, 2)                 6         
    _________________________________________________________________
    dense_10 (Dense)             (None, 1)                 3         
    =================================================================
    Total params: 19
    Trainable params: 19
    Non-trainable params: 0
    _________________________________________________________________



```python
q_test_X = q_X.reshape((*q_X.shape,1))
plt.figure(figsize=(10,5))

ax1 = plt.subplot(1,2,1)
ax1.set_title('parallel')
pred_y_p = approx_relu_model_p(q_test_X)
ax1.plot(q_X, pred_y_p)

ax2 = plt.subplot(1,2,2)
ax2.set_title('serial')
pred_y_s = approx_relu_model_s(q_test_X)
ax2.plot(q_X, pred_y_s)

plt.show()
```


    
![png](output_39_0.png)
    


parallel ë³‘ë ¬  
serial ì§ë ¬

ë³‘ë ¬ë¡œ ìŒ“ëŠ”ê²Œ ë” ë¶€ë“œëŸ¬ì›Œ ë³´ì´ì§€?

x** 2 ì˜ ê·¸ë˜í”„ë¥¼ ê·¼ì‚¬í•´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ë‹¤.

#### ë‹¨ì  : Dying ReLU
ëª¨ë¸ì—ì„œ ReLUë¥¼ ì‚¬ìš©í•œ ë…¸ë“œê°€ ë¹„í™œì„±í™”ë˜ë©° ì¶œë ¥ì„ 00ìœ¼ë¡œë§Œ í•˜ê²Œ ë˜ëŠ” ê²ƒ

ê°€ì¤‘ì¹˜ wê°’ì— ì˜í•´ ì…ë ¥ê°’ xì— ìƒê´€ì—†ì´ 0ì´í•˜ë¡œ ë‚˜ì˜¤ê²Œ ë˜ì—ˆë‹¤ë©´,  
ì´ ì´í›„ì˜ ì—…ë°ì´íŠ¸ì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ í•­ìƒ 0ì´ ë˜ì–´  
ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.  

ì¦‰, ì´ ë…¸ë“œì˜ ì¶œë ¥ê°’ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ë˜ì–´ ë…¸ë“œê°€ ì£½ì–´ë²„ë¦¬ëŠ” ê²ƒì´ë‹¤.

ì´ëŸ¬í•œ í˜„ìƒì„ ì¤„ì—¬ì£¼ê¸° ìœ„í•´ì„œëŠ” í•™ìŠµë¥ ì„ ë‚®ì¶°ì£¼ì–´ì•¼ í•œë‹¤.

í•™ìŠµë¥  ë¿ë§Œ ì•„ë‹ˆë¼ dying ReLU ë¥¼ ì—†ì• ê¸° ìœ„í•´ ì‹œë„í•œ ì—¬ëŸ¬ ê²½ìš°ê°€ ìˆë‹¤.

1. Leaky ReLU




```python
import os
img_path = os.getenv('HOME')+'/aiffel/Study/26/activation/me.jpg'

# leaky relu í•¨ìˆ˜
def leaky_relu(x):
    return max(0.01*x,x)

# ì‹œê°í™”
ax = plot_and_visulize(img_path, leaky_relu)
ax.show()
```


    
![png](output_41_0.png)
    


ë³´ë©´ ìŒìˆ˜ê°€ 0ì´ ì•„ë‹ˆë¼ 0ì— ê°€ê¹Œìš´ ìŒìˆ˜ì´ë‹¤. ì´ë ‡ê²Œ 0ì´ ë‚˜ì˜¤ëŠ” ê±¸ ë§‰ì•„ì£¼ì—ˆë‹¤.

2. PReLU

return ê°’ì„ ë³´ë©´ alpha ê°’ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ì§ì ‘ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.


```python
# PReLU í•¨ìˆ˜
def prelu(x, alpha):
    return max(alpha*x,x)

# ì‹œê°í™”
ax = plot_and_visulize(img_path, lambda x: prelu(x, 0.1)) # parameter alpha=0.1ì¼ ë•Œ

ax.show()
```


    
![png](output_44_0.png)
    


3. ELU

ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í–ˆë˜ 0ì˜ ë¶€ë¶„ì„ ìì—°ë¡œê·¸ë¡œ êº¾ì–´ì£¼ì–´ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë„ë¡ í–ˆë‹¤.  
(ê·¸ë§Œí¼ ê³„ì‚°ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤)


```python
# elu í•¨ìˆ˜
def elu(x, alpha):
    return x if x > 0 else alpha*(np.exp(x)-1)

def dev_elu(x, alpha):
    return 1 if x > 0 else elu(x, alpha) + alpha

# ì‹œê°í™”
ax = plot_and_visulize(img_path, lambda x: elu(x, 1), lambda x: dev_elu(x, 1)) # alphaê°€ 1ì¼ ë•Œ
ax.show()
```


    
![png](output_47_0.png)
    
```toc

```